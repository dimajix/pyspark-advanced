{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring Error Messages\n",
    "\n",
    "One common challenge with Spark and especially with PySpark is to make sense of error messages. In PySpark, an important part of the problem comes from the fact that two different technologies (JVM and Python) work in conjunction, and both produce stack traces. This notebook tries to provide some guidance for configuring simplified error messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/16 17:34:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/16 17:34:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://fe49fe119e4f:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f298837b070>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"24G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Provoke an Error\n",
    "\n",
    "First we provoke an error by creating an ill-formed program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "replication_df = spark.createDataFrame(pd.DataFrame(list(range(1,1000)),columns=['replication_id'])).repartition(1000, 'replication_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "outSchema = StructType([StructField('replication_id', IntegerType(), True),\n",
    "            StructField('sil_score', DoubleType(), True),\n",
    "            StructField('num_clusters', IntegerType(), True),\n",
    "            StructField('min_samples', IntegerType(), True),\n",
    "            StructField('min_cluster_size', IntegerType(), True)])\n",
    "\n",
    "\n",
    "def run_model(df_pandas: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Return result as a pandas data frame\n",
    "    return pd.DataFrame({'replication_id': replication_id, 'sil_score': 2,\n",
    "                           'num_clusters': 3, 'min_samples': 4,\n",
    "                           'min_cluster_size': 5}, index=[0])\n",
    "\n",
    "\n",
    "results = replication_df.groupBy(\"replication_id\").applyInPandas(run_model, outSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Configuring Error Messages\n",
    "\n",
    "PySpark provides two important configuration properties `spark.sql.pyspark.jvmStacktrace.enabled` and `spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled` which control how error messages are presented to the developer. We will try different settings and compare the output in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Full Detail\n",
    "\n",
    "First we turn on the JVM Stacktrace and disable a simplification for UDFs. Evil combination, as we will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/16 17:38:02 ERROR Executor: Exception in task 27.0 in stage 14.0 (TID 500)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n",
      "    return f(keys, vals)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n",
      "    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 187, in wrapped\n",
      "    result = f(pd.concat(value_series, axis=1))\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_10896/1928652177.py\", line 13, in run_model\n",
      "NameError: name 'replication_id' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "23/11/16 17:38:02 WARN TaskSetManager: Lost task 27.0 in stage 14.0 (TID 500) (fe49fe119e4f executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n",
      "    return f(keys, vals)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n",
      "    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 187, in wrapped\n",
      "    result = f(pd.concat(value_series, axis=1))\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_10896/1928652177.py\", line 13, in run_model\n",
      "NameError: name 'replication_id' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n",
      "23/11/16 17:38:02 ERROR TaskSetManager: Task 27 in stage 14.0 failed 1 times; aborting job\n",
      "23/11/16 17:38:02 ERROR Executor: Exception in task 1.0 in stage 14.0 (TID 474)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n",
      "    return f(keys, vals)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n",
      "    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 187, in wrapped\n",
      "    result = f(pd.concat(value_series, axis=1))\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_10896/1928652177.py\", line 13, in run_model\n",
      "NameError: name 'replication_id' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "23/11/16 17:38:02 ERROR Executor: Exception in task 10.0 in stage 14.0 (TID 483)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n",
      "    return f(keys, vals)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n",
      "    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 187, in wrapped\n",
      "    result = f(pd.concat(value_series, axis=1))\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_10896/1928652177.py\", line 13, in run_model\n",
      "NameError: name 'replication_id' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "23/11/16 17:38:02 ERROR Executor: Exception in task 20.0 in stage 14.0 (TID 493)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n",
      "    return f(keys, vals)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n",
      "    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 187, in wrapped\n",
      "    result = f(pd.concat(value_series, axis=1))\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_10896/1928652177.py\", line 13, in run_model\n",
      "NameError: name 'replication_id' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "23/11/16 17:38:02 ERROR Executor: Exception in task 28.0 in stage 14.0 (TID 501)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n",
      "    return f(keys, vals)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n",
      "    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 187, in wrapped\n",
      "    result = f(pd.concat(value_series, axis=1))\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_10896/1928652177.py\", line 13, in run_model\n",
      "NameError: name 'replication_id' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "23/11/16 17:38:02 ERROR Executor: Exception in task 5.0 in stage 14.0 (TID 478)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n",
      "    return f(keys, vals)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n",
      "    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 187, in wrapped\n",
      "    result = f(pd.concat(value_series, axis=1))\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_10896/1928652177.py\", line 13, in run_model\n",
      "NameError: name 'replication_id' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "23/11/16 17:38:02 ERROR Executor: Exception in task 19.0 in stage 14.0 (TID 492)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n",
      "    return f(keys, vals)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n",
      "    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 187, in wrapped\n",
      "    result = f(pd.concat(value_series, axis=1))\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_10896/1928652177.py\", line 13, in run_model\n",
      "NameError: name 'replication_id' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "23/11/16 17:38:02 ERROR Executor: Exception in task 39.0 in stage 14.0 (TID 512)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n",
      "    return f(keys, vals)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n",
      "    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 187, in wrapped\n",
      "    result = f(pd.concat(value_series, axis=1))\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_10896/1928652177.py\", line 13, in run_model\n",
      "NameError: name 'replication_id' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "23/11/16 17:38:02 ERROR Executor: Exception in task 21.0 in stage 14.0 (TID 494)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n",
      "    return f(keys, vals)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n",
      "    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 187, in wrapped\n",
      "    result = f(pd.concat(value_series, axis=1))\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_10896/1928652177.py\", line 13, in run_model\n",
      "NameError: name 'replication_id' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "23/11/16 17:38:02 ERROR Executor: Exception in task 13.0 in stage 14.0 (TID 486)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n",
      "    return f(keys, vals)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n",
      "    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 187, in wrapped\n",
      "    result = f(pd.concat(value_series, axis=1))\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_10896/1928652177.py\", line 13, in run_model\n",
      "NameError: name 'replication_id' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "23/11/16 17:38:02 ERROR Executor: Exception in task 17.0 in stage 14.0 (TID 490)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n",
      "    return f(keys, vals)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n",
      "    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 187, in wrapped\n",
      "    result = f(pd.concat(value_series, axis=1))\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_10896/1928652177.py\", line 13, in run_model\n",
      "NameError: name 'replication_id' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "23/11/16 17:38:02 ERROR Executor: Exception in task 11.0 in stage 14.0 (TID 484)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n",
      "    return f(keys, vals)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n",
      "    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 187, in wrapped\n",
      "    result = f(pd.concat(value_series, axis=1))\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_10896/1928652177.py\", line 13, in run_model\n",
      "NameError: name 'replication_id' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "23/11/16 17:38:02 WARN TaskSetManager: Lost task 33.0 in stage 14.0 (TID 506) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:02 WARN TaskSetManager: Lost task 67.0 in stage 14.0 (TID 540) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:02 WARN TaskSetManager: Lost task 3.0 in stage 14.0 (TID 476) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:02 WARN TaskSetManager: Lost task 31.0 in stage 14.0 (TID 504) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:02 WARN TaskSetManager: Lost task 7.0 in stage 14.0 (TID 480) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:02 WARN TaskSetManager: Lost task 18.0 in stage 14.0 (TID 491) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:02 WARN TaskSetManager: Lost task 45.0 in stage 14.0 (TID 518) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:02 WARN TaskSetManager: Lost task 15.0 in stage 14.0 (TID 488) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:02 WARN TaskSetManager: Lost task 50.0 in stage 14.0 (TID 523) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:02 WARN TaskSetManager: Lost task 43.0 in stage 14.0 (TID 516) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:02 WARN TaskSetManager: Lost task 36.0 in stage 14.0 (TID 509) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:02 WARN TaskSetManager: Lost task 40.0 in stage 14.0 (TID 513) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:02 WARN TaskSetManager: Lost task 54.0 in stage 14.0 (TID 527) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:02 WARN TaskSetManager: Lost task 42.0 in stage 14.0 (TID 515) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:02 WARN TaskSetManager: Lost task 57.0 in stage 14.0 (TID 530) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:02 WARN TaskSetManager: Lost task 0.0 in stage 14.0 (TID 473) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:02 WARN TaskSetManager: Lost task 30.0 in stage 14.0 (TID 503) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:02 WARN TaskSetManager: Lost task 64.0 in stage 14.0 (TID 537) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:02 WARN TaskSetManager: Lost task 55.0 in stage 14.0 (TID 528) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:02 WARN TaskSetManager: Lost task 51.0 in stage 14.0 (TID 524) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:03 WARN TaskSetManager: Lost task 47.0 in stage 14.0 (TID 520) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:03 WARN TaskSetManager: Lost task 58.0 in stage 14.0 (TID 531) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:03 WARN TaskSetManager: Lost task 56.0 in stage 14.0 (TID 529) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:03 WARN TaskSetManager: Lost task 63.0 in stage 14.0 (TID 536) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:03 WARN TaskSetManager: Lost task 14.0 in stage 14.0 (TID 487) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:03 WARN TaskSetManager: Lost task 62.0 in stage 14.0 (TID 535) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n    for batch in iterator:\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n    for series in iterator:\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n    return f(keys, vals)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 187, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_10896/1928652177.py\", line 13, in run_model\nNameError: name 'replication_id' is not defined\n\n\nJVM stacktrace:\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 27 in stage 14.0 failed 1 times, most recent failure: Lost task 27.0 in stage 14.0 (TID 500) (fe49fe119e4f executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n    for batch in iterator:\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n    for series in iterator:\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n    return f(keys, vals)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 187, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_10896/1928652177.py\", line 13, in run_model\nNameError: name 'replication_id' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n    for batch in iterator:\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n    for series in iterator:\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n    return f(keys, vals)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 187, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_10896/1928652177.py\", line 13, in run_model\nNameError: name 'replication_id' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.pyspark.jvmStacktrace.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.execution.pyspark.udf.simplifiedTraceback.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1193\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \n\u001b[1;32m   1173\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n    for batch in iterator:\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n    for series in iterator:\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n    return f(keys, vals)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 187, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_10896/1928652177.py\", line 13, in run_model\nNameError: name 'replication_id' is not defined\n\n\nJVM stacktrace:\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 27 in stage 14.0 failed 1 times, most recent failure: Lost task 27.0 in stage 14.0 (TID 500) (fe49fe119e4f executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n    for batch in iterator:\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n    for series in iterator:\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n    return f(keys, vals)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 187, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_10896/1928652177.py\", line 13, in run_model\nNameError: name 'replication_id' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n    for batch in iterator:\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n    for series in iterator:\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n    return f(keys, vals)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 187, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_10896/1928652177.py\", line 13, in run_model\nNameError: name 'replication_id' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/16 17:38:03 WARN TaskSetManager: Lost task 59.0 in stage 14.0 (TID 532) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:03 WARN TaskSetManager: Lost task 65.0 in stage 14.0 (TID 538) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:38:03 WARN TaskSetManager: Lost task 66.0 in stage 14.0 (TID 539) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.pyspark.jvmStacktrace.enabled\",True)\n",
    "spark.conf.set(\"spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled\",False)\n",
    "\n",
    "results.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Medium Detail\n",
    "\n",
    "That was too much. Let's turn off all the JVM Stacktraces, but let's still keep simplification for UDFs turned off. Looks better, but still not perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/16 17:36:04 ERROR Executor: Exception in task 15.0 in stage 11.0 (TID 380)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n",
      "    return f(keys, vals)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n",
      "    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 187, in wrapped\n",
      "    result = f(pd.concat(value_series, axis=1))\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_10896/3111444347.py\", line 17, in run_model\n",
      "NameError: name 'replication_id' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 15.0 in stage 11.0 (TID 380) (fe49fe119e4f executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n",
      "    return f(keys, vals)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n",
      "    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 187, in wrapped\n",
      "    result = f(pd.concat(value_series, axis=1))\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_10896/3111444347.py\", line 17, in run_model\n",
      "NameError: name 'replication_id' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n",
      "23/11/16 17:36:04 ERROR TaskSetManager: Task 15 in stage 11.0 failed 1 times; aborting job\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 67.0 in stage 11.0 (TID 432) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 33.0 in stage 11.0 (TID 398) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 30.0 in stage 11.0 (TID 395) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 11.0 in stage 11.0 (TID 376) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 10.0 in stage 11.0 (TID 375) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 43.0 in stage 11.0 (TID 408) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 7.0 in stage 11.0 (TID 372) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 27.0 in stage 11.0 (TID 392) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 1.0 in stage 11.0 (TID 366) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 19.0 in stage 11.0 (TID 384) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 21.0 in stage 11.0 (TID 386) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 28.0 in stage 11.0 (TID 393) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 31.0 in stage 11.0 (TID 396) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 39.0 in stage 11.0 (TID 404) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 17.0 in stage 11.0 (TID 382) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 42.0 in stage 11.0 (TID 407) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 14.0 in stage 11.0 (TID 379) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 20.0 in stage 11.0 (TID 385) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 5.0 in stage 11.0 (TID 370) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 13.0 in stage 11.0 (TID 378) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 36.0 in stage 11.0 (TID 401) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 45.0 in stage 11.0 (TID 410) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 56.0 in stage 11.0 (TID 421) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 3.0 in stage 11.0 (TID 368) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 18.0 in stage 11.0 (TID 383) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 47.0 in stage 11.0 (TID 412) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 54.0 in stage 11.0 (TID 419) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 50.0 in stage 11.0 (TID 415) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 0.0 in stage 11.0 (TID 365) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n    for batch in iterator:\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n    for series in iterator:\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n    return f(keys, vals)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 187, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_10896/3111444347.py\", line 17, in run_model\nNameError: name 'replication_id' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.pyspark.jvmStacktrace.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.execution.pyspark.udf.simplifiedTraceback.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1193\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \n\u001b[1;32m   1173\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n    for batch in iterator:\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n    for series in iterator:\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n    return f(keys, vals)\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 187, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/opt/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_10896/3111444347.py\", line 17, in run_model\nNameError: name 'replication_id' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 55.0 in stage 11.0 (TID 420) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 58.0 in stage 11.0 (TID 423) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 40.0 in stage 11.0 (TID 405) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 59.0 in stage 11.0 (TID 424) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 62.0 in stage 11.0 (TID 427) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 51.0 in stage 11.0 (TID 416) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 63.0 in stage 11.0 (TID 428) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 57.0 in stage 11.0 (TID 422) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 64.0 in stage 11.0 (TID 429) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 65.0 in stage 11.0 (TID 430) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:36:04 WARN TaskSetManager: Lost task 66.0 in stage 11.0 (TID 431) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.pyspark.jvmStacktrace.enabled\",False)\n",
    "spark.conf.set(\"spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled\",False)\n",
    "\n",
    "results.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Simplified Detail\n",
    "\n",
    "Last try: Turn off all the JVM Stacktraces, and enable  simplification for UDFs turned off.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/16 17:35:24 ERROR Executor: Exception in task 66.0 in stage 8.0 (TID 323)]\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_10896/3111444347.py\", line 17, in run_model\n",
      "NameError: name 'replication_id' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 66.0 in stage 8.0 (TID 323) (fe49fe119e4f executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_10896/3111444347.py\", line 17, in run_model\n",
      "NameError: name 'replication_id' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n",
      "23/11/16 17:35:24 ERROR TaskSetManager: Task 66 in stage 8.0 failed 1 times; aborting job\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 67.0 in stage 8.0 (TID 324) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 65.0 in stage 8.0 (TID 322) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 58.0 in stage 8.0 (TID 315) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 56.0 in stage 8.0 (TID 313) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 63.0 in stage 8.0 (TID 320) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 62.0 in stage 8.0 (TID 319) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 0.0 in stage 8.0 (TID 257) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 64.0 in stage 8.0 (TID 321) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 51.0 in stage 8.0 (TID 308) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 45.0 in stage 8.0 (TID 302) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 57.0 in stage 8.0 (TID 314) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 55.0 in stage 8.0 (TID 312) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 54.0 in stage 8.0 (TID 311) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 27.0 in stage 8.0 (TID 284) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 42.0 in stage 8.0 (TID 299) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 13.0 in stage 8.0 (TID 270) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 43.0 in stage 8.0 (TID 300) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 40.0 in stage 8.0 (TID 297) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 47.0 in stage 8.0 (TID 304) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 33.0 in stage 8.0 (TID 290) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 59.0 in stage 8.0 (TID 316) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 31.0 in stage 8.0 (TID 288) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 28.0 in stage 8.0 (TID 285) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 50.0 in stage 8.0 (TID 307) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_10896/3111444347.py\", line 17, in run_model\nNameError: name 'replication_id' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.pyspark.jvmStacktrace.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.execution.pyspark.udf.simplifiedTraceback.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1193\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \n\u001b[1;32m   1173\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_10896/3111444347.py\", line 17, in run_model\nNameError: name 'replication_id' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 11.0 in stage 8.0 (TID 268) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 1.0 in stage 8.0 (TID 258) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 19.0 in stage 8.0 (TID 276) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 36.0 in stage 8.0 (TID 293) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 18.0 in stage 8.0 (TID 275) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 21.0 in stage 8.0 (TID 278) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 20.0 in stage 8.0 (TID 277) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 15.0 in stage 8.0 (TID 272) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 7.0 in stage 8.0 (TID 264) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 10.0 in stage 8.0 (TID 267) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 3.0 in stage 8.0 (TID 260) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 17.0 in stage 8.0 (TID 274) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 5.0 in stage 8.0 (TID 262) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 30.0 in stage 8.0 (TID 287) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 14.0 in stage 8.0 (TID 271) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/16 17:35:24 WARN TaskSetManager: Lost task 39.0 in stage 8.0 (TID 296) (fe49fe119e4f executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.pyspark.jvmStacktrace.enabled\",False)\n",
    "spark.conf.set(\"spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled\",True)\n",
    "\n",
    "results.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
