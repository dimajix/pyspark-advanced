{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring Error Messages\n",
    "\n",
    "One common challenge with Spark and especially with PySpark is to make sense of error messages. In PySpark, an important part of the problem comes from the fact that two different technologies (JVM and Python) work in conjunction, and both produce stack traces. This notebook tries to provide some guidance for configuring simplified error messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"24G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Provoke an Error\n",
    "\n",
    "First we provoke an error by creating an ill-formed program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "replication_df = spark.createDataFrame(pd.DataFrame(list(range(1,1000)),columns=['replication_id'])).repartition(1000, 'replication_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "outSchema = StructType([StructField('replication_id', IntegerType(), True),\n",
    "            StructField('sil_score', DoubleType(), True),\n",
    "            StructField('num_clusters', IntegerType(), True),\n",
    "            StructField('min_samples', IntegerType(), True),\n",
    "            StructField('min_cluster_size', IntegerType(), True)])\n",
    "\n",
    "\n",
    "def run_model(df_pandas: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Return result as a pandas data frame\n",
    "    return pd.DataFrame({'replication_id': replication_id, 'sil_score': 2,\n",
    "                           'num_clusters': 3, 'min_samples': 4,\n",
    "                           'min_cluster_size': 5}, index=[0])\n",
    "\n",
    "\n",
    "results = replication_df.groupBy(\"replication_id\").applyInPandas(run_model, outSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Configuring Error Messages\n",
    "\n",
    "PySpark provides two important configuration properties `spark.sql.pyspark.jvmStacktrace.enabled` and `spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled` which control how error messages are presented to the developer. We will try different settings and compare the output in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Full Detail\n",
    "\n",
    "First we turn on the JVM Stacktrace and disable a simplification for UDFs. Evil combination, as we will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.pyspark.jvmStacktrace.enabled\",True)\n",
    "spark.conf.set(\"spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled\",False)\n",
    "\n",
    "results.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Medium Detail\n",
    "\n",
    "That was too much. Let's turn off all the JVM Stacktraces, but let's still keep simplification for UDFs turned off. Looks better, but still not perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.pyspark.jvmStacktrace.enabled\",False)\n",
    "spark.conf.set(\"spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled\",False)\n",
    "\n",
    "results.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Simplified Detail\n",
    "\n",
    "Last try: Turn off all the JVM Stacktraces, and enable  simplification for UDFs turned off.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.pyspark.jvmStacktrace.enabled\",False)\n",
    "spark.conf.set(\"spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled\",True)\n",
    "\n",
    "results.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
