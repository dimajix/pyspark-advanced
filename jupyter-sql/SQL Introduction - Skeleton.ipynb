{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c099bd72-855c-4afe-b915-20c96d13bdfb",
   "metadata": {},
   "source": [
    "# SQL Introduction\n",
    "\n",
    "SQL is a powerful language, which can get you extremly far. It is concise and sufficiently simple (once you get used to it). Spark supports SQL very well and is continuously improving the implementation by adding new features and providing better compatibility with other existing databases like Postgres etc.\n",
    "\n",
    "In this notebook we will give a solid introduction to the all important `SELECT` statements, which provide similar capabilites like all Spark DataFrame transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55d0110-afbd-4af6-9f47-59f35f1b66dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"4G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225b9734-2b5c-43ad-ae53-84e01adc6712",
   "metadata": {},
   "source": [
    "## 1 Watson Sales Product Sample Data\n",
    "\n",
    "First we load the data, which is provided as a single CSV file, which again is well supported by Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f272c98-3f69-40bc-8cbd-c3a4079f5c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = \"s3://dimajix-training/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecf8179-2578-4d78-afcc-845eb1a0ed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = spark.read.csv(basedir + \"persons_header.csv\", header=True, inferSchema=True)\n",
    "persons.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dbc5bb-4874-47cb-8dbc-73330c66d8cd",
   "metadata": {},
   "source": [
    "In order to work with SQL, we immediately register the Spark DataFrame as a temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e70ae7-8cbb-4313-b217-6ec8f815bea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons.createOrReplaceTempView(\"persons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be0de8a-d886-4f5d-b393-57925962fab3",
   "metadata": {},
   "source": [
    "# 2. Simple Transformations\n",
    "\n",
    "Following the original Spark introductionary notebook, we start with very simple transformations. These are always executed inside a `SELECT` statement. The first version uses the simplest structure:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    expression_1 AS result_column_1,\n",
    "    expression_2 AS result_column_2,\n",
    "    ...\n",
    "FROM some_table_name\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e7b694-3a4f-400b-9d83-fe7038e5b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display the result\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05905eb4-2304-417f-a874-948ced0772b8",
   "metadata": {},
   "source": [
    "Lets look at a different example where we want to create a new DataFrame with the appropriate salutation in front of the name. We achieve this by the following `SELECT` statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a365a42d-72fa-4a33-9e45-7af8b19339fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display the result\n",
    "result = spark.sql(query)\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b0e6ad-b8c4-4098-bcd0-842181d33458",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Using the `persons` temp view, perform the following operations:\n",
    "* Add a new column `status` which should be `child` if the person is younger than 18 and `adult` otherwise\n",
    "* Replace the column `name` by a new column `hashed_name` containing the hash value of the name\n",
    "* Drop the column `sex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61715c7c-7c64-44a6-98d8-4871edca1e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display the result\n",
    "result = spark.sql(query)\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dd66a1-e4aa-4bab-b7a9-204775f2e805",
   "metadata": {},
   "source": [
    "# 3. Filtering\n",
    "\n",
    "*Filtering* denotes the process of keeping only rows which meet a certain filter criteria. SQL uses a `WHERE` condition to specify which records should be kept. Note that the conditions in the `WHERE` clause refer to the original table, not to the columns specified in the `SELECT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8442a2a2-38e9-478d-8d94-42d8765e9339",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display the result\n",
    "result = spark.sql(query)\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86001f07-ffaa-4ce9-ae7a-60aa539ad035",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Perform two different filter operations (with two different result sets):\n",
    "* Select all women with a height of at least 160\n",
    "* Select all persons which are younger than 20 or older than 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b7bf9a-2a72-41e9-afa2-18b232dbb52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display the result\n",
    "result = spark.sql(query)\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a790a5-2a5d-419e-913b-767a06f65f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display the result\n",
    "result = spark.sql(query)\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6974866b-bb10-4501-8033-499f376d06c8",
   "metadata": {},
   "source": [
    "# 4. Simple Aggregations\n",
    "\n",
    "SQL supports aggregations without grouping inside a `SELECT` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5401ffe5-90df-46eb-ab7e-6f0524b9fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display the result\n",
    "result = spark.sql(query)\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e5b13c-2ef5-4b3d-b6c0-9376d8b9acad",
   "metadata": {},
   "source": [
    "# 5. Grouping & Aggregation\n",
    "\n",
    "An important class of operation is grouping and aggregation, which is provided in SQL via a `SELECT aggregation GROUP BY grouping` statement. Note that in contrast to PySpark, you need to explictily add the grouping column to the list of expressions in order to see it in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cb4d34-2f49-4d12-8776-78d2cc46a62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display the result\n",
    "result = spark.sql(query)\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7b5a2c-e61d-40db-b87b-d955934a065b",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Using the `persons` temp view, calculate the average height and the number of records per sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e62086-a7a8-4387-9e6c-e93cbfc671ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display the result\n",
    "result = spark.sql(query)\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32e4d41-a3a6-43d2-bed5-373667275f25",
   "metadata": {},
   "source": [
    "# 6. Sorting\n",
    "\n",
    "SQL also supports sorting data with the `ORDER BY` clause. For example we can sort all persons by their height as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d513c8b4-705d-4430-8b76-265d860c898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display the result\n",
    "result = spark.sql(query)\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10995a30-6b21-4d89-bade-46dc62cdd6ed",
   "metadata": {},
   "source": [
    "If nothing else is specified, SQL will sort the records in increasing order of the sort columns. If you require descending order, this can be specified by manipulating the sort column with the `DESC` modifier as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ba915b-2126-422e-9943-0c326a2ed46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display the result\n",
    "result = spark.sql(query)\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6bb5b7-f160-46b1-879a-c82d73761ee6",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "As an exercise we want to sort all persons first by their sex and then by their descening age. Sorting by multiple columns can easily be achieved by specifying multiple columns separated with a comma in the `ORDER BY` clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac6ebde-4cc4-49c3-97a2-8e74005b8264",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display the result\n",
    "result = spark.sql(query)\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf04e481-1e1f-43f5-a0ff-48ca1c20cf74",
   "metadata": {},
   "source": [
    "# 7. Joining Data\n",
    "\n",
    "Every relation algebra also contains join operations which lets you combine multiple tables by a matching criterion. SQL also supports joins of multiple tables/views. In order to shed some light on that, we need a second DataFrame in addition to the `persons` temp view. Therefore we load some address data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6a879e-af4a-4cc2-a0a3-5d8d9034e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = spark.read.json(basedir + \"addresses.json\")\n",
    "addresses.createOrReplaceTempView(\"addresses\")\n",
    "addresses.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7295a5-06f9-4ca6-bb49-fca40d42f0f7",
   "metadata": {},
   "source": [
    "Now that we have the `addresses` view, we want to combine it with the `persons` view such that the city of every person is added as a new column. This is achieved by the `JOIN` clause which together with two parameters: The first parameter specifies the second DataFrame to join with, and the second parameter specifies the join condition. In this case we want to join all records, where the `name` column matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9850a703-8cd2-4242-bd21-8d290cab279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display the result\n",
    "result = spark.sql(query)\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59b74c5-7ea9-46be-b4c7-31478b3ce165",
   "metadata": {},
   "source": [
    "Let me make some relevant remarks:\n",
    "* The resulting DataFrame now contains two `name` columns - one comes from the `persons` view, the other from the `addresses` view. Since the join condition could have used some more complex expression, this behaviour is only logical since SQL cannot assume that all joins simply use directly some column value. For example we could also have transformed the column on the fly by converting the name to upper case directly inside the join condition.\n",
    "* The result contains only persons where an address was found, although the original `persons` view contained more persons.\n",
    "* There are no records of addresses without any person, although the `addresses` view contains information about some persons not available in the `persons` DataFrame.\n",
    "\n",
    "So let us first address the first observation. We can easily get rid of the copied `name` column by either performing an explicit select of the desired columns, or by dropping the duplicate columns. The duplicate `name` columns can be addressed by their alias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e9f2f-a7ac-49e9-91d1-2ebf191027a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display the result\n",
    "result = spark.sql(query)\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bae5994-407f-4b52-ae0f-1dfc510474f8",
   "metadata": {},
   "source": [
    "Now let us explain the last two observations. These are due to the used join type, which was a so called *inner* join. In this case, only records with information from both sides of the `JOIN` are included in the result.\n",
    "\n",
    "In addition to the *inner* join, SQL also supports some additional joins:\n",
    "* *outer join* will contain records for all elements from both DataFrames. If either the left or right DataFrames doesn't contain any information, the result will contain `None` values (= `NULL` values) for the corresponding columns.\n",
    "* In a *right join*, the second view (the right view) as specified as an argument is the leading element. The result will contain records for every record in that view.\n",
    "* In a *left join*, the first view (the left view) as specified as the object iteself is the leading element. The result will contain records for every record in that view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fe66ac-c272-421b-a8c9-f6ad9fff8310",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display the result\n",
    "result = spark.sql(query)\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee4c05b-5007-4bf1-8020-e3512d5a74b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display the result\n",
    "result = spark.sql(query)\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbb3de3-cd4f-49bd-9863-3ca08684fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display the result\n",
    "result = spark.sql(query)\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99a5611-30a3-458c-9b4f-4c3b3837945b",
   "metadata": {},
   "source": [
    "# 8. Combining all Features\n",
    "\n",
    "A general `SELECT` statement looks as follows (we omit CTEs for the moment)\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    expression_1 AS column_1,\n",
    "    expression_2 AS column_2,\n",
    "    ...\n",
    "FROM table_1\n",
    "JOIN table_2 ON ...\n",
    "JOIN table_3 ON ...\n",
    "WHERE ...\n",
    "GROUP BY ...\n",
    "HAVING ...\n",
    "ORDER BY ...\n",
    "LIMIT n\n",
    "```\n",
    "\n",
    "The different parts need to be specified in exactly this order. And they are also evaluated in exactly this order, which means:\n",
    "* In the first step, all records are read from the table specified in the `FROM` clause.\n",
    "* Then `JOIN` clauses are executed by reading the appropriate tables and matching the records. There can be multiple `JOIN` clauses.\n",
    "* Then the `WHERE` clause is executed, i.e. all records are filtered.\n",
    "* Then the `GROUP BY` clause is executed\n",
    "* Then the `HAVING` clause is executed. It serves as an additional filter criteria *after* grouped aggregation.\n",
    "* Now all column expressions in the `SELECT` part are evaulated.\n",
    "* The result set is sorted according to the `ORDER BY` clause\n",
    "* The first `n` records are taken accoring to the `LIMIT` clause\n",
    "\n",
    "Of course, the SQL optimizer may execute things in a different order. But the conceptional ordering is exactly as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1cac1f-ad6c-4d47-b447-269aeb4bd27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display the result\n",
    "result = spark.sql(query)\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c25b55-23a6-460d-bcd3-153256f4b3c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
