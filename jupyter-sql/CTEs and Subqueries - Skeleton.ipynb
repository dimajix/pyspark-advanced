{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e68b24c5-e2f3-4b76-b741-494a7f0c567b",
   "metadata": {},
   "source": [
    "# SQL Common Table Expressions\n",
    "\n",
    "SQl is a very powerful language that allows the construction of very complex queries. But SQL only provides very limited support for structuting query in order to increase readability. So called \"Common Table Expressions\" (CTEs) are a common way to split up large SQL queries into smaller and more manageable chunks. This approach works with all SQL databases, not only with Spark SQL.\n",
    "\n",
    "**Attention!** In Spark, CTEs will be optimized away, such that there is no difference in execution speed. Different (relational) databases might handle CTEs differently and they might represent an optimization barrier. Please consult the manual of your database before blindlingly using CTEs!\n",
    "\n",
    "For this notebook, we will pick up the weather example and use Spark SQL to calculate some aggregates per year and country. We will not provide the most efficient implementation in order to make the whole structure more complicated to show the power of common table expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91552a1-0306-499c-926f-02297fc0d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"4G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7978a76-0dd3-4fca-8316-1f9fe4925cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "storageLocation = \"s3://dimajix-training/data/weather\"\n",
    "#storageLocation = \"/dimajix/data/weather-noaa-sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b68288a-94c3-422e-aa8a-ddb9bc7cc705",
   "metadata": {},
   "source": [
    "# 1. Register Temp Views\n",
    "\n",
    "In a first step, we will load the input data via Spark. In order to use Spark SQL, we will also immediately register the loaded DataFrames as temp views."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bffc30d-39ee-4cf0-9b24-51719ce03246",
   "metadata": {},
   "source": [
    "## 1.1 Load Raw Data\n",
    "\n",
    "Like before, we will load the raw measurement data and register it as a temp view called `raw_weather`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69aee34-b4fd-4cc6-87be-833172500216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# Read in all years, store them in an Python array\n",
    "raw_weather_per_year = [spark.read.text(storageLocation + \"/\" + str(i)).withColumn(\"year\", f.lit(i)) for i in range(2003,2006)]\n",
    "\n",
    "# Union all years together\n",
    "raw_weather = reduce(lambda l,r: l.union(r), raw_weather_per_year)                        \n",
    "\n",
    "# Register Spark DataFrame as named temporary view called 'raw_weather'\n",
    "raw_weather.createOrReplaceTempView(\"raw_weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe1a6a2-0881-4e14-9438-ba988a128754",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display first 10 records\n",
    "spark.sql(\"SELECT * FROM raw_weather LIMIT 10\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15a2383-02cd-4c23-8e26-0d47b4269583",
   "metadata": {},
   "source": [
    "## 2.2 Load Master Data\n",
    "\n",
    "Now we will load the stations master data and register a temp view called `stations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252be1c7-1562-45f4-a6d7-319280cf36fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = spark.read.csv(storageLocation + \"/isd-history\", header=True)\n",
    "\n",
    "# Register Spark DataFrame as named temporary view called 'stations'\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16dc731-065b-4529-8bac-06011475974b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display first 10 records\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9e0d77-9d48-4b82-95a0-dda345dc461b",
   "metadata": {},
   "source": [
    "# 2. Using Intermediate Tables\n",
    "\n",
    "Let us first perform the weather analysis step by step using more intermediate temporary tables. Of course, this approach is very specific to Apache Spark and not directly available in other databases. But additional tools like [dbt](https://getdbt.com) will provide similar capabilities for generic SQL databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6a5b14-3632-4a49-9745-e5a2e61695f2",
   "metadata": {},
   "source": [
    "## 2.1 Extract Measurements\n",
    "\n",
    "In the first step, we need to extract all the interesting attributes from the raw measurement data. We do this by using `substring` and approproiate casts and/or scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6276e53-ede3-40aa-ab51-31bb5c9d0774",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT\n",
    "        year,\n",
    "        substring(value,5,6) AS usaf,\n",
    "        substring(value,11,5) AS wban,\n",
    "        substring(value,16,8) AS `date`,\n",
    "        substring(value,24,4) AS `time`,\n",
    "        substring(value,42,5) AS report_type,\n",
    "        substring(value,61,3) AS wind_direction,\n",
    "        substring(value,64,1) AS wind_direction_qual,\n",
    "        substring(value,65,1) AS wind_observation,\n",
    "        CAST(substring(value,66,4) AS FLOAT) / 10.0 AS wind_speed,\n",
    "        substring(value,70,1) AS wind_speed_qual,\n",
    "        CAST(substring(value,88,5) AS FLOAT) / 10.0 AS air_temperature,\n",
    "        substring(value,93,1) AS air_temperature_qual\n",
    "    FROM raw_weather\n",
    "\"\"\"\n",
    "\n",
    "# Create a Spark DataFrame for the SQL query above\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Register the DataFrame as a temp view with name 'weather'\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Display the first 10 records from the newly created temp view 'weather'\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e6171-494c-4c77-9c47-26fc761a5f09",
   "metadata": {},
   "source": [
    "## 2.2 Join Data\n",
    "\n",
    "In the next step, we join the extracted measurement data with the stations master data. We use the two columns `usaf` and `wban` for joining. Since the join represents an enrichment of the measurements, we chose a *left join*. In order to access the extracted measurements, we can simply use the `weather` temp view we just created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9794d9-036d-4a7d-b326-7a2eceff1a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    -- YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Create a Spark DataFrame for the SQL query above\n",
    "joined_weather = spark.sql(query)\n",
    "\n",
    "# Register the DataFrame as a temp view with name 'joined_weather'\n",
    "joined_weather.createOrReplaceTempView(\"joined_weather\")\n",
    "\n",
    "# Display first 10 records from the newly created temp view 'joined_weather'\n",
    "spark.sql(\"SELECT * FROM joined_weather LIMIT 10\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c533a5dd-966a-4ea4-a697-b39e72559f34",
   "metadata": {},
   "source": [
    "## 2.3 Aggregate Temperature\n",
    "\n",
    "We now aggregate the min, max and average air temperature per country and year. We will use a simple `WHERE` condition to ignore all records with invalid air temperature. Of course, we remember a more efficient overall solution. In order to make the example more complicated, we chose to ignore the simpler solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bba304-de14-4fb3-a042-782977fc91f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    -- YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Create a Spark DataFrame for the SQL query above and register it as a temp view called 'year_country_temperature'\n",
    "year_country_temperature = spark.sql(query)\n",
    "year_country_temperature.createOrReplaceTempView(\"year_country_temperature\")\n",
    "\n",
    "# Display first 10 records from the newly created temp view\n",
    "spark.sql(\"SELECT * FROM year_country_temperature LIMIT 10\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4612aea3-c22b-4689-8dd7-2af062992076",
   "metadata": {},
   "source": [
    "## 2.4 Aggregate Wind (Exercise)\n",
    "\n",
    "As the next step, create a similar query for calculating the min, max and average wind speed per country and year. The relevant columns are `wind_speed` and `wind_speed_qual`. Register the resulting DataFrame as a temp view called `year_country_wind`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807855fa-accc-4803-89f6-4115193c9457",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    -- YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Create a Spark DataFrame for the SQL query above and register it as a temp view called 'year_country_wind'\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Display first 10 records from the newly created temp view\n",
    "spark.sql(\"SELECT * FROM year_country_wind LIMIT 10\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de8d3fc-d997-47d6-add2-e86d193c6890",
   "metadata": {},
   "source": [
    "## 2.5 Join Final Result\n",
    "\n",
    "So far, we have created the two temp views `year_country_temperature` and `year_country_wind`. Both contain some aggregated attributes per country and year. Now join together both data frames using the columns `country` and `year`. Here we chose an *outer join*, since both sides of the join could provide some relevant information without the other side. The final result should contain the following columns\n",
    "\n",
    "* `year`\n",
    "* `country`\n",
    "* `min_wind_speed`\n",
    "* `max_wind_speed`\n",
    "* `avg_wind_speed`\n",
    "* `min_air_temperature`\n",
    "* `max_air_temperature`\n",
    "* `avg_air_temperature`\n",
    "\n",
    "The function SQL function `COALESCE` can be used to merge the columns `year` and `country` from the left and right sides of the joint into two final columns `year` and `country`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d61c1-78b7-4b43-94f0-41832fb2aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    -- YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "result = spark.sql(query)\n",
    "# Convert the result to a Pandas dataframe - this time without a 'limit' \n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af21064-a458-4909-940f-da342c48652d",
   "metadata": {},
   "source": [
    "# 3.0 SQL Subqueries\n",
    "\n",
    "When trying to combine all logic into a single query, you can increase readability by using a SQL subquery. Actually, subqueries can be used in various places. We will look at different examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3b34dc-e11c-4036-a291-b92ef0391451",
   "metadata": {},
   "source": [
    "## 3.1 Subqueries in `FROM` clauses\n",
    "\n",
    "In our first example, we use a subquery inside the `FROM` clause. The pattern looks as follows:\n",
    "\n",
    "```sql\n",
    "    SELECT\n",
    "        ...\n",
    "    FROM (\n",
    "        -- subquery\n",
    "        SELECT\n",
    "            ...\n",
    "    ) AS my_alias\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f9699-0b7c-4315-be8f-dab40f7d9cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    year,\n",
    "    CTRY AS country,\n",
    "    min(air_temperature) AS min_air_temperature,\n",
    "    max(air_temperature) AS max_air_temperature,\n",
    "    avg(air_temperature) AS avg_air_temperature\n",
    "FROM (\n",
    "    -- Extract measurements\n",
    "    # YOUR CODE HERE\n",
    ") AS weather\n",
    "\n",
    "-- Join measurements and master data\n",
    "LEFT JOIN stations ON weather.usaf = stations.usaf AND weather.wban = stations.wban\n",
    "-- Only keep valid temperature\n",
    "WHERE air_temperature_qual = '1'\n",
    "-- Perform grouping\n",
    "GROUP BY year,CTRY\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and display result\n",
    "result = spark.sql(query)\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6133fca-0dee-42f6-b085-d0692730e528",
   "metadata": {},
   "source": [
    "## 3.2 Scalar Subqueries\n",
    "\n",
    "SQL also supports so callaed *scalar subqueries*, which return a single value. For example, if we wanted to add a column containing the overall average temperature, this could be done as followed (using the `joined_weather` view for simplifying the query)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d55f166-2ad6-4be6-8ec2-2d86534b759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT\n",
    "        year,\n",
    "        country,\n",
    "        min(air_temperature) AS min_air_temperature,\n",
    "        max(air_temperature) AS max_air_temperature,\n",
    "        avg(air_temperature) AS avg_air_temperature,\n",
    "        -- YOUR CODE HERE\n",
    "    FROM joined_weather\n",
    "    WHERE air_temperature_qual = '1'\n",
    "    GROUP BY year,country\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and display result\n",
    "result = spark.sql(query)\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e47a1d9-c8d6-4b42-9d6f-12b9c1f56d75",
   "metadata": {},
   "source": [
    "### Execution Plan\n",
    "\n",
    "Let's inspect the execution plan, which should also include the subquery as a separate stage/job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7936ef4f-a62c-43d5-832a-e21bdcdc0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0c5404-21a6-4b8d-9d1f-2a5b648e4b20",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Now implement a similar query, which only shows the years where the average temperature is higher than the overall average temperature. Note that you need to filter *after* the aggregation (in order to access the average per country and year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad224bd6-58e9-4213-a928-deacd79501be",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    -- YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and display result\n",
    "result = spark.sql(query)\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8037e821-14f5-4a81-b91a-d17e6fead590",
   "metadata": {},
   "source": [
    "## 3.3 Correlated Subquery\n",
    "\n",
    "A special case of a scalar subquery is the *correlated subquery*, which performs its calculation dependent on values from the main query. For example, the following query calculates the overall average temperature per country and adds it as a separate column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d200ce5b-e35a-46b2-9e85-5fe80b1e7d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT\n",
    "        w.year,\n",
    "        w.country,\n",
    "        w.air_temperature,\n",
    "        -- YOUR CODE HERE\n",
    "    FROM joined_weather w\n",
    "    WHERE w.air_temperature_qual = '1'\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and display result\n",
    "result = spark.sql(query)\n",
    "result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2974c8a-9d75-4595-b293-cfbcc596950a",
   "metadata": {},
   "source": [
    "### Execution Plan\n",
    "\n",
    "By looking at the execution plan, it becomes clear, that Spark transforms the correlated query into an equivalent query, which contains a grouped agreggregation followed by a join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8d6f60-a16f-4dfb-992f-7a9571611818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ba0f6-3b61-4b21-915d-112b575489af",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Now add a column containing the maximum temperature over all weather stations of the current year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06474112-d04c-45be-bd92-7a1408a11489",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT\n",
    "        w.year,\n",
    "        w.country,\n",
    "        w.air_temperature,\n",
    "        -- YOUR CODE HERE\n",
    "    FROM joined_weather w\n",
    "    WHERE w.air_temperature_qual = '1'\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and display result\n",
    "result = spark.sql(query)\n",
    "result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b44c9f0-7aa2-44b5-9f5b-1f8af59af3dc",
   "metadata": {},
   "source": [
    "# 4.0 Common Table Expression\n",
    "\n",
    "The following section will provide an SQL query, which essentially performs the very same calculation. But we implement all steps within a single query using *Common Table Expressions*, CTEs. A CTE can be seen as a local temp view, which is locally scoped to be only accessible within a single query. Therefore, a CTE can be interpreted as some sort of *table valued function* (but without a parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfbd90a-b5a7-46b1-a143-6435480d1abf",
   "metadata": {},
   "source": [
    "## 4.1 Air Temperature\n",
    "\n",
    "In order to provide an instructive example, let's start with a simpler query which only calculates the metrics related to the air temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726c29e1-a327-42d7-a147-15ad7a52fe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "-- Extract measurements\n",
    "-- YOUR CODE HERE\n",
    "\n",
    "-- Join measurements and master data\n",
    "-- YOUR CODE HERE\n",
    "\n",
    "-- Calculate min/max/avg air temperature per county and year\n",
    "-- YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and display result\n",
    "result = spark.sql(query)\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a098e675-5d58-4aa0-b9ed-2c2e8d6da2ed",
   "metadata": {},
   "source": [
    "## 4.2 Full Query (Exercise)\n",
    "\n",
    "In the next and final step, we construct a more complicated query, which also calculates the wind speed metrics and which performs the final join between the wind speed and air temperature metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c6a71-8f18-4932-90af-0cf4a880caa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "-- Extract measurements\n",
    "-- YOUR CODE HERE\n",
    "\n",
    "-- Join measurements and master data\n",
    "-- YOUR CODE HERE\n",
    "\n",
    "-- Aggregate air temperature\n",
    "-- YOUR CODE HERE\n",
    "\n",
    "-- Aggregate wind speed\n",
    "-- YOUR CODE HERE\n",
    "\n",
    "-- Join aggeragted wind speed and aggregated air temperature\n",
    "-- YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and display result\n",
    "result = spark.sql(query)\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacdeb96-a656-4632-97a8-ff6e050df38b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
