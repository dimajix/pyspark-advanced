{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Query Execution for Skewed Joins\n",
    "\n",
    "Skewed data sets are a big issue, especially for join operations. Before Spark 3, it was difficult to optimize such situations, which could easily end up either in very long running jobs where only a single task dominates the overall runtime or even in OOMs. In order to cope with such situations, people increased the number of Spark partitions via `spark.sql.shuffle.partitions` or salted the join keys (i.e. added random bits). While the first approach will affect all Spark operations, the second one is complex to implement.\n",
    "\n",
    "Luckily with Spark 3 the situation improved a lot, thanks to the new AQE (Adaptive Query Execution). This Spark internal framework allows Spark to dynamically change the execution plan of a query once some parts are executed and additional information is available to the query planner. And this framework provides support for skewed joins, in which case it will automatically split up huge partitions into smaller ones and still correctly execute the join operation.\n",
    "\n",
    "Let's have a look how this works. This notebook is heavily influenced by [a Medium article by Mario Cartia](https://medium.com/agile-lab-engineering/spark-3-0-first-hands-on-approach-with-adaptive-query-execution-part-3-ea6012a8f216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "import pandas as pd\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"24G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Create Skewed Test Data\n",
    "\n",
    "First we need to have a skewed data set. We create our own data set about cars. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Create Car Models\n",
    "\n",
    "First we create a small data set with car models, which will serve as the join key of two additional tables, which will be created afterwards. We will also implement a small function `random_make` which returns a random entry of the table - but with a small twist. With a chance of over 50%, the returned car model will be a Ford, which will later be responsible for the skewed partition. We also implement an additional function `random_config` which simply creates a random string representing the configuration of a specific car (colour, sports package, interior, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "makes = [\n",
    "    \"Ford\",\n",
    "    \"Nissan\",\n",
    "    \"Hyundai\",\n",
    "    \"Suzuki\",\n",
    "    \"Mercedes-Benz\",\n",
    "    \"Fiat\",\n",
    "    \"Skoda\",\n",
    "    \"Kia\",\n",
    "    \"Vw\",\n",
    "    \"Porsche\"\n",
    "]\n",
    "\n",
    "\n",
    "# Helper function to create random make & model\n",
    "def random_make(id):\n",
    "    is_ford = (id % 2 == 1)\n",
    "    if is_ford:\n",
    "        return makes[0]\n",
    "    else:\n",
    "        rnd = random.randint(0, len(makes) - 1)\n",
    "        return makes[rnd]\n",
    "\n",
    "def random_config(id):\n",
    "    letters = string.ascii_uppercase\n",
    "    reg = \"\"\n",
    "    for number in range(8):\n",
    "          reg += random.choice(letters)\n",
    "\n",
    "    return reg\n",
    "\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(random_make(i))\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(random_config(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas UDF\n",
    "\n",
    "Now create some Pandas UDFs from the previous pure Python functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf('string')\n",
    "def random_make_udf(ids:pd.Series) -> pd.Series:\n",
    "    return ids.apply(random_make)\n",
    "\n",
    "@pandas_udf('string')\n",
    "def random_config_udf(ids:pd.Series) -> pd.Series:\n",
    "    return ids.apply(random_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame / Table\n",
    "\n",
    "Now create a DataFrame containing lots of cars. These represent specific configurations from specific manufacturers. Each line is identified by an `id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cars = 100000000\n",
    "\n",
    "cars = spark.range(0,num_cars).select(\n",
    "        f.col(\"id\").alias(\"car_id\"),\n",
    "        random_make_udf('id').alias('make'),\n",
    "        random_config_udf('id').alias('config')\n",
    "    )\n",
    "\n",
    "cars.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Create Sales Table\n",
    "\n",
    "Now we create an additional table containing car informations, again highly skewed. We use a fictional `sales` table, where each line has again a `sales_id`, a reference to a speciifc car configuration via `car_id` and a sales date. Again, we create a highly skewed table by assigning 80% of all entries to the car with id `100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sales = 100000000\n",
    "\n",
    "sales = spark.range(0,num_sales).select(\n",
    "        f.col(\"id\").alias(\"sales_id\"),\n",
    "        f.date_add(f.current_date(), -(f.rand() * 360).cast('int')).alias(\"sales_date\"),\n",
    "        f.when(f.rand() < 0.8, 100).otherwise((f.rand()*num_cars).cast('long')).alias(\"car_id\")\n",
    "    )\n",
    "\n",
    "sales.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Peform JOIN\n",
    "\n",
    "Finally we will join the two tables on the join key `id` / `car_id`. Note that the join key is not unique in the second DataFrame and note that the join key is highly skewed in both DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Unoptimized Skewed Join\n",
    "\n",
    "First we will use a non-adaptive join as the performance baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable automatic broadcast. Default: 10MB\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "# Enable AQE. Default: False\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Count the number of sales per manufacterer. You need to join the sales table with the cars table and then count the number of records per make\n",
    "# YOUR CODE HERE\n",
    "\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Plan\n",
    "\n",
    "Let's inspect the execution plan after the query has succeeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Optimized Skewed Join (AQE)\n",
    "\n",
    "Now we will enable the Adaptive Query Execution in Spark and configure some thresholds such that it will work nicely with our rather small data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable AQE. Ddefault: False\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
    "# Enable skewed join optimization. Default: True\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", True)\n",
    "\n",
    "# The advisory size in bytes of the shuffle partition during adaptive optimization (when spark.sql.adaptive.enabled is true).\n",
    "spark.conf.set(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"8KB\")\n",
    "# A partition is considered as skewed if its size is larger than this factor multiplying the median partition size and also larger than 'spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes'. Default: 5\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionFactor\", 2)\n",
    "# A partition is considered as skewed if its size in bytes is larger than this threshold and also larger than 'spark.sql.adaptive.skewJoin.skewedPartitionFactor' multiplying the median partition size. Default: 256MB\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"16KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Perform the very same query, but now with a different configuration\n",
    "# YOUR CODE HERE\n",
    "\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Plan\n",
    "\n",
    "Again let's inspect the execution plan after the query has succeeded. Note that it will look significantly different now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
