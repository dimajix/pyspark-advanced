{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Query Execution for Skewed Joins\n",
    "\n",
    "Skewed data sets are a big issue, especially for join operations. Before Spark 3, it was difficult to optimize such situations, which could easily end up either in very long running jobs where only a single task dominates the overall runtime or even in OOMs. In order to cope with such situations, people increased the number of Spark partitions via `spark.sql.shuffle.partitions` or salted the join keys (i.e. added random bits). While the first approach will affect all Spark operations, the second one is complex to implement.\n",
    "\n",
    "Luckily with Spark 3 the situation improved a lot, thanks to the new AQE (Adaptive Query Execution). This Spark internal framework allows Spark to dynamically change the execution plan of a query once some parts are executed and additional information is available to the query planner. And this framework provides support for skewed joins, in which case it will automatically split up huge partitions into smaller ones and still correctly execute the join operation.\n",
    "\n",
    "Let's have a look how this works. This notebook is heavily influenced by [a Medium article by Mario Cartia](https://medium.com/agile-lab-engineering/spark-3-0-first-hands-on-approach-with-adaptive-query-execution-part-3-ea6012a8f216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"24G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Create Skewed Test Data\n",
    "\n",
    "First we need to have a skewed data set. We create our own data set about cars. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Create Car Models\n",
    "\n",
    "First we create a small data set with car models, which will serve as the join key of two additional tables, which will be created afterwards. We will also implement a small function `random_make_model` which returns a random entry of the table - but with a small twist. With a chance of over 50%, the returned car model will be a Ford Fiesta, which will later be responsible for the skewed partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import random\n",
    "\n",
    "MakeModel = Row(\"make\", \"model\")\n",
    "\n",
    "make_models = [\n",
    "    MakeModel(\"FORD\", \"FIESTA\"),\n",
    "    MakeModel(\"NISSAN\", \"QASHQAI\"),\n",
    "    MakeModel(\"HYUNDAI\", \"I20\"),\n",
    "    MakeModel(\"SUZUKI\", \"SWIFT\"),\n",
    "    MakeModel(\"MERCEDES-BENZ\", \"E CLASS\"),\n",
    "    MakeModel(\"FIAT\", \"500\"),\n",
    "    MakeModel(\"SKODA\", \"OCTAVIA\"),\n",
    "    MakeModel(\"KIA\", \"RIO\"),\n",
    "    MakeModel(\"VW\", \"TIGUAN\"),\n",
    "    MakeModel(\"PORSCHE\", \"911\"),\n",
    "]\n",
    "\n",
    "\n",
    "# Helper function to create random make & model\n",
    "def random_make_model():\n",
    "    is_ford = random.choice([True, False])\n",
    "    if is_ford:\n",
    "        return make_models[0]\n",
    "    else:\n",
    "        rnd = random.randint(0, len(make_models) - 1)\n",
    "        return make_models[rnd]\n",
    "    \n",
    "random_make_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Create First Car Table\n",
    "\n",
    "The first table simply contains car registrations, which will randomly generate a registration and pick a random car model from above. Remember that Fird Fiestas will be over-represented by over 50%, so the data set is already skewed in regards to the car model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "Table1 = Row(\"registration\", \"make\", \"model\", \"engine_size\")\n",
    "\n",
    "def random_t1():\n",
    "    def random_registration():\n",
    "        letters = string.ascii_uppercase\n",
    "        reg = \"\"\n",
    "        for number in range(8):\n",
    "              reg += random.choice(letters)\n",
    "\n",
    "        return reg\n",
    "\n",
    "    def random_engine_size():\n",
    "        return 1 + random.randint(0,9)/10.0\n",
    "    \n",
    "    make_model = random_make_model()\n",
    "    return Table1( random_registration(), make_model.make, make_model.model, random_engine_size())\n",
    "\n",
    "random_t1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame\n",
    "\n",
    "With the definitions above, let's create a Spark DataFrame containing random car registrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect DataFrame\n",
    "\n",
    "Now let's count the occurances of each car model. We expect that the Ford Fiesta will make up over 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Create Second Table\n",
    "\n",
    "Now we create an additional table containing car informations, again highly skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table2 = Row(\"make\", \"model\", \"engine_size\", \"sales_price\")\n",
    "\n",
    "def random_t2():\n",
    "    def random_engine_size():\n",
    "        return 1 + random.randint(0,9)/10.0\n",
    "\n",
    "    def random_sales_price():\n",
    "        return random.randint(10000, 40000)\n",
    "    \n",
    "    make_model = random_make_model()\n",
    "    return Table2(make_model.make, make_model.model, random_engine_size(), random_sales_price())\n",
    "\n",
    "random_t2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = spark.createDataFrame([random_t2() for i in range(200000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2.groupBy([\"make\", \"model\"]).count().orderBy(f.col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Peform JOIN\n",
    "\n",
    "Finally we will join the two tables on the join keys `make` and `model`. Note that the join keys are not unique in neither DataFrame and note that the join key is highly skewed in both DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Unoptimized Skewed Join\n",
    "\n",
    "First we will use a non-adaptive join as the performance baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable automatic broadcast. Default: 10MB\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "# Enable AQE. Default: False\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = t1.join(t2, [\"make\", \"model\"]) \\\n",
    "    .filter(f.abs(t1[\"engine_size\"] - t2[\"engine_size\"]) < 0.1) \\\n",
    "    .groupBy(\"registration\") \\\n",
    "    .agg(f.avg(\"sales_price\").alias(\"avg_sales_price\"))\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Optimized Skewed Join (AQE)\n",
    "\n",
    "Now we will enable the Adaptive Query Execution in Spark and configure some thresholds such that it will work nicely with our rather small data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable AQE. Ddefault: False\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
    "# Enable skewed join optimization. Default: True\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", True)\n",
    "\n",
    "# The advisory size in bytes of the shuffle partition during adaptive optimization (when spark.sql.adaptive.enabled is true).\n",
    "spark.conf.set(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"8KB\")\n",
    "# A partition is considered as skewed if its size is larger than this factor multiplying the median partition size and also larger than 'spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes'. Default: 5\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionFactor\", 2)\n",
    "# A partition is considered as skewed if its size in bytes is larger than this threshold and also larger than 'spark.sql.adaptive.skewJoin.skewedPartitionFactor' multiplying the median partition size. Default: 256MB\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"16KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = t1.join(t2, [\"make\", \"model\"]) \\\n",
    "    .filter(f.abs(t1[\"engine_size\"] - t2[\"engine_size\"]) < 0.1) \\\n",
    "    .groupBy(\"registration\") \\\n",
    "    .agg(f.avg(\"sales_price\").alias(\"avg_sales_price\"))\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
