{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Query Execution for Skewed Joins\n",
    "\n",
    "Skewed data sets are a big issue, especially for join operations. Before Spark 3, it was difficult to optimize such situations, which could easily end up either in very long running jobs where only a single task dominates the overall runtime or even in OOMs. In order to cope with such situations, people increased the number of Spark partitions via `spark.sql.shuffle.partitions` or salted the join keys (i.e. added random bits). While the first approach will affect all Spark operations, the second one is complex to implement.\n",
    "\n",
    "Luckily with Spark 3 the situation improved a lot, thanks to the new AQE (Adaptive Query Execution). This Spark internal framework allows Spark to dynamically change the execution plan of a query once some parts are executed and additional information is available to the query planner. And this framework provides support for skewed joins, in which case it will automatically split up huge partitions into smaller ones and still correctly execute the join operation.\n",
    "\n",
    "Let's have a look how this works. This notebook is heavily influenced by [a Medium article by Mario Cartia](https://medium.com/agile-lab-engineering/spark-3-0-first-hands-on-approach-with-adaptive-query-execution-part-3-ea6012a8f216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/11 13:44:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://6f64c2a1515a:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5103eef730>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "import pandas as pd\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"24G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Create Skewed Test Data\n",
    "\n",
    "First we need to have a skewed data set. We create our own data set about cars. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Create Car Models\n",
    "\n",
    "First we create a small data set with car models, which will serve as the join key of two additional tables, which will be created afterwards. We will also implement a small function `random_make` which returns a random entry of the table - but with a small twist. With a chance of over 50%, the returned car model will be a Ford, which will later be responsible for the skewed partition. We also implement an additional function `random_config` which simply creates a random string representing the configuration of a specific car (colour, sports package, interior, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nissan\n",
      "Ford\n",
      "Suzuki\n",
      "Ford\n",
      "Skoda\n",
      "Ford\n",
      "Suzuki\n",
      "Ford\n",
      "Fiat\n",
      "Ford\n",
      "XFRRWTAU\n",
      "ILEMKIMB\n",
      "SBTCTAMN\n",
      "BPPSDIDX\n",
      "IPKYDGGN\n",
      "QRBGPXJF\n",
      "JFWOYDQZ\n",
      "SMMYZSEV\n",
      "FEYASBQW\n",
      "ASLMKRWO\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "makes = [\n",
    "    \"Ford\",\n",
    "    \"Nissan\",\n",
    "    \"Hyundai\",\n",
    "    \"Suzuki\",\n",
    "    \"Mercedes-Benz\",\n",
    "    \"Fiat\",\n",
    "    \"Skoda\",\n",
    "    \"Kia\",\n",
    "    \"Vw\",\n",
    "    \"Porsche\"\n",
    "]\n",
    "\n",
    "\n",
    "# Helper function to create random make & model\n",
    "def random_make(id):\n",
    "    is_ford = (id % 2 == 1)\n",
    "    if is_ford:\n",
    "        return makes[0]\n",
    "    else:\n",
    "        rnd = random.randint(0, len(makes) - 1)\n",
    "        return makes[rnd]\n",
    "\n",
    "def random_config(id):\n",
    "    letters = string.ascii_uppercase\n",
    "    reg = \"\"\n",
    "    for number in range(8):\n",
    "          reg += random.choice(letters)\n",
    "\n",
    "    return reg\n",
    "\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(random_make(i))\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(random_config(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas UDF\n",
    "\n",
    "Now create some Pandas UDFs from the previous pure Python functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf('string')\n",
    "def random_make_udf(ids:pd.Series) -> pd.Series:\n",
    "    return ids.apply(random_make)\n",
    "\n",
    "@pandas_udf('string')\n",
    "def random_config_udf(ids:pd.Series) -> pd.Series:\n",
    "    return ids.apply(random_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame / Table\n",
    "\n",
    "Now create a DataFrame containing lots of cars. These represent specific configurations from specific manufacturers. Each line is identified by an `id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_id</th>\n",
       "      <th>make</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Vw</td>\n",
       "      <td>RYVUNGAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ford</td>\n",
       "      <td>FAEJQCKM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Porsche</td>\n",
       "      <td>RMNLMDSR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ford</td>\n",
       "      <td>SDWYNHMB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Suzuki</td>\n",
       "      <td>NMKSDQBR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Ford</td>\n",
       "      <td>BIYGLVPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Ford</td>\n",
       "      <td>KDOCUHAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Ford</td>\n",
       "      <td>UFUJSIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Kia</td>\n",
       "      <td>OVJWCCZM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Ford</td>\n",
       "      <td>UORGAFOC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   car_id     make    config\n",
       "0       0       Vw  RYVUNGAL\n",
       "1       1     Ford  FAEJQCKM\n",
       "2       2  Porsche  RMNLMDSR\n",
       "3       3     Ford  SDWYNHMB\n",
       "4       4   Suzuki  NMKSDQBR\n",
       "5       5     Ford  BIYGLVPG\n",
       "6       6     Ford  KDOCUHAG\n",
       "7       7     Ford  UFUJSIES\n",
       "8       8      Kia  OVJWCCZM\n",
       "9       9     Ford  UORGAFOC"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cars = 100000000\n",
    "\n",
    "cars = spark.range(0,num_cars).select(\n",
    "        f.col(\"id\").alias(\"car_id\"),\n",
    "        random_make_udf('id').alias('make'),\n",
    "        random_config_udf('id').alias('config')\n",
    "    )\n",
    "\n",
    "cars.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Create Sales Table\n",
    "\n",
    "Now we create an additional table containing car informations, again highly skewed. We use a fictional `sales` table, where each line has again a `sales_id`, a reference to a speciifc car configuration via `car_id` and a sales date. Again, we create a highly skewed table by assigning 80% of all entries to the car with id `100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales_id</th>\n",
       "      <th>sales_date</th>\n",
       "      <th>car_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-10-20</td>\n",
       "      <td>61426701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-04-18</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-02-16</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2023-05-27</td>\n",
       "      <td>50237808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2023-05-17</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2023-02-23</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2023-03-02</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sales_id  sales_date    car_id\n",
       "0         0  2023-10-20  61426701\n",
       "1         1  2023-04-18       100\n",
       "2         2  2023-05-19       100\n",
       "3         3  2023-02-28       100\n",
       "4         4  2022-12-30       100\n",
       "5         5  2023-02-16       100\n",
       "6         6  2023-05-27  50237808\n",
       "7         7  2023-05-17       100\n",
       "8         8  2023-02-23       100\n",
       "9         9  2023-03-02       100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sales = 100000000\n",
    "\n",
    "sales = spark.range(0,num_sales).select(\n",
    "        f.col(\"id\").alias(\"sales_id\"),\n",
    "        f.date_add(f.current_date(), -(f.rand() * 360).cast('int')).alias(\"sales_date\"),\n",
    "        f.when(f.rand() < 0.8, 100).otherwise((f.rand()*num_cars).cast('long')).alias(\"car_id\")\n",
    "    )\n",
    "\n",
    "sales.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Peform JOIN\n",
    "\n",
    "Finally we will join the two tables on the join key `id` / `car_id`. Note that the join key is not unique in the second DataFrame and note that the join key is highly skewed in both DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Unoptimized Skewed Join\n",
    "\n",
    "First we will use a non-adaptive join as the performance baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable automatic broadcast. Default: 10MB\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "# Enable AQE. Default: False\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:======================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.2 ms, sys: 56.4 ms, total: 106 ms\n",
      "Wall time: 54.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kia</td>\n",
       "      <td>999800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hyundai</td>\n",
       "      <td>998735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mercedes-Benz</td>\n",
       "      <td>1000218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vw</td>\n",
       "      <td>1000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suzuki</td>\n",
       "      <td>1000822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Porsche</td>\n",
       "      <td>999389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Skoda</td>\n",
       "      <td>1000256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>1000603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fiat</td>\n",
       "      <td>81001481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ford</td>\n",
       "      <td>10997896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            make  count(1)\n",
       "0            Kia    999800\n",
       "1        Hyundai    998735\n",
       "2  Mercedes-Benz   1000218\n",
       "3             Vw   1000800\n",
       "4         Suzuki   1000822\n",
       "5        Porsche    999389\n",
       "6          Skoda   1000256\n",
       "7         Nissan   1000603\n",
       "8           Fiat  81001481\n",
       "9           Ford  10997896"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "result = sales.join(cars, [\"car_id\"]) \\\n",
    "    .groupBy(\"make\") \\\n",
    "    .agg(f.count(\"*\"))\n",
    "\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Plan\n",
    "\n",
    "Let's inspect the execution plan after the query has succeeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(7) HashAggregate(keys=[make#4], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(make#4, 200), ENSURE_REQUIREMENTS, [plan_id=105]\n",
      "   +- *(6) HashAggregate(keys=[make#4], functions=[partial_count(1)])\n",
      "      +- *(6) Project [make#4]\n",
      "         +- *(6) SortMergeJoin [car_id#16L], [car_id#2L], Inner\n",
      "            :- *(2) Sort [car_id#16L ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(car_id#16L, 200), ENSURE_REQUIREMENTS, [plan_id=86]\n",
      "            :     +- *(1) Filter isnotnull(car_id#16L)\n",
      "            :        +- *(1) Project [CASE WHEN (rand(836370262552323682) < 0.8) THEN 100 ELSE cast((rand(-6483106927888745877) * 1.0E8) as bigint) END AS car_id#16L]\n",
      "            :           +- *(1) Range (0, 100000000, step=1, splits=40)\n",
      "            +- *(5) Sort [car_id#2L ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(car_id#2L, 200), ENSURE_REQUIREMENTS, [plan_id=96]\n",
      "                  +- *(4) Project [id#0L AS car_id#2L, pythonUDF0#34 AS make#4]\n",
      "                     +- ArrowEvalPython [random_make_udf(id#0L)#3], [pythonUDF0#34], 200\n",
      "                        +- *(3) Range (0, 100000000, step=1, splits=40)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Optimized Skewed Join (AQE)\n",
    "\n",
    "Now we will enable the Adaptive Query Execution in Spark and configure some thresholds such that it will work nicely with our rather small data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable AQE. Ddefault: False\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
    "# Enable skewed join optimization. Default: True\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", True)\n",
    "\n",
    "# The advisory size in bytes of the shuffle partition during adaptive optimization (when spark.sql.adaptive.enabled is true).\n",
    "spark.conf.set(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"8KB\")\n",
    "# A partition is considered as skewed if its size is larger than this factor multiplying the median partition size and also larger than 'spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes'. Default: 5\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionFactor\", 2)\n",
    "# A partition is considered as skewed if its size in bytes is larger than this threshold and also larger than 'spark.sql.adaptive.skewJoin.skewedPartitionFactor' multiplying the median partition size. Default: 256MB\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"16KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=====================================================>(217 + 2) / 219]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.6 ms, sys: 33.9 ms, total: 89.5 ms\n",
      "Wall time: 17.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kia</td>\n",
       "      <td>998706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hyundai</td>\n",
       "      <td>81003634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mercedes-Benz</td>\n",
       "      <td>1000380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vw</td>\n",
       "      <td>998569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suzuki</td>\n",
       "      <td>1000681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Porsche</td>\n",
       "      <td>1001252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Skoda</td>\n",
       "      <td>998986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>999031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fiat</td>\n",
       "      <td>999437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ford</td>\n",
       "      <td>10999324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            make  count(1)\n",
       "0            Kia    998706\n",
       "1        Hyundai  81003634\n",
       "2  Mercedes-Benz   1000380\n",
       "3             Vw    998569\n",
       "4         Suzuki   1000681\n",
       "5        Porsche   1001252\n",
       "6          Skoda    998986\n",
       "7         Nissan    999031\n",
       "8           Fiat    999437\n",
       "9           Ford  10999324"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "result = sales.join(cars, [\"car_id\"]) \\\n",
    "    .groupBy(\"make\") \\\n",
    "    .agg(f.count(\"*\"))\n",
    "\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Plan\n",
    "\n",
    "Again let's inspect the execution plan after the query has succeeded. Note that it will look significantly different now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=true\n",
      "+- == Final Plan ==\n",
      "   *(7) HashAggregate(keys=[make#4], functions=[count(1)])\n",
      "   +- AQEShuffleRead coalesced\n",
      "      +- ShuffleQueryStage 2\n",
      "         +- Exchange hashpartitioning(make#4, 200), ENSURE_REQUIREMENTS, [plan_id=353]\n",
      "            +- *(6) HashAggregate(keys=[make#4], functions=[partial_count(1)])\n",
      "               +- *(6) Project [make#4]\n",
      "                  +- *(6) SortMergeJoin(skew=true) [car_id#16L], [car_id#2L], Inner\n",
      "                     :- *(4) Sort [car_id#16L ASC NULLS FIRST], false, 0\n",
      "                     :  +- AQEShuffleRead skewed\n",
      "                     :     +- ShuffleQueryStage 0\n",
      "                     :        +- Exchange hashpartitioning(car_id#16L, 200), ENSURE_REQUIREMENTS, [plan_id=223]\n",
      "                     :           +- *(1) Filter isnotnull(car_id#16L)\n",
      "                     :              +- *(1) Project [CASE WHEN (rand(836370262552323682) < 0.8) THEN 100 ELSE cast((rand(-6483106927888745877) * 1.0E8) as bigint) END AS car_id#16L]\n",
      "                     :                 +- *(1) Range (0, 100000000, step=1, splits=40)\n",
      "                     +- *(5) Sort [car_id#2L ASC NULLS FIRST], false, 0\n",
      "                        +- AQEShuffleRead\n",
      "                           +- ShuffleQueryStage 1\n",
      "                              +- Exchange hashpartitioning(car_id#2L, 200), ENSURE_REQUIREMENTS, [plan_id=234]\n",
      "                                 +- *(3) Project [id#0L AS car_id#2L, pythonUDF0#51 AS make#4]\n",
      "                                    +- ArrowEvalPython [random_make_udf(id#0L)#3], [pythonUDF0#51], 200\n",
      "                                       +- *(2) Range (0, 100000000, step=1, splits=40)\n",
      "+- == Initial Plan ==\n",
      "   HashAggregate(keys=[make#4], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(make#4, 200), ENSURE_REQUIREMENTS, [plan_id=194]\n",
      "      +- HashAggregate(keys=[make#4], functions=[partial_count(1)])\n",
      "         +- Project [make#4]\n",
      "            +- SortMergeJoin [car_id#16L], [car_id#2L], Inner\n",
      "               :- Sort [car_id#16L ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(car_id#16L, 200), ENSURE_REQUIREMENTS, [plan_id=186]\n",
      "               :     +- Filter isnotnull(car_id#16L)\n",
      "               :        +- Project [CASE WHEN (rand(836370262552323682) < 0.8) THEN 100 ELSE cast((rand(-6483106927888745877) * 1.0E8) as bigint) END AS car_id#16L]\n",
      "               :           +- Range (0, 100000000, step=1, splits=40)\n",
      "               +- Sort [car_id#2L ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(car_id#2L, 200), ENSURE_REQUIREMENTS, [plan_id=187]\n",
      "                     +- Project [id#0L AS car_id#2L, pythonUDF0#51 AS make#4]\n",
      "                        +- ArrowEvalPython [random_make_udf(id#0L)#3], [pythonUDF0#51], 200\n",
      "                           +- Range (0, 100000000, step=1, splits=40)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
