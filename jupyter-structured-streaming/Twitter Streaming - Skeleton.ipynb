{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f293f5a-6634-41a3-94cd-cdd2c5c7cafc",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming\n",
    "\n",
    "This notebook is about *stream processing*. This term refers to *near realtime* or *low latency* data processing. Typically, there is a data source, which continuously emits new events. Then the task is to process these eventss immediately as they happen. Traditionally, Spark has been a *batch processing* framework optimized fro throughput and not for latency. Nevertheless, the Spark developers implemented a streaming mechanism which essentially processes incoming data in *micro batches* (i.e. very small batches with tens of records).\n",
    "\n",
    "In order to follow this notebook, you need a streaming source. We will use Kafka, which is a very commonly found platform. The data in this notebook comes from Twitter, and is freely available from The Internet Archive at https://archive.org/details/archiveteam-twitter-stream-2016-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d918e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"4G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166aeb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c38dbe",
   "metadata": {},
   "source": [
    "# 1. Connect to data source\n",
    "\n",
    "First you need to fill a Kafka topic, for example via\n",
    "\n",
    "    s3cat.py -I1 -B10 s3://dimajix-training/data/twitter-sample/ | /opt/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic twitter\n",
    "\n",
    "Then we connect to the raw data socket as the datasource by using the `DataStreamReader` API via `spark.readStream`. We need to specify the options `kafka.bootstrap.servers` and `subscribe` and we need to use the format `kafka` for connecting to the data source. The Kafka topic will stream Twitter data samples in raw JSON format, i.e. one JSON document per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c221fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10447f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the correct AWS VPC address of your master host\n",
    "master = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ee84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Kafka using the DataStreamReader API via spark.readStream. You need to specify the options `kafka.bootstrap.servers`, `subscribe` and you need to use the format `kafka`\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20383cb3",
   "metadata": {},
   "source": [
    "## 1.1 Inspect Schema\n",
    "\n",
    "The result of the load method is a `DataFrame` again, but a streaming one. This `DataFrame` again has a schema, which we can inspect with the usual method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f8fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5c91eb",
   "metadata": {},
   "source": [
    "# 2. Inspect Data\n",
    "\n",
    "Of course we also want to inspect the data inside the DataFrame. But this time, we cannot simply invoke `show`, because normal actions do not (directly) work on streaming DataFrames. Instead we need to create a continiuous query. Later, we will see a neat trick how a streaming query can be transformed into a volatile table.\n",
    "\n",
    "In order to create a continuous query, we need to perform the following steps\n",
    "\n",
    "1. Create a `DataStreamWriter` by using the `writeStream` method of a DataFrame\n",
    "2. Specify the output format. We use `console` in our case\n",
    "3. Specify a checkpoint location on HDFS. This is required for restarting\n",
    "4. Optionally specify a processing period\n",
    "5. Start the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c113fe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f94dd66",
   "metadata": {},
   "source": [
    "## 2.1 Stop Query\n",
    "\n",
    "In contrast to the RDD API, we can simply stop an individual query instead of a whole StreamingContext by simply calling the `stop` method on the query object. This makes working with streams much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c91fffa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ef25b",
   "metadata": {},
   "source": [
    "# 3. Counting Hash-Tags\n",
    "\n",
    "So we now want to create a streaming hashtag count. First we need to extract the Tweet itself from the JSON document, then we need to extract the hashtags in a similar way to the batch word traditional DataFrame word count example, i.e. we split every line into words, keep only hash-tags, group the words and count the sizes of the groups.\n",
    "\n",
    "Each query looks as follows\n",
    "\n",
    "```\n",
    "{ \"contributors\" : null,\n",
    "  \"coordinates\" : null,\n",
    "  \"created_at\" : \"Fri Jul 29 12:46:00 +0000 2016\",\n",
    "  \"entities\" : { \"hashtags\" : [  ],\n",
    "      \"symbols\" : [  ],\n",
    "      \"urls\" : [ { \"display_url\" : \"fb.me/ItnwZEhy\",\n",
    "            \"expanded_url\" : \"http://fb.me/ItnwZEhy\",\n",
    "            \"indices\" : [ 33,\n",
    "                56\n",
    "              ],\n",
    "            \"url\" : \"https://t.co/mM0if95F1K\"\n",
    "          } ],\n",
    "      \"user_mentions\" : [  ]\n",
    "    },\n",
    "  \"favorite_count\" : 0,\n",
    "  \"favorited\" : false,\n",
    "  \"filter_level\" : \"low\",\n",
    "  \"geo\" : null,\n",
    "  \"id\" : 759007065155117058,\n",
    "  \"id_str\" : \"759007065155117058\",\n",
    "  \"in_reply_to_screen_name\" : null,\n",
    "  \"in_reply_to_status_id\" : null,\n",
    "  \"in_reply_to_status_id_str\" : null,\n",
    "  \"in_reply_to_user_id\" : null,\n",
    "  \"in_reply_to_user_id_str\" : null,\n",
    "  \"is_quote_status\" : false,\n",
    "  \"lang\" : \"en\",\n",
    "  \"place\" : null,\n",
    "  \"possibly_sensitive\" : false,\n",
    "  \"retweet_count\" : 0,\n",
    "  \"retweeted\" : false,\n",
    "  \"source\" : \"<a href=\\\"http://www.facebook.com/twitter\\\" rel=\\\"nofollow\\\">Facebook</a>\",\n",
    "  \"text\" : \"I posted a new video to Facebook https://t.co/mM0if95F1K\",\n",
    "  \"timestamp_ms\" : \"1469796360659\",\n",
    "  \"truncated\" : false,\n",
    "  \"user\" : { \"contributors_enabled\" : false,\n",
    "      \"created_at\" : \"Sat Sep 08 08:28:55 +0000 2012\",\n",
    "      \"default_profile\" : false,\n",
    "      \"default_profile_image\" : false,\n",
    "      \"description\" : null,\n",
    "      \"favourites_count\" : 0,\n",
    "      \"follow_request_sent\" : null,\n",
    "      \"followers_count\" : 0,\n",
    "      \"following\" : null,\n",
    "      \"friends_count\" : 0,\n",
    "      \"geo_enabled\" : false,\n",
    "      \"id\" : 810489374,\n",
    "      \"id_str\" : \"810489374\",\n",
    "      \"is_translator\" : false,\n",
    "      \"lang\" : \"zh-tw\",\n",
    "      \"listed_count\" : 0,\n",
    "      \"location\" : null,\n",
    "      \"name\" : \"張冥閻\",\n",
    "      \"notifications\" : null,\n",
    "      \"profile_background_color\" : \"FFF04D\",\n",
    "      \"profile_background_image_url\" : \"http://abs.twimg.com/images/themes/theme19/bg.gif\",\n",
    "      \"profile_background_image_url_https\" : \"https://abs.twimg.com/images/themes/theme19/bg.gif\",\n",
    "      \"profile_background_tile\" : false,\n",
    "      \"profile_image_url\" : \"http://pbs.twimg.com/profile_images/378800000157469481/0a267258c8ccd1bf53d01c115677dbd7_normal.jpeg\",\n",
    "      \"profile_image_url_https\" : \"https://pbs.twimg.com/profile_images/378800000157469481/0a267258c8ccd1bf53d01c115677dbd7_normal.jpeg\",\n",
    "      \"profile_link_color\" : \"0099CC\",\n",
    "      \"profile_sidebar_border_color\" : \"FFF8AD\",\n",
    "      \"profile_sidebar_fill_color\" : \"F6FFD1\",\n",
    "      \"profile_text_color\" : \"333333\",\n",
    "      \"profile_use_background_image\" : true,\n",
    "      \"protected\" : false,\n",
    "      \"screen_name\" : \"nineemperor1\",\n",
    "      \"statuses_count\" : 9652,\n",
    "      \"time_zone\" : null,\n",
    "      \"url\" : null,\n",
    "      \"utc_offset\" : null,\n",
    "      \"verified\" : false\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "In order to extract a field from a JSON document, we can use the `get_json_object` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86cecc8",
   "metadata": {},
   "source": [
    "## 3.1 Extract Tweet\n",
    "\n",
    "First we need to extract the tweet text itself via the `get_json_object` function and store it into a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13935032",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_text = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66dc2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_text.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7365271",
   "metadata": {},
   "source": [
    "## 3.2 Extract Topics\n",
    "\n",
    "Now that we have the Tweet text itself, we extract all topics with the following approach:\n",
    "1. Split text along spaces using `split`\n",
    "2. Create multiple records from all words using `explode`\n",
    "3. Filter all hash-tags (words that start with a `#`)\n",
    "4. Filter out all empty topics (topic name only consists of hash-tag `#` itself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd54be9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c739ac",
   "metadata": {},
   "source": [
    "## 3.3 Count Topics\n",
    "\n",
    "Now that we have the hash tags (topics), we perform a simple aggregation as usual: Group by hashtag (`topic`) and count number of tweets (using `count` or `sum(1)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6191f29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917a78f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff326c9",
   "metadata": {},
   "source": [
    "## 3.4 Print Results onto Console\n",
    "\n",
    "Again we want to print the results onto the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bb8adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = counts.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/zeppelin/checkpoint-twitter-count-\" + str(time.time())) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4cfac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735447bc",
   "metadata": {},
   "source": [
    "# 4. Time-Windowed Aggregation\n",
    "\n",
    "Another interesting (and probably more realistic) application is to perform time windowed aggregations. This means that we define a sliding time window used in the `groupBy` clause. In addition we also define a so called *watermark* which tells Spark how long to wait for late arrivels of individual data points (we don't have them in our simple example)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bf9cbb-e4e3-449a-92b6-cbbad9ad443d",
   "metadata": {},
   "source": [
    "## 4.1 Define Window and Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3edd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowedCounts = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38424030",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowedCounts.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4612266",
   "metadata": {},
   "source": [
    "## 4.2 Start Query\n",
    "\n",
    "Let's again output the data. This time, we also like to investigate the different output modes `append`, `complete` and `update`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f7b0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = windowedCounts.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime=\"1 seconds\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/zeppelin/checkpoint-twitter-console-\" + str(time.time())) \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58621d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcd5b7d-88e4-448a-8754-d17408747bc0",
   "metadata": {},
   "source": [
    "# 5. Kafka Output\n",
    "\n",
    "So far, we have only used Kafka for reading and dumped the result onto the console. Of course, this is not a realtistic scenario. Instead, a good idea is to send the results to Kafka again. Then an additional system can fetch the records from Kafka again and do whatever it thinks could make sense. This approach will technically decouple Spark from a real sink."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2c3a9b-2413-49af-b051-34e7e2f2c07e",
   "metadata": {},
   "source": [
    "## 5.1 Format Result\n",
    "\n",
    "Kafka only accepts simple messages. We therefore store the result into a JSON object as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f515b29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowedCountsAsValues = windowedCounts.withColumn(\"value\", \n",
    "            f.to_json(\n",
    "                f.struct(\n",
    "                    windowedCounts[\"window\"],\n",
    "                    windowedCounts[\"topic\"],\n",
    "                    windowedCounts[\"count\"]\n",
    "                )\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d09a991-5bea-4c78-aa3e-c46f3aaf9afe",
   "metadata": {},
   "source": [
    "## 5.2 Start Query\n",
    "\n",
    "Now we use `kafka` as the output format. This requires some additional configuration (for example the address of some Kafka bootstrap servers and the Kafka topic name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f74f90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = windowedCountsAsValues.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    # YOUR CODE HERE\n",
    "    .trigger(processingTime=\"1 seconds\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/zeppelin/checkpoint-twitter-console-\" + str(time.time())) \\\n",
    "    .start()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49b6040",
   "metadata": {},
   "source": [
    "While the query is running, you can peek inside the Kafka topic via the following command line:\n",
    "\n",
    "```shell\n",
    "/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic kku\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3292c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08bccb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
