{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88cad95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d918e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-200-1-167.eu-central-1.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1-amzn-0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fdedc253150>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"4G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "166aeb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c38dbe",
   "metadata": {},
   "source": [
    "# 1. Connect to data source\n",
    "\n",
    "First you need to fill a Kafka topic, for example via\n",
    "\n",
    "    s3cat.py -I1 -B10 s3://dimajix-training/data/twitter-sample/ | /opt/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic twitter\n",
    "\n",
    "Then we connect to the raw data socket as the datasource by using the `DataStreamReader` API via `spark.readStream`. We need to specify the options `kafka.bootstrap.servers` and `subscribe` and we need to use the format `kafka` for connecting to the data source. The Kafka topic will stream Twitter data samples in raw JSON format, i.e. one JSON document per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c221fa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ip-10-200-1-167\r\n"
     ]
    }
   ],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10447f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the correct AWS VPC address of your master host\n",
    "master = \"ip-10-200-1-167:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03ee84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Kafka using the DataStreamReader API via spark.readStream. You need to specify the options `kafka.bootstrap.servers`, `subscribe` and you need to use the format `kafka`\n",
    "lines = spark.readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", master) \\\n",
    "  .option(\"subscribe\", \"twitter\") \\\n",
    "  .option(\"startingOffsets\", \"latest\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20383cb3",
   "metadata": {},
   "source": [
    "## 1.1 Inspect Schema\n",
    "\n",
    "The result of the load method is a `DataFrame` again, but a streaming one. This `DataFrame` again has a schema, which we can inspect with the usual method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d05f8fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5c91eb",
   "metadata": {},
   "source": [
    "# 2. Inspect Data\n",
    "\n",
    "Of course we also want to inspect the data inside the DataFrame. But this time, we cannot simply invoke `show`, because normal actions do not (directly) work on streaming DataFrames. Instead we need to create a continiuous query. Later, we will see a neat trick how a streaming query can be transformed into a volatile table.\n",
    "\n",
    "In order to create a continuous query, we need to perform the following steps\n",
    "\n",
    "1. Create a `DataStreamWriter` by using the `writeStream` method of a DataFrame\n",
    "2. Specify the output format. We use `console` in our case\n",
    "3. Specify a checkpoint location on HDFS. This is required for restarting\n",
    "4. Optionally specify a processing period\n",
    "5. Start the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c113fe0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/02 18:32:07 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/11/02 18:32:08 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "\n",
    "query = lines \\\n",
    "    .withColumn(\"value\", lines[\"value\"].cast(\"string\")) \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"truncate\", True) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/zeppelin/checkpoint-twitter-print-\" + str(time.time())) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f94dd66",
   "metadata": {},
   "source": [
    "## 2.1 Stop Query\n",
    "\n",
    "In contrast to the RDD API, we can simply stop an individual query instead of a whole StreamingContext by simply calling the `stop` method on the query object. This makes working with streams much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c91fffa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/02 18:32:09 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is aborting.\n",
      "23/11/02 18:32:09 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] aborted.\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ef25b",
   "metadata": {},
   "source": [
    "# 3. Counting Hash-Tags\n",
    "\n",
    "So we now want to create a streaming hashtag count. First we need to extract the Tweet itself from the JSON document, then we need to extract the hashtags in a similar way to the batch word traditional DataFrame word count example, i.e. we split every line into words, keep only hash-tags, group the words and count the sizes of the groups.\n",
    "\n",
    "Each query looks as follows\n",
    "\n",
    "```\n",
    "{ \"contributors\" : null,\n",
    "  \"coordinates\" : null,\n",
    "  \"created_at\" : \"Fri Jul 29 12:46:00 +0000 2016\",\n",
    "  \"entities\" : { \"hashtags\" : [  ],\n",
    "      \"symbols\" : [  ],\n",
    "      \"urls\" : [ { \"display_url\" : \"fb.me/ItnwZEhy\",\n",
    "            \"expanded_url\" : \"http://fb.me/ItnwZEhy\",\n",
    "            \"indices\" : [ 33,\n",
    "                56\n",
    "              ],\n",
    "            \"url\" : \"https://t.co/mM0if95F1K\"\n",
    "          } ],\n",
    "      \"user_mentions\" : [  ]\n",
    "    },\n",
    "  \"favorite_count\" : 0,\n",
    "  \"favorited\" : false,\n",
    "  \"filter_level\" : \"low\",\n",
    "  \"geo\" : null,\n",
    "  \"id\" : 759007065155117058,\n",
    "  \"id_str\" : \"759007065155117058\",\n",
    "  \"in_reply_to_screen_name\" : null,\n",
    "  \"in_reply_to_status_id\" : null,\n",
    "  \"in_reply_to_status_id_str\" : null,\n",
    "  \"in_reply_to_user_id\" : null,\n",
    "  \"in_reply_to_user_id_str\" : null,\n",
    "  \"is_quote_status\" : false,\n",
    "  \"lang\" : \"en\",\n",
    "  \"place\" : null,\n",
    "  \"possibly_sensitive\" : false,\n",
    "  \"retweet_count\" : 0,\n",
    "  \"retweeted\" : false,\n",
    "  \"source\" : \"<a href=\\\"http://www.facebook.com/twitter\\\" rel=\\\"nofollow\\\">Facebook</a>\",\n",
    "  \"text\" : \"I posted a new video to Facebook https://t.co/mM0if95F1K\",\n",
    "  \"timestamp_ms\" : \"1469796360659\",\n",
    "  \"truncated\" : false,\n",
    "  \"user\" : { \"contributors_enabled\" : false,\n",
    "      \"created_at\" : \"Sat Sep 08 08:28:55 +0000 2012\",\n",
    "      \"default_profile\" : false,\n",
    "      \"default_profile_image\" : false,\n",
    "      \"description\" : null,\n",
    "      \"favourites_count\" : 0,\n",
    "      \"follow_request_sent\" : null,\n",
    "      \"followers_count\" : 0,\n",
    "      \"following\" : null,\n",
    "      \"friends_count\" : 0,\n",
    "      \"geo_enabled\" : false,\n",
    "      \"id\" : 810489374,\n",
    "      \"id_str\" : \"810489374\",\n",
    "      \"is_translator\" : false,\n",
    "      \"lang\" : \"zh-tw\",\n",
    "      \"listed_count\" : 0,\n",
    "      \"location\" : null,\n",
    "      \"name\" : \"張冥閻\",\n",
    "      \"notifications\" : null,\n",
    "      \"profile_background_color\" : \"FFF04D\",\n",
    "      \"profile_background_image_url\" : \"http://abs.twimg.com/images/themes/theme19/bg.gif\",\n",
    "      \"profile_background_image_url_https\" : \"https://abs.twimg.com/images/themes/theme19/bg.gif\",\n",
    "      \"profile_background_tile\" : false,\n",
    "      \"profile_image_url\" : \"http://pbs.twimg.com/profile_images/378800000157469481/0a267258c8ccd1bf53d01c115677dbd7_normal.jpeg\",\n",
    "      \"profile_image_url_https\" : \"https://pbs.twimg.com/profile_images/378800000157469481/0a267258c8ccd1bf53d01c115677dbd7_normal.jpeg\",\n",
    "      \"profile_link_color\" : \"0099CC\",\n",
    "      \"profile_sidebar_border_color\" : \"FFF8AD\",\n",
    "      \"profile_sidebar_fill_color\" : \"F6FFD1\",\n",
    "      \"profile_text_color\" : \"333333\",\n",
    "      \"profile_use_background_image\" : true,\n",
    "      \"protected\" : false,\n",
    "      \"screen_name\" : \"nineemperor1\",\n",
    "      \"statuses_count\" : 9652,\n",
    "      \"time_zone\" : null,\n",
    "      \"url\" : null,\n",
    "      \"utc_offset\" : null,\n",
    "      \"verified\" : false\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "In order to extract a field from a JSON document, we can use the `get_json_object` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86cecc8",
   "metadata": {},
   "source": [
    "## 3.1 Extract Tweet\n",
    "\n",
    "First we need to extract the tweet text itself via the `get_json_object` function and store it into a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13935032",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_text = lines.select(\n",
    "        lines[\"timestamp\"],\n",
    "        f.get_json_object(lines[\"value\"].cast(\"string\"), \"$.text\").alias(\"text\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a66dc2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ts_text.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7365271",
   "metadata": {},
   "source": [
    "## 3.2 Extract Topics\n",
    "\n",
    "Now that we have the Tweet text itself, we extract all topics with the following approach:\n",
    "1. Split text along spaces using `split`\n",
    "2. Create multiple records from all words using `explode`\n",
    "3. Filter all hash-tags (words that start with a `#`)\n",
    "4. Filter out all empty topics (topic name only consists of hash-tag `#` itself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd54be9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ts_text.select(\n",
    "        ts_text[\"timestamp\"],\n",
    "        f.explode(f.split(ts_text[\"text\"],\" \")).alias(\"topic\")\n",
    "    ) \\\n",
    "    .filter(f.col(\"topic\").startswith(\"#\")) \\\n",
    "    .filter(f.col(\"topic\") != \"#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c739ac",
   "metadata": {},
   "source": [
    "## 3.3 Count Topics\n",
    "\n",
    "Now that we have the hash tags (topics), we perform a simple aggregation as usual: Group by hashtag (`topic`) and count number of tweets (using `count` or `sum(1)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6191f29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = topics \\\n",
    "    .groupBy(\"topic\") \\\n",
    "    .agg(f.sum(f.lit(1)).alias(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "917a78f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- topic: string (nullable = false)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff326c9",
   "metadata": {},
   "source": [
    "## 3.4 Print Results onto Console\n",
    "\n",
    "Again we want to print the results onto the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0bb8adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/02 18:25:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/11/02 18:25:41 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|topic|count|\n",
      "+-----+-----+\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----------------------------------------+-----+\n",
      "|topic                                   |count|\n",
      "+----------------------------------------+-----+\n",
      "|#오피뷰\\n♥비아그라/시알리스OIO↔4898↔875O|1    |\n",
      "|#JumatBerkah                            |1    |\n",
      "|#재혼만남\\n#돌싱카페                    |1    |\n",
      "|#돌싱카페                               |1    |\n",
      "|#深夜ラーメン                           |1    |\n",
      "|#재혼만남                               |1    |\n",
      "|#一振いちご                             |1    |\n",
      "|#돌싱카페\\n#애인대행                    |1    |\n",
      "|#애인대행\\n#비아그라파는곳              |1    |\n",
      "|#PJNET                                  |1    |\n",
      "|#New                                    |1    |\n",
      "|#BIGOLIVE.                              |1    |\n",
      "|#애인대행                               |1    |\n",
      "+----------------------------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------------------+-----+\n",
      "|topic               |count|\n",
      "+--------------------+-----+\n",
      "|#Jacky              |1    |\n",
      "|#PushAwardsKathNiels|1    |\n",
      "|#JACKSONWANG        |1    |\n",
      "|#GOT7               |1    |\n",
      "|#جمعه_مباركه        |1    |\n",
      "|#잭슨               |1    |\n",
      "|#王嘉尔             |1    |\n",
      "|#갓세븐             |1    |\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:======================================>                   (2 + 1) / 3]\r"
     ]
    }
   ],
   "source": [
    "query = counts.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/zeppelin/checkpoint-twitter-count-\" + str(time.time())) \\\n",
    "    .start()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c4cfac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|topic                  |count|\n",
      "+-----------------------+-----+\n",
      "|#発達障害労働を考える会|1    |\n",
      "+-----------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735447bc",
   "metadata": {},
   "source": [
    "# 4. Time-Windowed Aggregation\n",
    "\n",
    "Another interesting (and probably more realistic) application is to perform time windowed aggregations. This means that we define a sliding time window used in the `groupBy` clause. In addition we also define a so called *watermark* which tells Spark how long to wait for late arrivels of individual data points (we don't have them in our simple example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c3edd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowedCounts = topics \\\n",
    "    .withWatermark(\"timestamp\", \"10 seconds\") \\\n",
    "    .groupBy(f.window(topics[\"timestamp\"], \"5 seconds\", \"1 seconds\"), topics[\"topic\"]) \\\n",
    "    .agg(f.sum(f.lit(1)).alias(\"count\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38424030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- topic: string (nullable = false)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowedCounts.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4612266",
   "metadata": {},
   "source": [
    "## 4.1 Output Data\n",
    "\n",
    "Let's again output the data. This time, we also like to investigate the different output modes `append`, `complete` and `update`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84f7b0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/02 18:28:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/11/02 18:28:41 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "23/11/02 18:28:52 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 11048 milliseconds\n",
      "23/11/02 18:28:57 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 5021 milliseconds\n",
      "23/11/02 18:28:59 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1526 milliseconds\n",
      "23/11/02 18:29:01 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1976 milliseconds\n",
      "23/11/02 18:29:05 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1741 milliseconds\n",
      "23/11/02 18:29:06 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1246 milliseconds\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query = windowedCounts.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime=\"1 seconds\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/zeppelin/checkpoint-twitter-console-\" + str(time.time())) \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d58621d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/02 18:30:19 WARN SQLAppStatusListener: Unable to load custom metric object for class `org.apache.spark.sql.kafka010.OffsetOutOfRangeMetric`. Please make sure that the custom metric class is in the classpath and it has 0-arg constructor.\n",
      "java.lang.ClassNotFoundException: org.apache.spark.sql.kafka010.OffsetOutOfRangeMetric\n",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387) ~[?:1.8.0_382]\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_382]\n",
      "\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352) ~[?:1.8.0_382]\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_382]\n",
      "\tat java.lang.Class.forName0(Native Method) ~[?:1.8.0_382]\n",
      "\tat java.lang.Class.forName(Class.java:348) ~[?:1.8.0_382]\n",
      "\tat org.apache.spark.util.Utils$.classForName(Utils.scala:226) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$loadExtensions$1(Utils.scala:3042) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat scala.collection.immutable.List.flatMap(List.scala:366) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.util.Utils$.loadExtensions(Utils.scala:3040) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.ui.SQLAppStatusListener.$anonfun$aggregateMetrics$3(SQLAppStatusListener.scala:222) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat scala.Option.map(Option.scala:230) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.execution.ui.SQLAppStatusListener.$anonfun$aggregateMetrics$2(SQLAppStatusListener.scala:216) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat scala.collection.immutable.List.map(List.scala:297) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.execution.ui.SQLAppStatusListener.aggregateMetrics(SQLAppStatusListener.scala:214) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.ui.SQLAppStatusListener.$anonfun$liveExecutionMetrics$1(SQLAppStatusListener.scala:205) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat scala.Option.map(Option.scala:230) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.execution.ui.SQLAppStatusListener.liveExecutionMetrics(SQLAppStatusListener.scala:201) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.ui.SQLAppStatusStore.$anonfun$executionMetrics$2(SQLAppStatusStore.scala:72) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat scala.Option.flatMap(Option.scala:271) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.execution.ui.SQLAppStatusStore.$anonfun$executionMetrics$1(SQLAppStatusStore.scala:72) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat scala.Option.orElse(Option.scala:447) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.execution.ui.SQLAppStatusStore.executionMetrics(SQLAppStatusStore.scala:72) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.ui.ExecutionPage.$anonfun$render$2(ExecutionPage.scala:86) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat scala.Option.map(Option.scala:230) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.execution.ui.ExecutionPage.render(ExecutionPage.scala:45) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687) ~[javax.servlet-api-3.1.0.jar:3.1.0]\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790) ~[javax.servlet-api-3.1.0.jar:3.1.0]\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:799) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1656) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:185) ~[hadoop-yarn-server-web-proxy-3.3.3-amzn-5.jar:?]\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:772) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:732) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:479) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]\n",
      "23/11/02 18:30:19 WARN SQLAppStatusListener: Unable to load custom metric object for class `org.apache.spark.sql.kafka010.DataLossMetric`. Please make sure that the custom metric class is in the classpath and it has 0-arg constructor.\n",
      "java.lang.ClassNotFoundException: org.apache.spark.sql.kafka010.DataLossMetric\n",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387) ~[?:1.8.0_382]\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_382]\n",
      "\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352) ~[?:1.8.0_382]\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_382]\n",
      "\tat java.lang.Class.forName0(Native Method) ~[?:1.8.0_382]\n",
      "\tat java.lang.Class.forName(Class.java:348) ~[?:1.8.0_382]\n",
      "\tat org.apache.spark.util.Utils$.classForName(Utils.scala:226) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$loadExtensions$1(Utils.scala:3042) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat scala.collection.immutable.List.flatMap(List.scala:366) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.util.Utils$.loadExtensions(Utils.scala:3040) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.ui.SQLAppStatusListener.$anonfun$aggregateMetrics$3(SQLAppStatusListener.scala:222) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat scala.Option.map(Option.scala:230) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.execution.ui.SQLAppStatusListener.$anonfun$aggregateMetrics$2(SQLAppStatusListener.scala:216) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat scala.collection.immutable.List.map(List.scala:297) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.execution.ui.SQLAppStatusListener.aggregateMetrics(SQLAppStatusListener.scala:214) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.ui.SQLAppStatusListener.$anonfun$liveExecutionMetrics$1(SQLAppStatusListener.scala:205) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat scala.Option.map(Option.scala:230) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.execution.ui.SQLAppStatusListener.liveExecutionMetrics(SQLAppStatusListener.scala:201) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.ui.SQLAppStatusStore.$anonfun$executionMetrics$2(SQLAppStatusStore.scala:72) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat scala.Option.flatMap(Option.scala:271) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.execution.ui.SQLAppStatusStore.$anonfun$executionMetrics$1(SQLAppStatusStore.scala:72) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat scala.Option.orElse(Option.scala:447) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.execution.ui.SQLAppStatusStore.executionMetrics(SQLAppStatusStore.scala:72) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.ui.ExecutionPage.$anonfun$render$2(ExecutionPage.scala:86) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat scala.Option.map(Option.scala:230) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.execution.ui.ExecutionPage.render(ExecutionPage.scala:45) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687) ~[javax.servlet-api-3.1.0.jar:3.1.0]\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790) ~[javax.servlet-api-3.1.0.jar:3.1.0]\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:799) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1656) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:185) ~[hadoop-yarn-server-web-proxy-3.3.3-amzn-5.jar:?]\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:772) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:732) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:479) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034) ~[spark-core_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f515b29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowedCountsAsValues = windowedCounts.withColumn(\"value\", \n",
    "            f.to_json(\n",
    "                f.struct(\n",
    "                    windowedCounts[\"window\"],\n",
    "                    windowedCounts[\"topic\"],\n",
    "                    windowedCounts[\"count\"]\n",
    "                )\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57f74f90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/02 18:38:06 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/11/02 18:38:06 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "23/11/02 18:38:17 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 11145 milliseconds\n",
      "23/11/02 18:38:22 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 5015 milliseconds\n",
      "23/11/02 18:38:24 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 2012 milliseconds\n",
      "23/11/02 18:38:26 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 2093 milliseconds\n",
      "23/11/02 18:38:28 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1700 milliseconds\n",
      "23/11/02 18:38:30 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1584 milliseconds\n",
      "23/11/02 18:38:32 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1292 milliseconds\n",
      "23/11/02 18:38:34 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1691 milliseconds\n",
      "23/11/02 18:38:35 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1415 milliseconds\n",
      "23/11/02 18:38:36 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1331 milliseconds\n",
      "23/11/02 18:38:38 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1369 milliseconds\n",
      "23/11/02 18:38:40 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1172 milliseconds\n",
      "23/11/02 18:38:42 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1409 milliseconds\n",
      "23/11/02 18:38:44 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1581 milliseconds\n",
      "23/11/02 18:38:45 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1327 milliseconds\n",
      "23/11/02 18:38:46 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1225 milliseconds\n",
      "23/11/02 18:38:48 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 2274 milliseconds\n",
      "23/11/02 18:38:50 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1253 milliseconds\n",
      "23/11/02 18:38:51 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1209 milliseconds\n",
      "23/11/02 18:38:52 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1168 milliseconds\n",
      "23/11/02 18:38:54 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1633 milliseconds\n",
      "23/11/02 18:38:55 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1304 milliseconds\n",
      "23/11/02 18:38:57 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1270 milliseconds\n",
      "23/11/02 18:38:58 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1114 milliseconds\n",
      "23/11/02 18:39:00 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1254 milliseconds\n",
      "23/11/02 18:39:01 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1213 milliseconds\n",
      "23/11/02 18:39:03 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1116 milliseconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "query = windowedCountsAsValues.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", master) \\\n",
    "    .option(\"topic\", \"kku\") \\\n",
    "    .trigger(processingTime=\"1 seconds\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/zeppelin/checkpoint-twitter-console-\" + str(time.time())) \\\n",
    "    .start()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49b6040",
   "metadata": {},
   "source": [
    "/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic kku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3292c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/02 18:39:05 WARN DFSClient: Caught exception \n",
      "java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.lang.Thread.sleep(Native Method) ~[?:1.8.0_382]\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:973) ~[hadoop-client-api-3.3.3-amzn-5.jar:?]\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:909) ~[hadoop-client-api-3.3.3-amzn-5.jar:?]\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:892) ~[hadoop-client-api-3.3.3-amzn-5.jar:?]\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:847) ~[hadoop-client-api-3.3.3-amzn-5.jar:?]\n",
      "\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:78) ~[hadoop-client-api-3.3.3-amzn-5.jar:?]\n",
      "\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:107) ~[hadoop-client-api-3.3.3-amzn-5.jar:?]\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:152) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.write(HDFSMetadataLog.scala:204) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.addNewBatchByStream(HDFSMetadataLog.scala:237) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.add(HDFSMetadataLog.scala:130) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.markMicroBatchStart(MicroBatchExecution.scala:761) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$13(MicroBatchExecution.scala:534) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:533) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:802) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:473) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:266) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207) ~[spark-sql_2.12-3.4.1-amzn-0.jar:3.4.1-amzn-0]\n",
      "23/11/02 18:39:06 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1177 milliseconds\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08bccb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
