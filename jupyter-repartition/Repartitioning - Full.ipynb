{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repartitioning DataFrames\n",
    "\n",
    "Partitions are a central concept in Apache Spark. They are used for distributing and parallelizing work onto different executors, which run on multiple servers. \n",
    "\n",
    "### Determining Partitions\n",
    "Basically Spark uses two different strategies for splitting up data into multiple partitions:\n",
    "1. When Spark loads data, the records are put into partitions along natural borders. For example every HDFS block (and thereby every file) is represented by a different partition. Therefore the number of partitions of a DataFrame read from disk is solely determined by the number of HDFS blocks\n",
    "2. Certain operations like `JOIN`s and aggregations require that records with the same key are physically in the same partition. This is achieved by a shuffle phase. The number of partitions is specified by the global Spark configuration variable `spark.sql.shuffle.partitions` which has a default value of 200.\n",
    "\n",
    "### Repartitiong Data\n",
    "Since partitions have a huge influence on the execution, Spark also allows you to explicitly change the partitioning schema of a DataFrame. This makes sense only in a very limited (but still important) set of cases, which we will discuss in this notebook.\n",
    "\n",
    "### Weather Example\n",
    "Surprise, surprise, we will again use the weather example and see what explicit repartitioning gives us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-200-1-69.eu-central-1.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1-amzn-0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f77e05eae50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"24G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disable Automatic Broadcast JOINs\n",
    "In order to see the shuffle operations, we need to prevent Spark from executiong `JOIN` operations as broadcast joins. Again this can be turned off by setting the Spark configuration variable `spark.sql.autoBroadcastJoinThreshold` to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load Data\n",
    "\n",
    "First we load the weather data, which consists of the measurement data and some station metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageLocation = \"s3://dimajix-training/data/weather\"\n",
    "# storageLocation = \"/dimajix/data/weather-noaa-sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Measurements\n",
    "\n",
    "Measurements are stored in multiple directories (one per year). But we will limit ourselves to a single year in the analysis to improve readability of execution plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# Read in all years, store them in an Python array\n",
    "raw_weather_per_year = [spark.read.text(storageLocation + \"/\" + str(i)).withColumn(\"year\", f.lit(i)) for i in range(2003,2006)]\n",
    "\n",
    "# Union all years together\n",
    "raw_weather = reduce(lambda l,r: l.union(r), raw_weather_per_year)                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a single year to keep execution plans small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_weather = spark.read.text(storageLocation + \"/2003\").withColumn(\"year\", f.lit(2003))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Measurements\n",
    "\n",
    "Measurements were stored in a proprietary text based format, with some values at fixed positions. We need to extract these values with a simple `SELECT` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = raw_weather.select(\n",
    "    f.col(\"year\"),\n",
    "    f.substring(f.col(\"value\"),5,6).alias(\"usaf\"),\n",
    "    f.substring(f.col(\"value\"),11,5).alias(\"wban\"),\n",
    "    f.substring(f.col(\"value\"),16,8).alias(\"date\"),\n",
    "    f.substring(f.col(\"value\"),24,4).alias(\"time\"),\n",
    "    f.substring(f.col(\"value\"),42,5).alias(\"report_type\"),\n",
    "    f.substring(f.col(\"value\"),61,3).alias(\"wind_direction\"),\n",
    "    f.substring(f.col(\"value\"),64,1).alias(\"wind_direction_qual\"),\n",
    "    f.substring(f.col(\"value\"),65,1).alias(\"wind_observation\"),\n",
    "    (f.substring(f.col(\"value\"),66,4).cast(\"float\") / f.lit(10.0)).alias(\"wind_speed\"),\n",
    "    f.substring(f.col(\"value\"),70,1).alias(\"wind_speed_qual\"),\n",
    "    (f.substring(f.col(\"value\"),88,5).cast(\"float\") / f.lit(10.0)).alias(\"air_temperature\"),\n",
    "    f.substring(f.col(\"value\"),93,1).alias(\"air_temperature_qual\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load Station Metadata\n",
    "\n",
    "We also need to load the weather station meta data containing information about the geo location, country etc of individual weather stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "stations = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(storageLocation + \"/isd-history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Partitions\n",
    "\n",
    "Since partitions is a concept at the RDD level and a DataFrame per se does not contain an RDD, we need to access the RDD in order to inspect the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Repartitioning Data\n",
    "\n",
    "You can repartition any DataFrame by specifying the target number of partitions and the partitioning columns. While it should be clear what *number of partitions* actually means, the term *partitionng columns* might require some explanation.\n",
    "\n",
    "### Partitioning Columns\n",
    "Except for the case when Spark initially reads data, all DataFrames are partitioned along *partitioning columns*, which means that all records having the same values in the corresponding columns will end up in the same partition. Spark implicitly performs such repartitioning as shuffle operations for `JOIN`s and grouped aggregation (except when a DataFrame already has the correct partitioning columns and number of partitions)\n",
    "\n",
    "### Manual Repartitioning\n",
    "As already mentioned, you can explicitly repartition a DataFrame using teh `repartition()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = weather.repartition(10, weather[\"usaf\"], weather[\"wban\"])\n",
    "result.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Repartition\n",
    "\n",
    "Apart from introducing an additional shuffle operation, repartitioning a dataset will effectevely control the level of parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1807253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count(1)\n",
       "0   1807253"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = weather.repartition(20).select(f.count(\"*\"))\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[count(1)])\n",
      "+- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=66]\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_count(1)])\n",
      "      +- Exchange RoundRobinPartitioning(20), REPARTITION_BY_NUM, [plan_id=58]\n",
      "         +- FileScan text [] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex(1 paths)[s3://dimajix-training/data/weather/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Repartition & Joins\n",
    "\n",
    "As already mentioned, Spark implicitly performs a repartitioning aka shuffle for `JOIN` operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Plan\n",
    "\n",
    "So let us inspect the execution plan of a `JOIN` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [usaf#122, wban#123, 2003 AS year#118, date#124, time#125, report_type#126, wind_direction#127, wind_direction_qual#128, wind_observation#129, wind_speed#130, wind_speed_qual#131, air_temperature#132, air_temperature_qual#133, STATION NAME#166, CTRY#167, STATE#168, ICAO#169, LAT#170, LON#171, ELEV(M)#172, BEGIN#173, END#174]\n",
      "+- *(5) SortMergeJoin [usaf#122, wban#123], [USAF#164, WBAN#165], Inner\n",
      "   :- *(2) Sort [usaf#122 ASC NULLS FIRST, wban#123 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(usaf#122, wban#123, 200), ENSURE_REQUIREMENTS, [plan_id=114]\n",
      "   :     +- *(1) Project [substring(value#116, 5, 6) AS usaf#122, substring(value#116, 11, 5) AS wban#123, substring(value#116, 16, 8) AS date#124, substring(value#116, 24, 4) AS time#125, substring(value#116, 42, 5) AS report_type#126, substring(value#116, 61, 3) AS wind_direction#127, substring(value#116, 64, 1) AS wind_direction_qual#128, substring(value#116, 65, 1) AS wind_observation#129, (cast(cast(substring(value#116, 66, 4) as float) as double) / 10.0) AS wind_speed#130, substring(value#116, 70, 1) AS wind_speed_qual#131, (cast(cast(substring(value#116, 88, 5) as float) as double) / 10.0) AS air_temperature#132, substring(value#116, 93, 1) AS air_temperature_qual#133]\n",
      "   :        +- *(1) Filter (isnotnull(substring(value#116, 5, 6)) AND isnotnull(substring(value#116, 11, 5)))\n",
      "   :           +- FileScan text [value#116] Batched: false, DataFilters: [isnotnull(substring(value#116, 5, 6)), isnotnull(substring(value#116, 11, 5))], Format: Text, Location: InMemoryFileIndex(1 paths)[s3://dimajix-training/data/weather/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "   +- *(4) Sort [USAF#164 ASC NULLS FIRST, WBAN#165 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(USAF#164, WBAN#165, 200), ENSURE_REQUIREMENTS, [plan_id=122]\n",
      "         +- *(3) Filter (isnotnull(USAF#164) AND isnotnull(WBAN#165))\n",
      "            +- FileScan csv [USAF#164,WBAN#165,STATION NAME#166,CTRY#167,STATE#168,ICAO#169,LAT#170,LON#171,ELEV(M)#172,BEGIN#173,END#174] Batched: false, DataFilters: [isnotnull(USAF#164), isnotnull(WBAN#165)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://dimajix-training/data/weather/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = weather.join(stations, [\"usaf\", \"wban\"])\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "As we already discussed, each `JOIN` is executed with the following steps\n",
    "1. Filter `NULL` values (it's an inner join)\n",
    "2. Repartition DataFrame on the join columns with 200 partitions\n",
    "3. Sort each partition independently\n",
    "4. Perform a `SortMergeJoin`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Pre-partition data (first try)\n",
    "\n",
    "Now let us try what happens when we explicitly repartition the data before the join operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_rep = weather.repartition(10, weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_rep = stations.repartition(10, stations[\"usaf\"], stations[\"wban\"])\n",
    "stations_rep.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution Plan\n",
    "\n",
    "Let's analyze the resulting execution plan. Ideally all the preparation work before the `SortMergeJoin` happens before the `cache` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [usaf#122, wban#123, 2003 AS year#118, date#124, time#125, report_type#126, wind_direction#127, wind_direction_qual#128, wind_observation#129, wind_speed#130, wind_speed_qual#131, air_temperature#132, air_temperature_qual#133, STATION NAME#166, CTRY#167, STATE#168, ICAO#169, LAT#170, LON#171, ELEV(M)#172, BEGIN#173, END#174]\n",
      "+- *(5) SortMergeJoin [usaf#122, wban#123], [USAF#164, WBAN#165], Inner\n",
      "   :- *(2) Sort [usaf#122 ASC NULLS FIRST, wban#123 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(usaf#122, wban#123, 200), REPARTITION_BY_NUM, [plan_id=315]\n",
      "   :     +- *(1) Project [substring(value#116, 5, 6) AS usaf#122, substring(value#116, 11, 5) AS wban#123, substring(value#116, 16, 8) AS date#124, substring(value#116, 24, 4) AS time#125, substring(value#116, 42, 5) AS report_type#126, substring(value#116, 61, 3) AS wind_direction#127, substring(value#116, 64, 1) AS wind_direction_qual#128, substring(value#116, 65, 1) AS wind_observation#129, (cast(cast(substring(value#116, 66, 4) as float) as double) / 10.0) AS wind_speed#130, substring(value#116, 70, 1) AS wind_speed_qual#131, (cast(cast(substring(value#116, 88, 5) as float) as double) / 10.0) AS air_temperature#132, substring(value#116, 93, 1) AS air_temperature_qual#133]\n",
      "   :        +- *(1) Filter (((isnotnull(substring(value#116, 5, 6)) AND isnotnull(substring(value#116, 11, 5))) AND bloomfilter#250 of [bf250 USAF#164 estimatedNumRows=12790] filtering [substring(value#116, 5, 6)]) AND bloomfilter#251 of [bf251 WBAN#165 estimatedNumRows=12790] filtering [substring(value#116, 11, 5)])\n",
      "   :           :  :- GenerateBloomFilter bf250, 12790, 0, false, [id=#303]\n",
      "   :           :  :  +- ReusedExchange [USAF#164, WBAN#165, STATION NAME#166, CTRY#167, STATE#168, ICAO#169, LAT#170, LON#171, ELEV(M)#172, BEGIN#173, END#174], Exchange hashpartitioning(USAF#164, WBAN#165, 200), REPARTITION_BY_NUM, [plan_id=266]\n",
      "   :           :  +- GenerateBloomFilter bf251, 12790, 1, false, [id=#305]\n",
      "   :           :     +- ReusedExchange [USAF#164, WBAN#165, STATION NAME#166, CTRY#167, STATE#168, ICAO#169, LAT#170, LON#171, ELEV(M)#172, BEGIN#173, END#174], Exchange hashpartitioning(USAF#164, WBAN#165, 200), REPARTITION_BY_NUM, [plan_id=266]\n",
      "   :           +- FileScan text [value#116] Batched: false, DataFilters: [isnotnull(substring(value#116, 5, 6)), isnotnull(substring(value#116, 11, 5))], Format: Text, Location: InMemoryFileIndex(1 paths)[s3://dimajix-training/data/weather/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "   +- *(4) Sort [USAF#164 ASC NULLS FIRST, WBAN#165 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(USAF#164, WBAN#165, 200), REPARTITION_BY_NUM, [plan_id=266]\n",
      "         +- *(3) Filter (isnotnull(USAF#164) AND isnotnull(WBAN#165))\n",
      "            +- FileScan csv [USAF#164,WBAN#165,STATION NAME#166,CTRY#167,STATE#168,ICAO#169,LAT#170,LON#171,ELEV(M)#172,BEGIN#173,END#174] Batched: false, DataFilters: [isnotnull(USAF#164), isnotnull(WBAN#165)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://dimajix-training/data/weather/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = weather_rep.join(stations_rep, [\"usaf\",\"wban\"])\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Spark removed our explicit repartition, since it doesn't help and replaced it with the implicit repartition with 200 partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Pre-partition and Cache (second try)\n",
    "\n",
    "Now let us try if we can cache the shuffle (repartition) and sort operation. This is useful in cases, where you have to perform multiple joins on the same set of columns, for example with different DataFrames.\n",
    "\n",
    "So let's simply repartition the `weather` DataFrame on the two columns `usaf` and `wban`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, usaf: string, wban: string, date: string, time: string, report_type: string, wind_direction: string, wind_direction_qual: string, wind_observation: string, wind_speed: double, wind_speed_qual: string, air_temperature: double, air_temperature_qual: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_rep = weather.repartition(20, weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution Plan\n",
    "\n",
    "Let's analyze the resulting execution plan. Ideally all the preparation work before the `SortMergeJoin` happens before the `cache` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) Project [usaf#122, wban#123, year#118, date#124, time#125, report_type#126, wind_direction#127, wind_direction_qual#128, wind_observation#129, wind_speed#130, wind_speed_qual#131, air_temperature#132, air_temperature_qual#133, STATION NAME#166, CTRY#167, STATE#168, ICAO#169, LAT#170, LON#171, ELEV(M)#172, BEGIN#173, END#174]\n",
      "+- *(4) SortMergeJoin [usaf#122, wban#123], [USAF#164, WBAN#165], Inner\n",
      "   :- *(1) Sort [usaf#122 ASC NULLS FIRST, wban#123 ASC NULLS FIRST], false, 0\n",
      "   :  +- *(1) Filter (isnotnull(usaf#122) AND isnotnull(wban#123))\n",
      "   :     +- InMemoryTableScan [year#118, usaf#122, wban#123, date#124, time#125, report_type#126, wind_direction#127, wind_direction_qual#128, wind_observation#129, wind_speed#130, wind_speed_qual#131, air_temperature#132, air_temperature_qual#133], [isnotnull(usaf#122), isnotnull(wban#123)]\n",
      "   :           +- InMemoryRelation [year#118, usaf#122, wban#123, date#124, time#125, report_type#126, wind_direction#127, wind_direction_qual#128, wind_observation#129, wind_speed#130, wind_speed_qual#131, air_temperature#132, air_temperature_qual#133], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :                 +- Exchange hashpartitioning(usaf#122, wban#123, 20), REPARTITION_BY_NUM, [plan_id=410]\n",
      "   :                    +- *(1) Project [2003 AS year#118, substring(value#116, 5, 6) AS usaf#122, substring(value#116, 11, 5) AS wban#123, substring(value#116, 16, 8) AS date#124, substring(value#116, 24, 4) AS time#125, substring(value#116, 42, 5) AS report_type#126, substring(value#116, 61, 3) AS wind_direction#127, substring(value#116, 64, 1) AS wind_direction_qual#128, substring(value#116, 65, 1) AS wind_observation#129, (cast(cast(substring(value#116, 66, 4) as float) as double) / 10.0) AS wind_speed#130, substring(value#116, 70, 1) AS wind_speed_qual#131, (cast(cast(substring(value#116, 88, 5) as float) as double) / 10.0) AS air_temperature#132, substring(value#116, 93, 1) AS air_temperature_qual#133]\n",
      "   :                       +- FileScan text [value#116] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex(1 paths)[s3://dimajix-training/data/weather/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "   +- *(3) Sort [USAF#164 ASC NULLS FIRST, WBAN#165 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(USAF#164, WBAN#165, 20), ENSURE_REQUIREMENTS, [plan_id=391]\n",
      "         +- *(2) Filter (isnotnull(USAF#164) AND isnotnull(WBAN#165))\n",
      "            +- FileScan csv [USAF#164,WBAN#165,STATION NAME#166,CTRY#167,STATE#168,ICAO#169,LAT#170,LON#171,ELEV(M)#172,BEGIN#173,END#174] Batched: false, DataFilters: [isnotnull(USAF#164), isnotnull(WBAN#165)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://dimajix-training/data/weather/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = weather_rep.join(stations, [\"usaf\",\"wban\"])\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "Caching seems to fix the new partitioning, and the second DataFrame (`stations`) will be repartitioned accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Pre-partition and Cache (third try)\n",
    "\n",
    "We already partially achieved our goal of caching all preparational work of the `SortMergeJoin`, but the sorting was still preformed after the caching. So let's try to insert an appropriate sort operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, usaf: string, wban: string, date: string, time: string, report_type: string, wind_direction: string, wind_direction_qual: string, wind_observation: string, wind_speed: double, wind_speed_qual: string, air_temperature: double, air_temperature_qual: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Release cache to simplify execution plan\n",
    "weather_rep.unpersist()\n",
    "\n",
    "weather_rep = weather.repartition(200, weather[\"usaf\"], weather[\"wban\"]) \\\n",
    "    .orderBy(weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [usaf#122, wban#123, year#118, date#124, time#125, report_type#126, wind_direction#127, wind_direction_qual#128, wind_observation#129, wind_speed#130, wind_speed_qual#131, air_temperature#132, air_temperature_qual#133, STATION NAME#166, CTRY#167, STATE#168, ICAO#169, LAT#170, LON#171, ELEV(M)#172, BEGIN#173, END#174]\n",
      "+- *(5) SortMergeJoin [usaf#122, wban#123], [USAF#164, WBAN#165], Inner\n",
      "   :- *(2) Sort [usaf#122 ASC NULLS FIRST, wban#123 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(usaf#122, wban#123, 200), ENSURE_REQUIREMENTS, [plan_id=479]\n",
      "   :     +- *(1) Filter (isnotnull(usaf#122) AND isnotnull(wban#123))\n",
      "   :        +- InMemoryTableScan [year#118, usaf#122, wban#123, date#124, time#125, report_type#126, wind_direction#127, wind_direction_qual#128, wind_observation#129, wind_speed#130, wind_speed_qual#131, air_temperature#132, air_temperature_qual#133], [isnotnull(usaf#122), isnotnull(wban#123)]\n",
      "   :              +- InMemoryRelation [year#118, usaf#122, wban#123, date#124, time#125, report_type#126, wind_direction#127, wind_direction_qual#128, wind_observation#129, wind_speed#130, wind_speed_qual#131, air_temperature#132, air_temperature_qual#133], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :                    +- *(2) Sort [usaf#122 ASC NULLS FIRST, wban#123 ASC NULLS FIRST], true, 0\n",
      "   :                       +- Exchange rangepartitioning(usaf#122 ASC NULLS FIRST, wban#123 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=514]\n",
      "   :                          +- Exchange hashpartitioning(usaf#122, wban#123, 200), REPARTITION_BY_NUM, [plan_id=513]\n",
      "   :                             +- *(1) Project [2003 AS year#118, substring(value#116, 5, 6) AS usaf#122, substring(value#116, 11, 5) AS wban#123, substring(value#116, 16, 8) AS date#124, substring(value#116, 24, 4) AS time#125, substring(value#116, 42, 5) AS report_type#126, substring(value#116, 61, 3) AS wind_direction#127, substring(value#116, 64, 1) AS wind_direction_qual#128, substring(value#116, 65, 1) AS wind_observation#129, (cast(cast(substring(value#116, 66, 4) as float) as double) / 10.0) AS wind_speed#130, substring(value#116, 70, 1) AS wind_speed_qual#131, (cast(cast(substring(value#116, 88, 5) as float) as double) / 10.0) AS air_temperature#132, substring(value#116, 93, 1) AS air_temperature_qual#133]\n",
      "   :                                +- FileScan text [value#116] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex(1 paths)[s3://dimajix-training/data/weather/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "   +- *(4) Sort [USAF#164 ASC NULLS FIRST, WBAN#165 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(USAF#164, WBAN#165, 200), ENSURE_REQUIREMENTS, [plan_id=487]\n",
      "         +- *(3) Filter (isnotnull(USAF#164) AND isnotnull(WBAN#165))\n",
      "            +- FileScan csv [USAF#164,WBAN#165,STATION NAME#166,CTRY#167,STATE#168,ICAO#169,LAT#170,LON#171,ELEV(M)#172,BEGIN#173,END#174] Batched: false, DataFilters: [isnotnull(USAF#164), isnotnull(WBAN#165)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://dimajix-training/data/weather/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = weather_rep.join(stations, [\"usaf\",\"wban\"])\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarks\n",
    "\n",
    "We actually created a worse situation: Now we have two sort operations! Definately not what we wanted to have.\n",
    "\n",
    "So let's think for a moment: The `SortMergeJoin` requires that each partition is sorted, but after the repartioning occured. The `orderBy` operation we used above will create a global order over all partitions (and thereby destroy all the repartition work immediately). So we need something else, which still keeps the current partitions but only sort in each partition independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Pre-partition and Cache (final try)\n",
    "\n",
    "Fortunately Spark provides a `sortWithinPartitions` method, which does exactly what it sounds like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, usaf: string, wban: string, date: string, time: string, report_type: string, wind_direction: string, wind_direction_qual: string, wind_observation: string, wind_speed: double, wind_speed_qual: string, air_temperature: double, air_temperature_qual: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Release cache to simplify execution plan\n",
    "weather_rep.unpersist()\n",
    "\n",
    "weather_rep = weather.repartition(200, weather[\"usaf\"], weather[\"wban\"]) \\\n",
    "    .sortWithinPartitions(weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) SortMergeJoin [usaf#122, wban#123], [usaf#164, wban#165], Inner\n",
      ":- *(1) Filter (isnotnull(usaf#122) AND isnotnull(wban#123))\n",
      ":  +- InMemoryTableScan [year#118, usaf#122, wban#123, date#124, time#125, report_type#126, wind_direction#127, wind_direction_qual#128, wind_observation#129, wind_speed#130, wind_speed_qual#131, air_temperature#132, air_temperature_qual#133], [isnotnull(usaf#122), isnotnull(wban#123)]\n",
      ":        +- InMemoryRelation [year#118, usaf#122, wban#123, date#124, time#125, report_type#126, wind_direction#127, wind_direction_qual#128, wind_observation#129, wind_speed#130, wind_speed_qual#131, air_temperature#132, air_temperature_qual#133], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      ":              +- *(2) Sort [usaf#122 ASC NULLS FIRST, wban#123 ASC NULLS FIRST], false, 0\n",
      ":                 +- Exchange hashpartitioning(usaf#122, wban#123, 200), REPARTITION_BY_NUM, [plan_id=593]\n",
      ":                    +- *(1) Project [2003 AS year#118, substring(value#116, 5, 6) AS usaf#122, substring(value#116, 11, 5) AS wban#123, substring(value#116, 16, 8) AS date#124, substring(value#116, 24, 4) AS time#125, substring(value#116, 42, 5) AS report_type#126, substring(value#116, 61, 3) AS wind_direction#127, substring(value#116, 64, 1) AS wind_direction_qual#128, substring(value#116, 65, 1) AS wind_observation#129, (cast(cast(substring(value#116, 66, 4) as float) as double) / 10.0) AS wind_speed#130, substring(value#116, 70, 1) AS wind_speed_qual#131, (cast(cast(substring(value#116, 88, 5) as float) as double) / 10.0) AS air_temperature#132, substring(value#116, 93, 1) AS air_temperature_qual#133]\n",
      ":                       +- FileScan text [value#116] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex(1 paths)[s3://dimajix-training/data/weather/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "+- *(3) Sort [usaf#164 ASC NULLS FIRST, wban#165 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(usaf#164, wban#165, 200), ENSURE_REQUIREMENTS, [plan_id=575]\n",
      "      +- *(2) Filter (isnotnull(usaf#164) AND isnotnull(wban#165))\n",
      "         +- FileScan csv [USAF#164,WBAN#165,STATION NAME#166,CTRY#167,STATE#168,ICAO#169,LAT#170,LON#171,ELEV(M)#172,BEGIN#173,END#174] Batched: false, DataFilters: [isnotnull(USAF#164), isnotnull(WBAN#165)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://dimajix-training/data/weather/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = weather_rep.join(stations, (weather[\"usaf\"] == stations[\"usaf\"]) & (weather[\"wban\"] == stations[\"wban\"]))\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "That looks really good. The filter operation is still executed after the cache, but that cannot be cached such that Spark uses this information.\n",
    "\n",
    "So whenever you want to prepartition data, you need to execute the following steps:\n",
    "* repartition with the join columns and default number of partitions\n",
    "* sortWithinPartitions with the join columns\n",
    "* probably cache (otherwise there is no benefit at all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect WebUI\n",
    "\n",
    "We can also inspect the WebUI and see how everything is executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 1: Build cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1807253"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 2: Use cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1807253"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Repartition & Aggregations\n",
    "\n",
    "Similar to `JOIN` operations, Spark also requires an appropriate partitioning in grouped aggregations. Again, we can use the same strategy and appropriateky prepartition data in cases where multiple joins and aggregations are performed using the same columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Simple Aggregation\n",
    "\n",
    "So let's perform the usual aggregation (but this time without a previous `JOIN`) with groups defined by the station id (`usaf` and `wban`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[usaf#122, wban#123], functions=[min(CASE WHEN (cast(air_temperature_qual#133 as int) = 1) THEN air_temperature#132 END), max(CASE WHEN (cast(air_temperature_qual#133 as int) = 1) THEN air_temperature#132 END)])\n",
      "+- Exchange hashpartitioning(usaf#122, wban#123, 200), ENSURE_REQUIREMENTS, [plan_id=830]\n",
      "   +- *(1) HashAggregate(keys=[usaf#122, wban#123], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#133 as int) = 1) THEN air_temperature#132 END), partial_max(CASE WHEN (cast(air_temperature_qual#133 as int) = 1) THEN air_temperature#132 END)])\n",
      "      +- *(1) Project [substring(value#116, 5, 6) AS usaf#122, substring(value#116, 11, 5) AS wban#123, (cast(cast(substring(value#116, 88, 5) as float) as double) / 10.0) AS air_temperature#132, substring(value#116, 93, 1) AS air_temperature_qual#133]\n",
      "         +- FileScan text [value#116] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex(1 paths)[s3://dimajix-training/data/weather/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = weather.groupBy(weather[\"usaf\"], weather[\"wban\"]).agg(\n",
    "        f.min(f.when(weather.air_temperature_qual == f.lit(1), weather.air_temperature)).alias('min_temp'),\n",
    "        f.max(f.when(weather.air_temperature_qual == f.lit(1), weather.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "Each grouped aggregation is executed with the following steps:\n",
    "1. Perform partial aggregation (`HashAggregate`)\n",
    "2. Shuffle intermediate result (`Exchange hashpartitioning`)\n",
    "3. Perform final aggregation (`HashAggregate`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Aggregation after repartition\n",
    "\n",
    "Now let us perform the same aggregation, but this time let's use the preaggregated weather data set `weather_rep` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, usaf: string, wban: string, date: string, time: string, report_type: string, wind_direction: string, wind_direction_qual: string, wind_observation: string, wind_speed: double, wind_speed_qual: string, air_temperature: double, air_temperature_qual: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_rep = weather.repartition(87, weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[usaf#122, wban#123], functions=[min(CASE WHEN (cast(air_temperature_qual#133 as int) = 1) THEN air_temperature#132 END), max(CASE WHEN (cast(air_temperature_qual#133 as int) = 1) THEN air_temperature#132 END)])\n",
      "+- *(2) HashAggregate(keys=[usaf#122, wban#123], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#133 as int) = 1) THEN air_temperature#132 END), partial_max(CASE WHEN (cast(air_temperature_qual#133 as int) = 1) THEN air_temperature#132 END)])\n",
      "   +- Exchange hashpartitioning(usaf#122, wban#123, 87), REPARTITION_BY_NUM, [plan_id=862]\n",
      "      +- *(1) Project [substring(value#116, 5, 6) AS usaf#122, substring(value#116, 11, 5) AS wban#123, (cast(cast(substring(value#116, 88, 5) as float) as double) / 10.0) AS air_temperature#132, substring(value#116, 93, 1) AS air_temperature_qual#133]\n",
      "         +- FileScan text [value#116] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex(1 paths)[s3://dimajix-training/data/weather/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = weather_rep.groupBy(weather[\"usaf\"], weather[\"wban\"]).agg(\n",
    "        f.min(f.when(weather_rep.air_temperature_qual == f.lit(1), weather_rep.air_temperature)).alias('min_temp'),\n",
    "        f.max(f.when(weather_rep.air_temperature_qual == f.lit(1), weather_rep.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "Spark obviously detects the correct partitioning of the `weather_rep` DataFrame. The sorting actually is not required, but does not hurt either (except performance...). Therefore only two steps are executed after the cache operation:\n",
    "1. Partial aggregation (`HashAggregate`)\n",
    "2. Final aggregation (`HashAggregate`)\n",
    "\n",
    "But note that although you saved a shuffle operation of partial aggregates, in most cases it is not adviseable to prepartition data only for aggregations for the following reasons:\n",
    "* You could perform all aggregations in a single `groupBy` and `agg` chain\n",
    "* In most cases the preaggregated data is significantly smaller than the original data, therefore the shuffle doesn't hurt that much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Interaction between Join, Aggregate & Repartition\n",
    "\n",
    "Now we have seen two operations which require a shuffle of the data. Of course Spark is clever enough to avoid an additional shuffle operation in chains of `JOIN` and grouped aggregations, which use the same aggregation columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Aggregation after Join on same key\n",
    "\n",
    "So let's see what happens with a grouped aggregation after a join operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) HashAggregate(keys=[usaf#100, wban#101], functions=[min(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END), max(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END)])\n",
      "+- *(5) HashAggregate(keys=[usaf#100, wban#101], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END), partial_max(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END)])\n",
      "   +- *(5) Project [usaf#100, wban#101, air_temperature#110, air_temperature_qual#111]\n",
      "      +- *(5) SortMergeJoin [usaf#100, wban#101], [usaf#142, wban#143], Inner\n",
      "         :- *(2) Sort [usaf#100 ASC NULLS FIRST, wban#101 ASC NULLS FIRST], false, 0\n",
      "         :  +- Exchange hashpartitioning(usaf#100, wban#101, 200), ENSURE_REQUIREMENTS, [plan_id=790]\n",
      "         :     +- *(1) Project [substring(value#94, 5, 6) AS usaf#100, substring(value#94, 11, 5) AS wban#101, (cast(cast(substring(value#94, 88, 5) as float) as double) / 10.0) AS air_temperature#110, substring(value#94, 93, 1) AS air_temperature_qual#111]\n",
      "         :        +- *(1) Filter (isnotnull(substring(value#94, 5, 6)) AND isnotnull(substring(value#94, 11, 5)))\n",
      "         :           +- FileScan text [value#94] Batched: false, DataFilters: [isnotnull(substring(value#94, 5, 6)), isnotnull(substring(value#94, 11, 5))], Format: Text, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "         +- *(4) Sort [usaf#142 ASC NULLS FIRST, wban#143 ASC NULLS FIRST], false, 0\n",
      "            +- Exchange hashpartitioning(usaf#142, wban#143, 200), ENSURE_REQUIREMENTS, [plan_id=798]\n",
      "               +- *(3) Filter (isnotnull(usaf#142) AND isnotnull(wban#143))\n",
      "                  +- FileScan csv [USAF#142,WBAN#143] Batched: false, DataFilters: [isnotnull(USAF#142), isnotnull(WBAN#143)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined = weather.join(stations, (weather[\"usaf\"] == stations[\"usaf\"]) & (weather[\"wban\"] == stations[\"wban\"]))\n",
    "result = joined.groupBy(weather[\"usaf\"], weather[\"wban\"]).agg(\n",
    "        f.min(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('min_temp'),\n",
    "        f.max(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "As you can see, Spark performs a single shuffle operation. The order of operation is as follows:\n",
    "1. Filter `NULL` values (it's an inner join)\n",
    "2. Shuffle data on `usaf` and `wban`\n",
    "3. Sort partitions by `usaf` and `wban`\n",
    "4. Perform `SortMergeJoin`\n",
    "5. Perform partial aggregation `HashAggregate`\n",
    "6. Perform final aggregation `HashAggregate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Aggregation after Join using repartitioned data\n",
    "\n",
    "Of course we can also use the pre-repartitioned weather DataFrame. This will work as expected, Spark does not add any additional shuffle operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) HashAggregate(keys=[usaf#100, wban#101], functions=[min(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END), max(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END)])\n",
      "+- *(5) HashAggregate(keys=[usaf#100, wban#101], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END), partial_max(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END)])\n",
      "   +- *(5) Project [usaf#100, wban#101, air_temperature#110, air_temperature_qual#111]\n",
      "      +- *(5) SortMergeJoin [usaf#100, wban#101], [USAF#142, WBAN#143], Inner\n",
      "         :- *(2) Sort [usaf#100 ASC NULLS FIRST, wban#101 ASC NULLS FIRST], false, 0\n",
      "         :  +- Exchange hashpartitioning(usaf#100, wban#101, 200), REPARTITION_BY_NUM, [plan_id=875]\n",
      "         :     +- *(1) Project [substring(value#94, 5, 6) AS usaf#100, substring(value#94, 11, 5) AS wban#101, (cast(cast(substring(value#94, 88, 5) as float) as double) / 10.0) AS air_temperature#110, substring(value#94, 93, 1) AS air_temperature_qual#111]\n",
      "         :        +- *(1) Filter (isnotnull(substring(value#94, 5, 6)) AND isnotnull(substring(value#94, 11, 5)))\n",
      "         :           +- FileScan text [value#94] Batched: false, DataFilters: [isnotnull(substring(value#94, 5, 6)), isnotnull(substring(value#94, 11, 5))], Format: Text, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "         +- *(4) Sort [USAF#142 ASC NULLS FIRST, WBAN#143 ASC NULLS FIRST], false, 0\n",
      "            +- Exchange hashpartitioning(USAF#142, WBAN#143, 200), ENSURE_REQUIREMENTS, [plan_id=883]\n",
      "               +- *(3) Filter (isnotnull(USAF#142) AND isnotnull(WBAN#143))\n",
      "                  +- FileScan csv [USAF#142,WBAN#143] Batched: false, DataFilters: [isnotnull(USAF#142), isnotnull(WBAN#143)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_rep = weather.repartition(84, weather[\"usaf\"], weather[\"wban\"])\n",
    "\n",
    "joined = weather_rep.join(stations, [\"usaf\",\"wban\"])\n",
    "result = joined.groupBy(weather[\"usaf\"], weather[\"wban\"]).agg(\n",
    "        f.min(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('min_temp'),\n",
    "        f.max(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the explicit repartition has been removed by Spark - therefore it doesn't make any sense to `repartition` before a join operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Aggregation after Join with different key\n",
    "\n",
    "So far we only looked at join and grouping operations using the same keys. If we use different keys (for example the country) in both operations, we expect Spark to add an additional shuffle operations. Let's see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(6) HashAggregate(keys=[ctry#145], functions=[min(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END), max(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END)])\n",
      "+- Exchange hashpartitioning(ctry#145, 200), ENSURE_REQUIREMENTS, [plan_id=975]\n",
      "   +- *(5) HashAggregate(keys=[ctry#145], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END), partial_max(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END)])\n",
      "      +- *(5) Project [air_temperature#110, air_temperature_qual#111, CTRY#145]\n",
      "         +- *(5) SortMergeJoin [usaf#100, wban#101], [USAF#142, WBAN#143], Inner\n",
      "            :- *(2) Sort [usaf#100 ASC NULLS FIRST, wban#101 ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(usaf#100, wban#101, 200), ENSURE_REQUIREMENTS, [plan_id=958]\n",
      "            :     +- *(1) Project [substring(value#94, 5, 6) AS usaf#100, substring(value#94, 11, 5) AS wban#101, (cast(cast(substring(value#94, 88, 5) as float) as double) / 10.0) AS air_temperature#110, substring(value#94, 93, 1) AS air_temperature_qual#111]\n",
      "            :        +- *(1) Filter (isnotnull(substring(value#94, 5, 6)) AND isnotnull(substring(value#94, 11, 5)))\n",
      "            :           +- FileScan text [value#94] Batched: false, DataFilters: [isnotnull(substring(value#94, 5, 6)), isnotnull(substring(value#94, 11, 5))], Format: Text, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "            +- *(4) Sort [USAF#142 ASC NULLS FIRST, WBAN#143 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(USAF#142, WBAN#143, 200), ENSURE_REQUIREMENTS, [plan_id=966]\n",
      "                  +- *(3) Filter (isnotnull(USAF#142) AND isnotnull(WBAN#143))\n",
      "                     +- FileScan csv [USAF#142,WBAN#143,CTRY#145] Batched: false, DataFilters: [isnotnull(USAF#142), isnotnull(WBAN#143)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,CTRY:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined = weather.join(stations, [\"usaf\",\"wban\"])\n",
    "result = joined.groupBy(stations[\"ctry\"]).agg(\n",
    "        f.min(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('min_temp'),\n",
    "        f.max(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Aggregation after Broadcast-Join \n",
    "\n",
    "If we use a broadcast join instead of a sort merge join, the we will have a shuffle operation for the aggregation again (since the broadcast join just avoids the shuffle). Let's verify that theory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) HashAggregate(keys=[usaf#100, wban#101], functions=[min(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END), max(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END)])\n",
      "+- Exchange hashpartitioning(usaf#100, wban#101, 200), ENSURE_REQUIREMENTS, [plan_id=1067]\n",
      "   +- *(2) HashAggregate(keys=[usaf#100, wban#101], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END), partial_max(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END)])\n",
      "      +- *(2) Project [usaf#100, wban#101, air_temperature#110, air_temperature_qual#111]\n",
      "         +- *(2) BroadcastHashJoin [usaf#100, wban#101], [USAF#142, WBAN#143], Inner, BuildRight, false\n",
      "            :- *(2) Project [substring(value#94, 5, 6) AS usaf#100, substring(value#94, 11, 5) AS wban#101, (cast(cast(substring(value#94, 88, 5) as float) as double) / 10.0) AS air_temperature#110, substring(value#94, 93, 1) AS air_temperature_qual#111]\n",
      "            :  +- *(2) Filter (isnotnull(substring(value#94, 5, 6)) AND isnotnull(substring(value#94, 11, 5)))\n",
      "            :     +- FileScan text [value#94] Batched: false, DataFilters: [isnotnull(substring(value#94, 5, 6)), isnotnull(substring(value#94, 11, 5))], Format: Text, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "            +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false], input[1, string, false]),false), [plan_id=1061]\n",
      "               +- *(1) Filter (isnotnull(USAF#142) AND isnotnull(WBAN#143))\n",
      "                  +- FileScan csv [USAF#142,WBAN#143] Batched: false, DataFilters: [isnotnull(USAF#142), isnotnull(WBAN#143)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined = weather.join(f.broadcast(stations), [\"usaf\",\"wban\"])\n",
    "result = joined.groupBy(weather[\"usaf\"], weather[\"wban\"]).agg(\n",
    "        f.min(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('min_temp'),\n",
    "        f.max(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Coalesce\n",
    "\n",
    "There is another use case for changing the number of partitions: Writing results to HDFS/S3/whatever. Per design Spark writes each partition into a separate file, and there is no way around that. But when partitions do not contain many records, this may not only be ugly, but also unperformant and might cause additional trouble. Specifically currently HDFS is not designed to handle many small files, but prefers fewer large files instead.\n",
    "\n",
    "Therefore it is often desireable to reduce the number of partitions of a DataFrame just before writing the result to disk. You could perform this task by a `repartition` operation, but this is an expensive operation requiring an additional shuffle operation. Therefore Spark provides an additional method called `coalesce` which can be used to reduce the number of partitions without incurring an additional shuffle. Spark simply logically concatenates multiple partitions into new partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Number of Partitions\n",
    "\n",
    "For this example, we will use the `weather_rep` DataFrame, which contains exactly 200 partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, usaf: string, wban: string, date: string, time: string, report_type: string, wind_direction: string, wind_direction_qual: string, wind_observation: string, wind_speed: double, wind_speed_qual: string, air_temperature: double, air_temperature_qual: string]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_rep = weather.repartition(200, weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_rep.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Reducing partitions before writing\n",
    "\n",
    "In order to reduce the number of partitions, we simply use the `coalesce` method. This is often used to reduce the number of files before writing. But we will see that this might not be the best option and using a `repartiton` might be faster in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write without `coalesce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "joined = weather_rep.join(stations, [\"usaf\", \"wban\"])\n",
    "joined.write.mode(\"overwrite\").parquet(\"/tmp/weather_200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 91 items\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup          0 2023-12-05 13:53 /tmp/weather_200/_SUCCESS\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup       2187 2023-12-05 13:53 /tmp/weather_200/part-00000-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      27660 2023-12-05 13:53 /tmp/weather_200/part-00003-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      70444 2023-12-05 13:53 /tmp/weather_200/part-00005-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      36221 2023-12-05 13:53 /tmp/weather_200/part-00006-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup     136849 2023-12-05 13:53 /tmp/weather_200/part-00011-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      95227 2023-12-05 13:53 /tmp/weather_200/part-00013-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      76908 2023-12-05 13:53 /tmp/weather_200/part-00014-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      75126 2023-12-05 13:53 /tmp/weather_200/part-00016-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      66563 2023-12-05 13:53 /tmp/weather_200/part-00017-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup     193252 2023-12-05 13:53 /tmp/weather_200/part-00025-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      72152 2023-12-05 13:53 /tmp/weather_200/part-00026-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup     101022 2023-12-05 13:53 /tmp/weather_200/part-00028-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      82382 2023-12-05 13:53 /tmp/weather_200/part-00031-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      19540 2023-12-05 13:53 /tmp/weather_200/part-00032-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      73188 2023-12-05 13:53 /tmp/weather_200/part-00033-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      91401 2023-12-05 13:53 /tmp/weather_200/part-00034-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      77539 2023-12-05 13:53 /tmp/weather_200/part-00035-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup     131670 2023-12-05 13:53 /tmp/weather_200/part-00036-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      41524 2023-12-05 13:53 /tmp/weather_200/part-00038-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      61596 2023-12-05 13:53 /tmp/weather_200/part-00039-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      71281 2023-12-05 13:53 /tmp/weather_200/part-00040-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      59650 2023-12-05 13:53 /tmp/weather_200/part-00041-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      67194 2023-12-05 13:53 /tmp/weather_200/part-00043-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      86176 2023-12-05 13:53 /tmp/weather_200/part-00046-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      91094 2023-12-05 13:53 /tmp/weather_200/part-00048-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup     142627 2023-12-05 13:53 /tmp/weather_200/part-00049-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      82287 2023-12-05 13:53 /tmp/weather_200/part-00050-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      63790 2023-12-05 13:53 /tmp/weather_200/part-00053-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      81540 2023-12-05 13:53 /tmp/weather_200/part-00058-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup     210071 2023-12-05 13:53 /tmp/weather_200/part-00059-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      44048 2023-12-05 13:53 /tmp/weather_200/part-00066-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup     103326 2023-12-05 13:53 /tmp/weather_200/part-00068-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      32333 2023-12-05 13:53 /tmp/weather_200/part-00071-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      77300 2023-12-05 13:53 /tmp/weather_200/part-00078-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup       6537 2023-12-05 13:53 /tmp/weather_200/part-00081-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      66866 2023-12-05 13:53 /tmp/weather_200/part-00084-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      31412 2023-12-05 13:53 /tmp/weather_200/part-00088-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      39625 2023-12-05 13:53 /tmp/weather_200/part-00092-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      24744 2023-12-05 13:53 /tmp/weather_200/part-00096-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      80727 2023-12-05 13:53 /tmp/weather_200/part-00098-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      52699 2023-12-05 13:53 /tmp/weather_200/part-00099-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      34738 2023-12-05 13:53 /tmp/weather_200/part-00100-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      64903 2023-12-05 13:53 /tmp/weather_200/part-00102-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup     152332 2023-12-05 13:53 /tmp/weather_200/part-00104-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      64812 2023-12-05 13:53 /tmp/weather_200/part-00108-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      82550 2023-12-05 13:53 /tmp/weather_200/part-00111-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      35102 2023-12-05 13:53 /tmp/weather_200/part-00112-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      97469 2023-12-05 13:53 /tmp/weather_200/part-00113-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      68684 2023-12-05 13:53 /tmp/weather_200/part-00114-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      26437 2023-12-05 13:53 /tmp/weather_200/part-00118-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      70746 2023-12-05 13:53 /tmp/weather_200/part-00119-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup     172739 2023-12-05 13:53 /tmp/weather_200/part-00122-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      87589 2023-12-05 13:53 /tmp/weather_200/part-00124-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      76651 2023-12-05 13:53 /tmp/weather_200/part-00127-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup     129426 2023-12-05 13:53 /tmp/weather_200/part-00129-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      81578 2023-12-05 13:53 /tmp/weather_200/part-00130-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      94632 2023-12-05 13:53 /tmp/weather_200/part-00132-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      61276 2023-12-05 13:53 /tmp/weather_200/part-00133-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup       6297 2023-12-05 13:53 /tmp/weather_200/part-00134-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup     137053 2023-12-05 13:53 /tmp/weather_200/part-00137-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      76359 2023-12-05 13:53 /tmp/weather_200/part-00141-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      41179 2023-12-05 13:53 /tmp/weather_200/part-00143-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      84507 2023-12-05 13:53 /tmp/weather_200/part-00145-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      15915 2023-12-05 13:53 /tmp/weather_200/part-00150-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      55988 2023-12-05 13:53 /tmp/weather_200/part-00151-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      84192 2023-12-05 13:53 /tmp/weather_200/part-00152-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup     101511 2023-12-05 13:53 /tmp/weather_200/part-00154-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup     146683 2023-12-05 13:53 /tmp/weather_200/part-00156-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup     161202 2023-12-05 13:53 /tmp/weather_200/part-00157-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      67849 2023-12-05 13:53 /tmp/weather_200/part-00158-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup     150373 2023-12-05 13:53 /tmp/weather_200/part-00163-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      60313 2023-12-05 13:53 /tmp/weather_200/part-00164-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup     173675 2023-12-05 13:53 /tmp/weather_200/part-00165-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      66478 2023-12-05 13:53 /tmp/weather_200/part-00166-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      22480 2023-12-05 13:53 /tmp/weather_200/part-00171-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      11518 2023-12-05 13:53 /tmp/weather_200/part-00172-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      83535 2023-12-05 13:53 /tmp/weather_200/part-00173-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      72966 2023-12-05 13:53 /tmp/weather_200/part-00174-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      92883 2023-12-05 13:53 /tmp/weather_200/part-00178-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      33936 2023-12-05 13:53 /tmp/weather_200/part-00179-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      83222 2023-12-05 13:53 /tmp/weather_200/part-00181-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      73451 2023-12-05 13:53 /tmp/weather_200/part-00182-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      19096 2023-12-05 13:53 /tmp/weather_200/part-00186-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      74492 2023-12-05 13:53 /tmp/weather_200/part-00187-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      36937 2023-12-05 13:53 /tmp/weather_200/part-00189-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      63029 2023-12-05 13:53 /tmp/weather_200/part-00191-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      88159 2023-12-05 13:53 /tmp/weather_200/part-00192-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      55080 2023-12-05 13:53 /tmp/weather_200/part-00195-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      87029 2023-12-05 13:53 /tmp/weather_200/part-00198-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup      64033 2023-12-05 13:53 /tmp/weather_200/part-00199-561efd9a-67f4-4ea2-b070-fba80ab86166-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /tmp/weather_200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write with `coalesce`\n",
    "\n",
    "First we try to reduce the number of partitions by using `coalesce`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = joined.coalesce(8)\n",
    "result.write.mode(\"overwrite\").parquet(\"/tmp/weather_coalesce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's inspect the result. Check both the files in HDFS and the Spark web UI for performance metrics. Note the uneven file size and that the (expensive) join operation is executed using only 8 tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 items\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup          0 2023-12-05 13:57 /tmp/weather_coalesce/_SUCCESS\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup     581677 2023-12-05 13:57 /tmp/weather_coalesce/part-00000-8331a7ca-8e0f-49b2-9b25-3f15bbb3dddb-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup    1370508 2023-12-05 13:57 /tmp/weather_coalesce/part-00001-8331a7ca-8e0f-49b2-9b25-3f15bbb3dddb-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup     622394 2023-12-05 13:57 /tmp/weather_coalesce/part-00002-8331a7ca-8e0f-49b2-9b25-3f15bbb3dddb-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup     338500 2023-12-05 13:57 /tmp/weather_coalesce/part-00003-8331a7ca-8e0f-49b2-9b25-3f15bbb3dddb-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup     954745 2023-12-05 13:57 /tmp/weather_coalesce/part-00004-8331a7ca-8e0f-49b2-9b25-3f15bbb3dddb-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup     738033 2023-12-05 13:57 /tmp/weather_coalesce/part-00005-8331a7ca-8e0f-49b2-9b25-3f15bbb3dddb-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup    1221386 2023-12-05 13:57 /tmp/weather_coalesce/part-00006-8331a7ca-8e0f-49b2-9b25-3f15bbb3dddb-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup     771904 2023-12-05 13:57 /tmp/weather_coalesce/part-00007-8331a7ca-8e0f-49b2-9b25-3f15bbb3dddb-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /tmp/weather_coalesce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write with `repartition`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = joined.repartition(8)\n",
    "result.write.mode(\"overwrite\").parquet(\"/tmp/weather_repartition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's inspect the result. Check both the files in HDFS and the Spark web UI for performance metrics. Note that the files are much more balanced in size and also note that the join operation is now executed using a 200 tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 items\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup          0 2023-12-05 13:57 /tmp/weather_repartition/_SUCCESS\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup    1856429 2023-12-05 13:57 /tmp/weather_repartition/part-00000-dabeabe3-b108-4413-9c51-c4142528e112-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup    1871473 2023-12-05 13:57 /tmp/weather_repartition/part-00001-dabeabe3-b108-4413-9c51-c4142528e112-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup    1831624 2023-12-05 13:57 /tmp/weather_repartition/part-00002-dabeabe3-b108-4413-9c51-c4142528e112-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup    1874020 2023-12-05 13:57 /tmp/weather_repartition/part-00003-dabeabe3-b108-4413-9c51-c4142528e112-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup    1859269 2023-12-05 13:57 /tmp/weather_repartition/part-00004-dabeabe3-b108-4413-9c51-c4142528e112-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup    1877768 2023-12-05 13:57 /tmp/weather_repartition/part-00005-dabeabe3-b108-4413-9c51-c4142528e112-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup    1845687 2023-12-05 13:57 /tmp/weather_repartition/part-00006-dabeabe3-b108-4413-9c51-c4142528e112-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hdfsadmingroup    1847080 2023-12-05 13:57 /tmp/weather_repartition/part-00007-dabeabe3-b108-4413-9c51-c4142528e112-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /tmp/weather_repartition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 `coalesce` vs `repartition`\n",
    "\n",
    "At first sight, `coalesce` seems to be the more performant option, because it is essentially only a management operation which merges multiple tasks into one. But that also implies that the work of these tasks won't be executed with a parallelism higher than the number of tasks. And if the last operation is expensive (i.e. a join), then overall performance might suffer. In these cases, a repartition might be the better option, altough it will introduce an additional shuffle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Sort before Writing\n",
    "\n",
    "When writing Parquet or ORC files (even indirectly when writing to Hive), you can reduce the file size by sorting the data by appropriate keys. This will help compression algorithms to achieve better file sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = joined.repartition(8).sortWithinPartitions(\"wban\", \"usaf\", \"date\", \"time\")\n",
    "result.write.mode(\"overwrite\").parquet(\"/tmp/weather_sorted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /tmp/weather_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
