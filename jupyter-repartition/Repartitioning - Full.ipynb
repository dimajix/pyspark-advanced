{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repartitioning DataFrames\n",
    "\n",
    "Partitions are a central concept in Apache Spark. They are used for distributing and parallelizing work onto different executors, which run on multiple servers. \n",
    "\n",
    "### Determining Partitions\n",
    "Basically Spark uses two different strategies for splitting up data into multiple partitions:\n",
    "1. When Spark loads data, the records are put into partitions along natural borders. For example every HDFS block (and thereby every file) is represented by a different partition. Therefore the number of partitions of a DataFrame read from disk is solely determined by the number of HDFS blocks\n",
    "2. Certain operations like `JOIN`s and aggregations require that records with the same key are physically in the same partition. This is achieved by a shuffle phase. The number of partitions is specified by the global Spark configuration variable `spark.sql.shuffle.partitions` which has a default value of 200.\n",
    "\n",
    "### Repartitiong Data\n",
    "Since partitions have a huge influence on the execution, Spark also allows you to explicitly change the partitioning schema of a DataFrame. This makes sense only in a very limited (but still important) set of cases, which we will discuss in this notebook.\n",
    "\n",
    "### Weather Example\n",
    "Surprise, surprise, we will again use the weather example and see what explicit repartitioning gives us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/25 17:03:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/25 17:03:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://fe49fe119e4f:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe14c2a6fb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"24G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disable Automatic Broadcast JOINs\n",
    "In order to see the shuffle operations, we need to prevent Spark from executiong `JOIN` operations as broadcast joins. Again this can be turned off by setting the Spark configuration variable `spark.sql.autoBroadcastJoinThreshold` to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load Data\n",
    "\n",
    "First we load the weather data, which consists of the measurement data and some station metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageLocation = \"s3://dimajix-training/data/weather\"\n",
    "# storageLocation = \"/dimajix/data/weather-noaa-sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Measurements\n",
    "\n",
    "Measurements are stored in multiple directories (one per year). But we will limit ourselves to a single year in the analysis to improve readability of execution plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# Read in all years, store them in an Python array\n",
    "raw_weather_per_year = [spark.read.text(storageLocation + \"/\" + str(i)).withColumn(\"year\", f.lit(i)) for i in range(2003,2015)]\n",
    "\n",
    "# Union all years together\n",
    "raw_weather = reduce(lambda l,r: l.union(r), raw_weather_per_year)                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a single year to keep execution plans small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_weather = spark.read.text(storageLocation + \"/2003\").withColumn(\"year\", f.lit(2003))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Measurements\n",
    "\n",
    "Measurements were stored in a proprietary text based format, with some values at fixed positions. We need to extract these values with a simple `SELECT` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = raw_weather.select(\n",
    "    f.col(\"year\"),\n",
    "    f.substring(f.col(\"value\"),5,6).alias(\"usaf\"),\n",
    "    f.substring(f.col(\"value\"),11,5).alias(\"wban\"),\n",
    "    f.substring(f.col(\"value\"),16,8).alias(\"date\"),\n",
    "    f.substring(f.col(\"value\"),24,4).alias(\"time\"),\n",
    "    f.substring(f.col(\"value\"),42,5).alias(\"report_type\"),\n",
    "    f.substring(f.col(\"value\"),61,3).alias(\"wind_direction\"),\n",
    "    f.substring(f.col(\"value\"),64,1).alias(\"wind_direction_qual\"),\n",
    "    f.substring(f.col(\"value\"),65,1).alias(\"wind_observation\"),\n",
    "    (f.substring(f.col(\"value\"),66,4).cast(\"float\") / f.lit(10.0)).alias(\"wind_speed\"),\n",
    "    f.substring(f.col(\"value\"),70,1).alias(\"wind_speed_qual\"),\n",
    "    (f.substring(f.col(\"value\"),88,5).cast(\"float\") / f.lit(10.0)).alias(\"air_temperature\"),\n",
    "    f.substring(f.col(\"value\"),93,1).alias(\"air_temperature_qual\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load Station Metadata\n",
    "\n",
    "We also need to load the weather station meta data containing information about the geo location, country etc of individual weather stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(storageLocation + \"/isd-history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Partitions\n",
    "\n",
    "Since partitions is a concept at the RDD level and a DataFrame per se does not contain an RDD, we need to access the RDD in order to inspect the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Repartitioning Data\n",
    "\n",
    "You can repartition any DataFrame by specifying the target number of partitions and the partitioning columns. While it should be clear what *number of partitions* actually means, the term *partitionng columns* might require some explanation.\n",
    "\n",
    "### Partitioning Columns\n",
    "Except for the case when Spark initially reads data, all DataFrames are partitioned along *partitioning columns*, which means that all records having the same values in the corresponding columns will end up in the same partition. Spark implicitly performs such repartitioning as shuffle operations for `JOIN`s and grouped aggregation (except when a DataFrame already has the correct partitioning columns and number of partitions)\n",
    "\n",
    "### Manual Repartitioning\n",
    "As already mentioned, you can explicitly repartition a DataFrame using teh `repartition()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = weather.repartition(10, weather[\"usaf\"], weather[\"wban\"])\n",
    "result.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Repartition\n",
    "\n",
    "Apart from introducing an additional shuffle operation, repartitioning a dataset will effectevely control the level of parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1807253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count(1)\n",
       "0   1807253"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = weather.repartition(20).select(f.count(\"*\"))\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[count(1)])\n",
      "+- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=86]\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_count(1)])\n",
      "      +- Exchange RoundRobinPartitioning(20), REPARTITION_BY_NUM, [plan_id=78]\n",
      "         +- FileScan text [] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Repartition & Joins\n",
    "\n",
    "As already mentioned, Spark implicitly performs a repartitioning aka shuffle for `JOIN` operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Plan\n",
    "\n",
    "So let us inspect the execution plan of a `JOIN` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [usaf#100, wban#101, 2003 AS year#96, date#102, time#103, report_type#104, wind_direction#105, wind_direction_qual#106, wind_observation#107, wind_speed#108, wind_speed_qual#109, air_temperature#110, air_temperature_qual#111, STATION NAME#144, CTRY#145, STATE#146, ICAO#147, LAT#148, LON#149, ELEV(M)#150, BEGIN#151, END#152]\n",
      "+- *(5) SortMergeJoin [usaf#100, wban#101], [USAF#142, WBAN#143], Inner\n",
      "   :- *(2) Sort [usaf#100 ASC NULLS FIRST, wban#101 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(usaf#100, wban#101, 200), ENSURE_REQUIREMENTS, [plan_id=132]\n",
      "   :     +- *(1) Project [substring(value#94, 5, 6) AS usaf#100, substring(value#94, 11, 5) AS wban#101, substring(value#94, 16, 8) AS date#102, substring(value#94, 24, 4) AS time#103, substring(value#94, 42, 5) AS report_type#104, substring(value#94, 61, 3) AS wind_direction#105, substring(value#94, 64, 1) AS wind_direction_qual#106, substring(value#94, 65, 1) AS wind_observation#107, (cast(cast(substring(value#94, 66, 4) as float) as double) / 10.0) AS wind_speed#108, substring(value#94, 70, 1) AS wind_speed_qual#109, (cast(cast(substring(value#94, 88, 5) as float) as double) / 10.0) AS air_temperature#110, substring(value#94, 93, 1) AS air_temperature_qual#111]\n",
      "   :        +- *(1) Filter (isnotnull(substring(value#94, 5, 6)) AND isnotnull(substring(value#94, 11, 5)))\n",
      "   :           +- FileScan text [value#94] Batched: false, DataFilters: [isnotnull(substring(value#94, 5, 6)), isnotnull(substring(value#94, 11, 5))], Format: Text, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "   +- *(4) Sort [USAF#142 ASC NULLS FIRST, WBAN#143 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(USAF#142, WBAN#143, 200), ENSURE_REQUIREMENTS, [plan_id=140]\n",
      "         +- *(3) Filter (isnotnull(USAF#142) AND isnotnull(WBAN#143))\n",
      "            +- FileScan csv [USAF#142,WBAN#143,STATION NAME#144,CTRY#145,STATE#146,ICAO#147,LAT#148,LON#149,ELEV(M)#150,BEGIN#151,END#152] Batched: false, DataFilters: [isnotnull(USAF#142), isnotnull(WBAN#143)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = weather.join(stations, [\"usaf\", \"wban\"])\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "As we already discussed, each `JOIN` is executed with the following steps\n",
    "1. Filter `NULL` values (it's an inner join)\n",
    "2. Repartition DataFrame on the join columns with 200 partitions\n",
    "3. Sort each partition independently\n",
    "4. Perform a `SortMergeJoin`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Pre-partition data (first try)\n",
    "\n",
    "Now let us try what happens when we explicitly repartition the data before the join operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_rep = weather.repartition(10, weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_rep = stations.repartition(10, stations[\"usaf\"], stations[\"wban\"])\n",
    "stations_rep.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution Plan\n",
    "\n",
    "Let's analyze the resulting execution plan. Ideally all the preparation work before the `SortMergeJoin` happens before the `cache` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [usaf#100, wban#101, 2003 AS year#96, date#102, time#103, report_type#104, wind_direction#105, wind_direction_qual#106, wind_observation#107, wind_speed#108, wind_speed_qual#109, air_temperature#110, air_temperature_qual#111, STATION NAME#144, CTRY#145, STATE#146, ICAO#147, LAT#148, LON#149, ELEV(M)#150, BEGIN#151, END#152]\n",
      "+- *(5) SortMergeJoin [usaf#100, wban#101], [USAF#142, WBAN#143], Inner\n",
      "   :- *(2) Sort [usaf#100 ASC NULLS FIRST, wban#101 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(usaf#100, wban#101, 200), REPARTITION_BY_NUM, [plan_id=239]\n",
      "   :     +- *(1) Project [substring(value#94, 5, 6) AS usaf#100, substring(value#94, 11, 5) AS wban#101, substring(value#94, 16, 8) AS date#102, substring(value#94, 24, 4) AS time#103, substring(value#94, 42, 5) AS report_type#104, substring(value#94, 61, 3) AS wind_direction#105, substring(value#94, 64, 1) AS wind_direction_qual#106, substring(value#94, 65, 1) AS wind_observation#107, (cast(cast(substring(value#94, 66, 4) as float) as double) / 10.0) AS wind_speed#108, substring(value#94, 70, 1) AS wind_speed_qual#109, (cast(cast(substring(value#94, 88, 5) as float) as double) / 10.0) AS air_temperature#110, substring(value#94, 93, 1) AS air_temperature_qual#111]\n",
      "   :        +- *(1) Filter (isnotnull(substring(value#94, 5, 6)) AND isnotnull(substring(value#94, 11, 5)))\n",
      "   :           +- FileScan text [value#94] Batched: false, DataFilters: [isnotnull(substring(value#94, 5, 6)), isnotnull(substring(value#94, 11, 5))], Format: Text, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "   +- *(4) Sort [USAF#142 ASC NULLS FIRST, WBAN#143 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(USAF#142, WBAN#143, 200), REPARTITION_BY_NUM, [plan_id=247]\n",
      "         +- *(3) Filter (isnotnull(USAF#142) AND isnotnull(WBAN#143))\n",
      "            +- FileScan csv [USAF#142,WBAN#143,STATION NAME#144,CTRY#145,STATE#146,ICAO#147,LAT#148,LON#149,ELEV(M)#150,BEGIN#151,END#152] Batched: false, DataFilters: [isnotnull(USAF#142), isnotnull(WBAN#143)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = weather_rep.join(stations_rep, [\"usaf\",\"wban\"])\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Spark removed our explicit repartition, since it doesn't help and replaced it with the implicit repartition with 200 partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Pre-partition and Cache (second try)\n",
    "\n",
    "Now let us try if we can cache the shuffle (repartition) and sort operation. This is useful in cases, where you have to perform multiple joins on the same set of columns, for example with different DataFrames.\n",
    "\n",
    "So let's simply repartition the `weather` DataFrame on the two columns `usaf` and `wban`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, usaf: string, wban: string, date: string, time: string, report_type: string, wind_direction: string, wind_direction_qual: string, wind_observation: string, wind_speed: double, wind_speed_qual: string, air_temperature: double, air_temperature_qual: string]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_rep = weather.repartition(20, weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution Plan\n",
    "\n",
    "Let's analyze the resulting execution plan. Ideally all the preparation work before the `SortMergeJoin` happens before the `cache` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) Project [usaf#100, wban#101, year#96, date#102, time#103, report_type#104, wind_direction#105, wind_direction_qual#106, wind_observation#107, wind_speed#108, wind_speed_qual#109, air_temperature#110, air_temperature_qual#111, STATION NAME#144, CTRY#145, STATE#146, ICAO#147, LAT#148, LON#149, ELEV(M)#150, BEGIN#151, END#152]\n",
      "+- *(4) SortMergeJoin [usaf#100, wban#101], [USAF#142, WBAN#143], Inner\n",
      "   :- *(1) Sort [usaf#100 ASC NULLS FIRST, wban#101 ASC NULLS FIRST], false, 0\n",
      "   :  +- *(1) Filter (isnotnull(usaf#100) AND isnotnull(wban#101))\n",
      "   :     +- InMemoryTableScan [year#96, usaf#100, wban#101, date#102, time#103, report_type#104, wind_direction#105, wind_direction_qual#106, wind_observation#107, wind_speed#108, wind_speed_qual#109, air_temperature#110, air_temperature_qual#111], [isnotnull(usaf#100), isnotnull(wban#101)]\n",
      "   :           +- InMemoryRelation [year#96, usaf#100, wban#101, date#102, time#103, report_type#104, wind_direction#105, wind_direction_qual#106, wind_observation#107, wind_speed#108, wind_speed_qual#109, air_temperature#110, air_temperature_qual#111], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :                 +- Exchange hashpartitioning(usaf#100, wban#101, 20), REPARTITION_BY_NUM, [plan_id=283]\n",
      "   :                    +- *(1) Project [2003 AS year#96, substring(value#94, 5, 6) AS usaf#100, substring(value#94, 11, 5) AS wban#101, substring(value#94, 16, 8) AS date#102, substring(value#94, 24, 4) AS time#103, substring(value#94, 42, 5) AS report_type#104, substring(value#94, 61, 3) AS wind_direction#105, substring(value#94, 64, 1) AS wind_direction_qual#106, substring(value#94, 65, 1) AS wind_observation#107, (cast(cast(substring(value#94, 66, 4) as float) as double) / 10.0) AS wind_speed#108, substring(value#94, 70, 1) AS wind_speed_qual#109, (cast(cast(substring(value#94, 88, 5) as float) as double) / 10.0) AS air_temperature#110, substring(value#94, 93, 1) AS air_temperature_qual#111]\n",
      "   :                       +- FileScan text [value#94] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "   +- *(3) Sort [USAF#142 ASC NULLS FIRST, WBAN#143 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(USAF#142, WBAN#143, 20), ENSURE_REQUIREMENTS, [plan_id=329]\n",
      "         +- *(2) Filter (isnotnull(USAF#142) AND isnotnull(WBAN#143))\n",
      "            +- FileScan csv [USAF#142,WBAN#143,STATION NAME#144,CTRY#145,STATE#146,ICAO#147,LAT#148,LON#149,ELEV(M)#150,BEGIN#151,END#152] Batched: false, DataFilters: [isnotnull(USAF#142), isnotnull(WBAN#143)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = weather_rep.join(stations, [\"usaf\",\"wban\"])\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "Caching seems to fix the new partitioning, and the second DataFrame (`stations`) will be repartitioned accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Pre-partition and Cache (third try)\n",
    "\n",
    "We already partially achieved our goal of caching all preparational work of the `SortMergeJoin`, but the sorting was still preformed after the caching. So let's try to insert an appropriate sort operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, usaf: string, wban: string, date: string, time: string, report_type: string, wind_direction: string, wind_direction_qual: string, wind_observation: string, wind_speed: double, wind_speed_qual: string, air_temperature: double, air_temperature_qual: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Release cache to simplify execution plan\n",
    "weather_rep.unpersist()\n",
    "\n",
    "weather_rep = weather.repartition(200, weather[\"usaf\"], weather[\"wban\"]) \\\n",
    "    .orderBy(weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [usaf#100, wban#101, year#96, date#102, time#103, report_type#104, wind_direction#105, wind_direction_qual#106, wind_observation#107, wind_speed#108, wind_speed_qual#109, air_temperature#110, air_temperature_qual#111, STATION NAME#144, CTRY#145, STATE#146, ICAO#147, LAT#148, LON#149, ELEV(M)#150, BEGIN#151, END#152]\n",
      "+- *(5) SortMergeJoin [usaf#100, wban#101], [USAF#142, WBAN#143], Inner\n",
      "   :- *(2) Sort [usaf#100 ASC NULLS FIRST, wban#101 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(usaf#100, wban#101, 200), ENSURE_REQUIREMENTS, [plan_id=409]\n",
      "   :     +- *(1) Filter (isnotnull(usaf#100) AND isnotnull(wban#101))\n",
      "   :        +- InMemoryTableScan [year#96, usaf#100, wban#101, date#102, time#103, report_type#104, wind_direction#105, wind_direction_qual#106, wind_observation#107, wind_speed#108, wind_speed_qual#109, air_temperature#110, air_temperature_qual#111], [isnotnull(usaf#100), isnotnull(wban#101)]\n",
      "   :              +- InMemoryRelation [year#96, usaf#100, wban#101, date#102, time#103, report_type#104, wind_direction#105, wind_direction_qual#106, wind_observation#107, wind_speed#108, wind_speed_qual#109, air_temperature#110, air_temperature_qual#111], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :                    +- *(2) Sort [usaf#100 ASC NULLS FIRST, wban#101 ASC NULLS FIRST], true, 0\n",
      "   :                       +- Exchange rangepartitioning(usaf#100 ASC NULLS FIRST, wban#101 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=363]\n",
      "   :                          +- Exchange hashpartitioning(usaf#100, wban#101, 200), REPARTITION_BY_NUM, [plan_id=362]\n",
      "   :                             +- *(1) Project [2003 AS year#96, substring(value#94, 5, 6) AS usaf#100, substring(value#94, 11, 5) AS wban#101, substring(value#94, 16, 8) AS date#102, substring(value#94, 24, 4) AS time#103, substring(value#94, 42, 5) AS report_type#104, substring(value#94, 61, 3) AS wind_direction#105, substring(value#94, 64, 1) AS wind_direction_qual#106, substring(value#94, 65, 1) AS wind_observation#107, (cast(cast(substring(value#94, 66, 4) as float) as double) / 10.0) AS wind_speed#108, substring(value#94, 70, 1) AS wind_speed_qual#109, (cast(cast(substring(value#94, 88, 5) as float) as double) / 10.0) AS air_temperature#110, substring(value#94, 93, 1) AS air_temperature_qual#111]\n",
      "   :                                +- FileScan text [value#94] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "   +- *(4) Sort [USAF#142 ASC NULLS FIRST, WBAN#143 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(USAF#142, WBAN#143, 200), ENSURE_REQUIREMENTS, [plan_id=417]\n",
      "         +- *(3) Filter (isnotnull(USAF#142) AND isnotnull(WBAN#143))\n",
      "            +- FileScan csv [USAF#142,WBAN#143,STATION NAME#144,CTRY#145,STATE#146,ICAO#147,LAT#148,LON#149,ELEV(M)#150,BEGIN#151,END#152] Batched: false, DataFilters: [isnotnull(USAF#142), isnotnull(WBAN#143)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = weather_rep.join(stations, [\"usaf\",\"wban\"])\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarks\n",
    "\n",
    "We actually created a worse situation: Now we have two sort operations! Definately not what we wanted to have.\n",
    "\n",
    "So let's think for a moment: The `SortMergeJoin` requires that each partition is sorted, but after the repartioning occured. The `orderBy` operation we used above will create a global order over all partitions (and thereby destroy all the repartition work immediately). So we need something else, which still keeps the current partitions but only sort in each partition independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Pre-partition and Cache (final try)\n",
    "\n",
    "Fortunately Spark provides a `sortWithinPartitions` method, which does exactly what it sounds like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, usaf: string, wban: string, date: string, time: string, report_type: string, wind_direction: string, wind_direction_qual: string, wind_observation: string, wind_speed: double, wind_speed_qual: string, air_temperature: double, air_temperature_qual: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Release cache to simplify execution plan\n",
    "weather_rep.unpersist()\n",
    "\n",
    "weather_rep = weather.repartition(200, weather[\"usaf\"], weather[\"wban\"]) \\\n",
    "    .sortWithinPartitions(weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) SortMergeJoin [usaf#100, wban#101], [usaf#142, wban#143], Inner\n",
      ":- *(1) Filter (isnotnull(usaf#100) AND isnotnull(wban#101))\n",
      ":  +- InMemoryTableScan [year#96, usaf#100, wban#101, date#102, time#103, report_type#104, wind_direction#105, wind_direction_qual#106, wind_observation#107, wind_speed#108, wind_speed_qual#109, air_temperature#110, air_temperature_qual#111], [isnotnull(usaf#100), isnotnull(wban#101)]\n",
      ":        +- InMemoryRelation [year#96, usaf#100, wban#101, date#102, time#103, report_type#104, wind_direction#105, wind_direction_qual#106, wind_observation#107, wind_speed#108, wind_speed_qual#109, air_temperature#110, air_temperature_qual#111], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      ":              +- *(2) Sort [usaf#100 ASC NULLS FIRST, wban#101 ASC NULLS FIRST], false, 0\n",
      ":                 +- Exchange hashpartitioning(usaf#100, wban#101, 200), REPARTITION_BY_NUM, [plan_id=455]\n",
      ":                    +- *(1) Project [2003 AS year#96, substring(value#94, 5, 6) AS usaf#100, substring(value#94, 11, 5) AS wban#101, substring(value#94, 16, 8) AS date#102, substring(value#94, 24, 4) AS time#103, substring(value#94, 42, 5) AS report_type#104, substring(value#94, 61, 3) AS wind_direction#105, substring(value#94, 64, 1) AS wind_direction_qual#106, substring(value#94, 65, 1) AS wind_observation#107, (cast(cast(substring(value#94, 66, 4) as float) as double) / 10.0) AS wind_speed#108, substring(value#94, 70, 1) AS wind_speed_qual#109, (cast(cast(substring(value#94, 88, 5) as float) as double) / 10.0) AS air_temperature#110, substring(value#94, 93, 1) AS air_temperature_qual#111]\n",
      ":                       +- FileScan text [value#94] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "+- *(3) Sort [usaf#142 ASC NULLS FIRST, wban#143 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(usaf#142, wban#143, 200), ENSURE_REQUIREMENTS, [plan_id=495]\n",
      "      +- *(2) Filter (isnotnull(usaf#142) AND isnotnull(wban#143))\n",
      "         +- FileScan csv [USAF#142,WBAN#143,STATION NAME#144,CTRY#145,STATE#146,ICAO#147,LAT#148,LON#149,ELEV(M)#150,BEGIN#151,END#152] Batched: false, DataFilters: [isnotnull(USAF#142), isnotnull(WBAN#143)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = weather_rep.join(stations, (weather[\"usaf\"] == stations[\"usaf\"]) & (weather[\"wban\"] == stations[\"wban\"]))\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "That looks really good. The filter operation is still executed after the cache, but that cannot be cached such that Spark uses this information.\n",
    "\n",
    "So whenever you want to prepartition data, you need to execute the following steps:\n",
    "* repartition with the join columns and default number of partitions\n",
    "* sortWithinPartitions with the join columns\n",
    "* probably cache (otherwise there is no benefit at all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect WebUI\n",
    "\n",
    "We can also inspect the WebUI and see how everything is executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 1: Build cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1807253"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 2: Use cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1807253"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Repartition & Aggregations\n",
    "\n",
    "Similar to `JOIN` operations, Spark also requires an appropriate partitioning in grouped aggregations. Again, we can use the same strategy and appropriateky prepartition data in cases where multiple joins and aggregations are performed using the same columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Simple Aggregation\n",
    "\n",
    "So let's perform the usual aggregation (but this time without a previous `JOIN`) with groups defined by the station id (`usaf` and `wban`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[usaf#100, wban#101], functions=[min(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END), max(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END)])\n",
      "+- Exchange hashpartitioning(usaf#100, wban#101, 200), ENSURE_REQUIREMENTS, [plan_id=699]\n",
      "   +- *(1) HashAggregate(keys=[usaf#100, wban#101], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END), partial_max(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END)])\n",
      "      +- *(1) Project [substring(value#94, 5, 6) AS usaf#100, substring(value#94, 11, 5) AS wban#101, (cast(cast(substring(value#94, 88, 5) as float) as double) / 10.0) AS air_temperature#110, substring(value#94, 93, 1) AS air_temperature_qual#111]\n",
      "         +- FileScan text [value#94] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = weather.groupBy(weather[\"usaf\"], weather[\"wban\"]).agg(\n",
    "        f.min(f.when(weather.air_temperature_qual == f.lit(1), weather.air_temperature)).alias('min_temp'),\n",
    "        f.max(f.when(weather.air_temperature_qual == f.lit(1), weather.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "Each grouped aggregation is executed with the following steps:\n",
    "1. Perform partial aggregation (`HashAggregate`)\n",
    "2. Shuffle intermediate result (`Exchange hashpartitioning`)\n",
    "3. Perform final aggregation (`HashAggregate`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Aggregation after repartition\n",
    "\n",
    "Now let us perform the same aggregation, but this time let's use the preaggregated weather data set `weather_rep` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, usaf: string, wban: string, date: string, time: string, report_type: string, wind_direction: string, wind_direction_qual: string, wind_observation: string, wind_speed: double, wind_speed_qual: string, air_temperature: double, air_temperature_qual: string]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_rep = weather.repartition(87, weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[usaf#100, wban#101], functions=[min(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END), max(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END)])\n",
      "+- *(2) HashAggregate(keys=[usaf#100, wban#101], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END), partial_max(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END)])\n",
      "   +- Exchange hashpartitioning(usaf#100, wban#101, 87), REPARTITION_BY_NUM, [plan_id=731]\n",
      "      +- *(1) Project [substring(value#94, 5, 6) AS usaf#100, substring(value#94, 11, 5) AS wban#101, (cast(cast(substring(value#94, 88, 5) as float) as double) / 10.0) AS air_temperature#110, substring(value#94, 93, 1) AS air_temperature_qual#111]\n",
      "         +- FileScan text [value#94] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = weather_rep.groupBy(weather[\"usaf\"], weather[\"wban\"]).agg(\n",
    "        f.min(f.when(weather_rep.air_temperature_qual == f.lit(1), weather_rep.air_temperature)).alias('min_temp'),\n",
    "        f.max(f.when(weather_rep.air_temperature_qual == f.lit(1), weather_rep.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "Spark obviously detects the correct partitioning of the `weather_rep` DataFrame. The sorting actually is not required, but does not hurt either (except performance...). Therefore only two steps are executed after the cache operation:\n",
    "1. Partial aggregation (`HashAggregate`)\n",
    "2. Final aggregation (`HashAggregate`)\n",
    "\n",
    "But note that although you saved a shuffle operation of partial aggregates, in most cases it is not adviseable to prepartition data only for aggregations for the following reasons:\n",
    "* You could perform all aggregations in a single `groupBy` and `agg` chain\n",
    "* In most cases the preaggregated data is significantly smaller than the original data, therefore the shuffle doesn't hurt that much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Interaction between Join, Aggregate & Repartition\n",
    "\n",
    "Now we have seen two operations which require a shuffle of the data. Of course Spark is clever enough to avoid an additional shuffle operation in chains of `JOIN` and grouped aggregations, which use the same aggregation columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Aggregation after Join on same key\n",
    "\n",
    "So let's see what happens with a grouped aggregation after a join operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) HashAggregate(keys=[usaf#100, wban#101], functions=[min(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END), max(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END)])\n",
      "+- *(5) HashAggregate(keys=[usaf#100, wban#101], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END), partial_max(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END)])\n",
      "   +- *(5) Project [usaf#100, wban#101, air_temperature#110, air_temperature_qual#111]\n",
      "      +- *(5) SortMergeJoin [usaf#100, wban#101], [usaf#142, wban#143], Inner\n",
      "         :- *(2) Sort [usaf#100 ASC NULLS FIRST, wban#101 ASC NULLS FIRST], false, 0\n",
      "         :  +- Exchange hashpartitioning(usaf#100, wban#101, 200), ENSURE_REQUIREMENTS, [plan_id=790]\n",
      "         :     +- *(1) Project [substring(value#94, 5, 6) AS usaf#100, substring(value#94, 11, 5) AS wban#101, (cast(cast(substring(value#94, 88, 5) as float) as double) / 10.0) AS air_temperature#110, substring(value#94, 93, 1) AS air_temperature_qual#111]\n",
      "         :        +- *(1) Filter (isnotnull(substring(value#94, 5, 6)) AND isnotnull(substring(value#94, 11, 5)))\n",
      "         :           +- FileScan text [value#94] Batched: false, DataFilters: [isnotnull(substring(value#94, 5, 6)), isnotnull(substring(value#94, 11, 5))], Format: Text, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "         +- *(4) Sort [usaf#142 ASC NULLS FIRST, wban#143 ASC NULLS FIRST], false, 0\n",
      "            +- Exchange hashpartitioning(usaf#142, wban#143, 200), ENSURE_REQUIREMENTS, [plan_id=798]\n",
      "               +- *(3) Filter (isnotnull(usaf#142) AND isnotnull(wban#143))\n",
      "                  +- FileScan csv [USAF#142,WBAN#143] Batched: false, DataFilters: [isnotnull(USAF#142), isnotnull(WBAN#143)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined = weather.join(stations, (weather[\"usaf\"] == stations[\"usaf\"]) & (weather[\"wban\"] == stations[\"wban\"]))\n",
    "result = joined.groupBy(weather[\"usaf\"], weather[\"wban\"]).agg(\n",
    "        f.min(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('min_temp'),\n",
    "        f.max(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "As you can see, Spark performs a single shuffle operation. The order of operation is as follows:\n",
    "1. Filter `NULL` values (it's an inner join)\n",
    "2. Shuffle data on `usaf` and `wban`\n",
    "3. Sort partitions by `usaf` and `wban`\n",
    "4. Perform `SortMergeJoin`\n",
    "5. Perform partial aggregation `HashAggregate`\n",
    "6. Perform final aggregation `HashAggregate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Aggregation after Join using repartitioned data\n",
    "\n",
    "Of course we can also use the pre-repartitioned weather DataFrame. This will work as expected, Spark does not add any additional shuffle operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) HashAggregate(keys=[usaf#100, wban#101], functions=[min(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END), max(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END)])\n",
      "+- *(5) HashAggregate(keys=[usaf#100, wban#101], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END), partial_max(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END)])\n",
      "   +- *(5) Project [usaf#100, wban#101, air_temperature#110, air_temperature_qual#111]\n",
      "      +- *(5) SortMergeJoin [usaf#100, wban#101], [USAF#142, WBAN#143], Inner\n",
      "         :- *(2) Sort [usaf#100 ASC NULLS FIRST, wban#101 ASC NULLS FIRST], false, 0\n",
      "         :  +- Exchange hashpartitioning(usaf#100, wban#101, 200), REPARTITION_BY_NUM, [plan_id=875]\n",
      "         :     +- *(1) Project [substring(value#94, 5, 6) AS usaf#100, substring(value#94, 11, 5) AS wban#101, (cast(cast(substring(value#94, 88, 5) as float) as double) / 10.0) AS air_temperature#110, substring(value#94, 93, 1) AS air_temperature_qual#111]\n",
      "         :        +- *(1) Filter (isnotnull(substring(value#94, 5, 6)) AND isnotnull(substring(value#94, 11, 5)))\n",
      "         :           +- FileScan text [value#94] Batched: false, DataFilters: [isnotnull(substring(value#94, 5, 6)), isnotnull(substring(value#94, 11, 5))], Format: Text, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "         +- *(4) Sort [USAF#142 ASC NULLS FIRST, WBAN#143 ASC NULLS FIRST], false, 0\n",
      "            +- Exchange hashpartitioning(USAF#142, WBAN#143, 200), ENSURE_REQUIREMENTS, [plan_id=883]\n",
      "               +- *(3) Filter (isnotnull(USAF#142) AND isnotnull(WBAN#143))\n",
      "                  +- FileScan csv [USAF#142,WBAN#143] Batched: false, DataFilters: [isnotnull(USAF#142), isnotnull(WBAN#143)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_rep = weather.repartition(84, weather[\"usaf\"], weather[\"wban\"])\n",
    "\n",
    "joined = weather_rep.join(stations, [\"usaf\",\"wban\"])\n",
    "result = joined.groupBy(weather[\"usaf\"], weather[\"wban\"]).agg(\n",
    "        f.min(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('min_temp'),\n",
    "        f.max(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the explicit repartition has been removed by Spark - therefore it doesn't make any sense to `repartition` before a join operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Aggregation after Join with different key\n",
    "\n",
    "So far we only looked at join and grouping operations using the same keys. If we use different keys (for example the country) in both operations, we expect Spark to add an additional shuffle operations. Let's see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(6) HashAggregate(keys=[ctry#145], functions=[min(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END), max(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END)])\n",
      "+- Exchange hashpartitioning(ctry#145, 200), ENSURE_REQUIREMENTS, [plan_id=975]\n",
      "   +- *(5) HashAggregate(keys=[ctry#145], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END), partial_max(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END)])\n",
      "      +- *(5) Project [air_temperature#110, air_temperature_qual#111, CTRY#145]\n",
      "         +- *(5) SortMergeJoin [usaf#100, wban#101], [USAF#142, WBAN#143], Inner\n",
      "            :- *(2) Sort [usaf#100 ASC NULLS FIRST, wban#101 ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(usaf#100, wban#101, 200), ENSURE_REQUIREMENTS, [plan_id=958]\n",
      "            :     +- *(1) Project [substring(value#94, 5, 6) AS usaf#100, substring(value#94, 11, 5) AS wban#101, (cast(cast(substring(value#94, 88, 5) as float) as double) / 10.0) AS air_temperature#110, substring(value#94, 93, 1) AS air_temperature_qual#111]\n",
      "            :        +- *(1) Filter (isnotnull(substring(value#94, 5, 6)) AND isnotnull(substring(value#94, 11, 5)))\n",
      "            :           +- FileScan text [value#94] Batched: false, DataFilters: [isnotnull(substring(value#94, 5, 6)), isnotnull(substring(value#94, 11, 5))], Format: Text, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "            +- *(4) Sort [USAF#142 ASC NULLS FIRST, WBAN#143 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(USAF#142, WBAN#143, 200), ENSURE_REQUIREMENTS, [plan_id=966]\n",
      "                  +- *(3) Filter (isnotnull(USAF#142) AND isnotnull(WBAN#143))\n",
      "                     +- FileScan csv [USAF#142,WBAN#143,CTRY#145] Batched: false, DataFilters: [isnotnull(USAF#142), isnotnull(WBAN#143)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,CTRY:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined = weather.join(stations, [\"usaf\",\"wban\"])\n",
    "result = joined.groupBy(stations[\"ctry\"]).agg(\n",
    "        f.min(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('min_temp'),\n",
    "        f.max(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Aggregation after Broadcast-Join \n",
    "\n",
    "If we use a broadcast join instead of a sort merge join, the we will have a shuffle operation for the aggregation again (since the broadcast join just avoids the shuffle). Let's verify that theory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) HashAggregate(keys=[usaf#100, wban#101], functions=[min(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END), max(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END)])\n",
      "+- Exchange hashpartitioning(usaf#100, wban#101, 200), ENSURE_REQUIREMENTS, [plan_id=1067]\n",
      "   +- *(2) HashAggregate(keys=[usaf#100, wban#101], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END), partial_max(CASE WHEN (cast(air_temperature_qual#111 as int) = 1) THEN air_temperature#110 END)])\n",
      "      +- *(2) Project [usaf#100, wban#101, air_temperature#110, air_temperature_qual#111]\n",
      "         +- *(2) BroadcastHashJoin [usaf#100, wban#101], [USAF#142, WBAN#143], Inner, BuildRight, false\n",
      "            :- *(2) Project [substring(value#94, 5, 6) AS usaf#100, substring(value#94, 11, 5) AS wban#101, (cast(cast(substring(value#94, 88, 5) as float) as double) / 10.0) AS air_temperature#110, substring(value#94, 93, 1) AS air_temperature_qual#111]\n",
      "            :  +- *(2) Filter (isnotnull(substring(value#94, 5, 6)) AND isnotnull(substring(value#94, 11, 5)))\n",
      "            :     +- FileScan text [value#94] Batched: false, DataFilters: [isnotnull(substring(value#94, 5, 6)), isnotnull(substring(value#94, 11, 5))], Format: Text, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "            +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false], input[1, string, false]),false), [plan_id=1061]\n",
      "               +- *(1) Filter (isnotnull(USAF#142) AND isnotnull(WBAN#143))\n",
      "                  +- FileScan csv [USAF#142,WBAN#143] Batched: false, DataFilters: [isnotnull(USAF#142), isnotnull(WBAN#143)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined = weather.join(f.broadcast(stations), [\"usaf\",\"wban\"])\n",
    "result = joined.groupBy(weather[\"usaf\"], weather[\"wban\"]).agg(\n",
    "        f.min(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('min_temp'),\n",
    "        f.max(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Coalesce\n",
    "\n",
    "There is another use case for changing the number of partitions: Writing results to HDFS/S3/whatever. Per design Spark writes each partition into a separate file, and there is no way around that. But when partitions do not contain many records, this may not only be ugly, but also unperformant and might cause additional trouble. Specifically currently HDFS is not designed to handle many small files, but prefers fewer large files instead.\n",
    "\n",
    "Therefore it is often desireable to reduce the number of partitions of a DataFrame just before writing the result to disk. You could perform this task by a `repartition` operation, but this is an expensive operation requiring an additional shuffle operation. Therefore Spark provides an additional method called `coalesce` which can be used to reduce the number of partitions without incurring an additional shuffle. Spark simply logically concatenates multiple partitions into new partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Number of Partitions\n",
    "\n",
    "For this example, we will use the `weather_rep` DataFrame, which contains exactly 200 partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, usaf: string, wban: string, date: string, time: string, report_type: string, wind_direction: string, wind_direction_qual: string, wind_observation: string, wind_speed: double, wind_speed_qual: string, air_temperature: double, air_temperature_qual: string]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_rep = weather.repartition(200, weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_rep.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Merge Partitions using coalesce\n",
    "\n",
    "In order to reduce the number of partitions, we simply use the `coalesce` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:================================================>       (32 + 5) / 37]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(6) HashAggregate(keys=[], functions=[count(1)])\n",
      "+- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=1687]\n",
      "   +- *(5) HashAggregate(keys=[], functions=[partial_count(1)])\n",
      "      +- *(5) Project\n",
      "         +- *(5) SortMergeJoin [usaf#100, wban#101], [USAF#142, WBAN#143], Inner\n",
      "            :- *(2) Sort [usaf#100 ASC NULLS FIRST, wban#101 ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(usaf#100, wban#101, 200), ENSURE_REQUIREMENTS, [plan_id=1670]\n",
      "            :     +- *(1) Project [substring(value#94, 5, 6) AS usaf#100, substring(value#94, 11, 5) AS wban#101]\n",
      "            :        +- *(1) Filter (isnotnull(substring(value#94, 5, 6)) AND isnotnull(substring(value#94, 11, 5)))\n",
      "            :           +- FileScan text [value#94] Batched: false, DataFilters: [isnotnull(substring(value#94, 5, 6)), isnotnull(substring(value#94, 11, 5))], Format: Text, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "            +- *(4) Sort [USAF#142 ASC NULLS FIRST, WBAN#143 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(USAF#142, WBAN#143, 200), ENSURE_REQUIREMENTS, [plan_id=1678]\n",
      "                  +- *(3) Filter (isnotnull(USAF#142) AND isnotnull(WBAN#143))\n",
      "                     +- FileScan csv [USAF#142,WBAN#143] Batched: false, DataFilters: [isnotnull(USAF#142), isnotnull(WBAN#143)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = weather.join(stations, [\"usaf\", \"wban\"]).select(f.count(\"*\"))\n",
    "result.toPandas()\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:======================================================> (36 + 1) / 37]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(7) HashAggregate(keys=[], functions=[count(1)])\n",
      "+- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=1799]\n",
      "   +- *(6) HashAggregate(keys=[], functions=[partial_count(1)])\n",
      "      +- Coalesce 16\n",
      "         +- *(5) Project\n",
      "            +- *(5) SortMergeJoin [usaf#100, wban#101], [USAF#142, WBAN#143], Inner\n",
      "               :- *(2) Sort [usaf#100 ASC NULLS FIRST, wban#101 ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(usaf#100, wban#101, 200), ENSURE_REQUIREMENTS, [plan_id=1779]\n",
      "               :     +- *(1) Project [substring(value#94, 5, 6) AS usaf#100, substring(value#94, 11, 5) AS wban#101]\n",
      "               :        +- *(1) Filter (isnotnull(substring(value#94, 5, 6)) AND isnotnull(substring(value#94, 11, 5)))\n",
      "               :           +- FileScan text [value#94] Batched: false, DataFilters: [isnotnull(substring(value#94, 5, 6)), isnotnull(substring(value#94, 11, 5))], Format: Text, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "               +- *(4) Sort [USAF#142 ASC NULLS FIRST, WBAN#143 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(USAF#142, WBAN#143, 200), ENSURE_REQUIREMENTS, [plan_id=1787]\n",
      "                     +- *(3) Filter (isnotnull(USAF#142) AND isnotnull(WBAN#143))\n",
      "                        +- FileScan csv [USAF#142,WBAN#143] Batched: false, DataFilters: [isnotnull(USAF#142), isnotnull(WBAN#143)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = weather.join(stations, [\"usaf\", \"wban\"]).coalesce(16).select(f.count(\"*\"))\n",
    "result.toPandas()\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Saving files\n",
    "\n",
    "We already discussed that Spark writes a separate file per partition. So let's see the result when we write the `weather` DataFrame with 20 partitions. We will comapre the results of `repartition` and `coalesce`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write 20 Partitions with `repartition`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "weather.repartition(20).write.mode(\"overwrite\").parquet(\"/tmp/weather_rep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the Result\n",
    "Using a simple HDFS CLI util, we can inspect the result on HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /tmp/weather_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write 20 Partitions with `coalesce`\n",
    "\n",
    "Now let's write the `coalesce`d DataFrame and inspect the result on HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "weather.coalesce(20).write.mode(\"overwrite\").parquet(\"/tmp/weather_rep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /tmp/weather_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
