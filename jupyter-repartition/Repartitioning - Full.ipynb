{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repartitioning DataFrames\n",
    "\n",
    "Partitions are a central concept in Apache Spark. They are used for distributing and parallelizing work onto different executors, which run on multiple servers. \n",
    "\n",
    "### Determining Partitions\n",
    "Basically Spark uses two different strategies for splitting up data into multiple partitions:\n",
    "1. When Spark loads data, the records are put into partitions along natural borders. For example every HDFS block (and thereby every file) is represented by a different partition. Therefore the number of partitions of a DataFrame read from disk is solely determined by the number of HDFS blocks\n",
    "2. Certain operations like `JOIN`s and aggregations require that records with the same key are physically in the same partition. This is achieved by a shuffle phase. The number of partitions is specified by the global Spark configuration variable `spark.sql.shuffle.partitions` which has a default value of 200.\n",
    "\n",
    "### Repartitiong Data\n",
    "Since partitions have a huge influence on the execution, Spark also allows you to explicitly change the partitioning schema of a DataFrame. This makes sense only in a very limited (but still important) set of cases, which we will discuss in this notebook.\n",
    "\n",
    "### Weather Example\n",
    "Surprise, surprise, we will again use the weather example and see what explicit repartitioning gives us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://3b97b950691e:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f143945d450>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"24G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disable Automatic Broadcast JOINs\n",
    "In order to see the shuffle operations, we need to prevent Spark from executiong `JOIN` operations as broadcast joins. Again this can be turned off by setting the Spark configuration variable `spark.sql.autoBroadcastJoinThreshold` to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load Data\n",
    "\n",
    "First we load the weather data, which consists of the measurement data and some station metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageLocation = \"s3://dimajix-training/data/weather\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Measurements\n",
    "\n",
    "Measurements are stored in multiple directories (one per year). But we will limit ourselves to a single year in the analysis to improve readability of execution plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from functools import reduce\n",
    "\n",
    "# Read in all years, store them in an Python array\n",
    "raw_weather_per_year = [spark.read.text(storageLocation + \"/\" + str(i)).withColumn(\"year\", f.lit(i)) for i in range(2003,2015)]\n",
    "\n",
    "# Union all years together\n",
    "raw_weather = reduce(lambda l,r: l.union(r), raw_weather_per_year)                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a single year to keep execution plans small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_weather = spark.read.text(storageLocation + \"/2003\").withColumn(\"year\", f.lit(2003))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Measurements\n",
    "\n",
    "Measurements were stored in a proprietary text based format, with some values at fixed positions. We need to extract these values with a simple `SELECT` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = raw_weather.select(\n",
    "    f.col(\"year\"),\n",
    "    f.substring(f.col(\"value\"),5,6).alias(\"usaf\"),\n",
    "    f.substring(f.col(\"value\"),11,5).alias(\"wban\"),\n",
    "    f.substring(f.col(\"value\"),16,8).alias(\"date\"),\n",
    "    f.substring(f.col(\"value\"),24,4).alias(\"time\"),\n",
    "    f.substring(f.col(\"value\"),42,5).alias(\"report_type\"),\n",
    "    f.substring(f.col(\"value\"),61,3).alias(\"wind_direction\"),\n",
    "    f.substring(f.col(\"value\"),64,1).alias(\"wind_direction_qual\"),\n",
    "    f.substring(f.col(\"value\"),65,1).alias(\"wind_observation\"),\n",
    "    (f.substring(f.col(\"value\"),66,4).cast(\"float\") / f.lit(10.0)).alias(\"wind_speed\"),\n",
    "    f.substring(f.col(\"value\"),70,1).alias(\"wind_speed_qual\"),\n",
    "    (f.substring(f.col(\"value\"),88,5).cast(\"float\") / f.lit(10.0)).alias(\"air_temperature\"),\n",
    "    f.substring(f.col(\"value\"),93,1).alias(\"air_temperature_qual\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load Station Metadata\n",
    "\n",
    "We also need to load the weather station meta data containing information about the geo location, country etc of individual weather stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(storageLocation + \"/isd-history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Partitions\n",
    "\n",
    "Since partitions is a concept at the RDD level and a DataFrame per se does not contain an RDD, we need to access the RDD in order to inspect the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Repartitioning Data\n",
    "\n",
    "You can repartition any DataFrame by specifying the target number of partitions and the partitioning columns. While it should be clear what *number of partitions* actually means, the term *partitionng columns* might require some explanation.\n",
    "\n",
    "### Partitioning Columns\n",
    "Except for the case when Spark initially reads data, all DataFrames are partitioned along *partitioning columns*, which means that all records having the same values in the corresponding columns will end up in the same partition. Spark implicitly performs such repartitioning as shuffle operations for `JOIN`s and grouped aggregation (except when a DataFrame already has the correct partitioning columns and number of partitions)\n",
    "\n",
    "### Manual Repartitioning\n",
    "As already mentioned, you can explicitly repartition a DataFrame using teh `repartition()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_rep = weather.repartition(10, weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Repartition & Joins\n",
    "\n",
    "As already mentioned, Spark implicitly performs a repartitioning aka shuffle for `JOIN` operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Plan\n",
    "\n",
    "So let us inspect the execution plan of a `JOIN` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [usaf#87, wban#88], [usaf#128, wban#129], Inner\n",
      ":- *(2) Sort [usaf#87 ASC NULLS FIRST, wban#88 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(usaf#87, wban#88, 200), true, [id=#69]\n",
      ":     +- *(1) Project [2003 AS year#84, substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, substring(value#82, 16, 8) AS date#89, substring(value#82, 24, 4) AS time#90, substring(value#82, 42, 5) AS report_type#91, substring(value#82, 61, 3) AS wind_direction#92, substring(value#82, 64, 1) AS wind_direction_qual#93, substring(value#82, 65, 1) AS wind_observation#94, (cast(cast(substring(value#82, 66, 4) as float) as double) / 10.0) AS wind_speed#95, substring(value#82, 70, 1) AS wind_speed_qual#96, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      ":        +- *(1) Filter (isnotnull(substring(value#82, 11, 5)) AND isnotnull(substring(value#82, 5, 6)))\n",
      ":           +- FileScan text [value#82] Batched: false, DataFilters: [isnotnull(substring(value#82, 11, 5)), isnotnull(substring(value#82, 5, 6))], Format: Text, Location: InMemoryFileIndex[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "+- *(4) Sort [usaf#128 ASC NULLS FIRST, wban#129 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(usaf#128, wban#129, 200), true, [id=#78]\n",
      "      +- *(3) Project [USAF#128, WBAN#129, STATION NAME#130, CTRY#131, STATE#132, ICAO#133, LAT#134, LON#135, ELEV(M)#136, BEGIN#137, END#138]\n",
      "         +- *(3) Filter (isnotnull(usaf#128) AND isnotnull(wban#129))\n",
      "            +- FileScan csv [USAF#128,WBAN#129,STATION NAME#130,CTRY#131,STATE#132,ICAO#133,LAT#134,LON#135,ELEV(M)#136,BEGIN#137,END#138] Batched: false, DataFilters: [isnotnull(USAF#128), isnotnull(WBAN#129)], Format: CSV, Location: InMemoryFileIndex[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = weather.join(stations, (weather[\"usaf\"] == stations[\"usaf\"]) & (weather[\"wban\"] == stations[\"wban\"]))\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "As we already discussed, each `JOIN` is executed with the following steps\n",
    "1. Filter `NULL` values (it's an inner join)\n",
    "2. Repartition DataFrame on the join columns with 200 partitions\n",
    "3. Sort each partition independently\n",
    "4. Perform a `SortMergeJoin`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Pre-partition data (first try)\n",
    "\n",
    "Now let us try what happens when we explicitly repartition the data before the join operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_rep = weather.repartition(10, weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution Plan\n",
    "\n",
    "Let's analyze the resulting execution plan. Ideally all the preparation work before the `SortMergeJoin` happens before the `cache` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [usaf#87, wban#88, 2003 AS year#84, date#89, time#90, report_type#91, wind_direction#92, wind_direction_qual#93, wind_observation#94, wind_speed#95, wind_speed_qual#96, air_temperature#97, air_temperature_qual#98, STATION NAME#130, CTRY#131, STATE#132, ICAO#133, LAT#134, LON#135, ELEV(M)#136, BEGIN#137, END#138]\n",
      "+- *(5) SortMergeJoin [usaf#87, wban#88], [USAF#128, WBAN#129], Inner\n",
      "   :- *(2) Sort [usaf#87 ASC NULLS FIRST, wban#88 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(usaf#87, wban#88, 200), true, [id=#963]\n",
      "   :     +- *(1) Project [substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, substring(value#82, 16, 8) AS date#89, substring(value#82, 24, 4) AS time#90, substring(value#82, 42, 5) AS report_type#91, substring(value#82, 61, 3) AS wind_direction#92, substring(value#82, 64, 1) AS wind_direction_qual#93, substring(value#82, 65, 1) AS wind_observation#94, (cast(cast(substring(value#82, 66, 4) as float) as double) / 10.0) AS wind_speed#95, substring(value#82, 70, 1) AS wind_speed_qual#96, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      "   :        +- *(1) Filter (isnotnull(substring(value#82, 5, 6)) AND isnotnull(substring(value#82, 11, 5)))\n",
      "   :           +- FileScan text [value#82] Batched: false, DataFilters: [isnotnull(substring(value#82, 5, 6)), isnotnull(substring(value#82, 11, 5))], Format: Text, Location: InMemoryFileIndex[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "   +- *(4) Sort [USAF#128 ASC NULLS FIRST, WBAN#129 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(USAF#128, WBAN#129, 200), true, [id=#972]\n",
      "         +- *(3) Project [USAF#128, WBAN#129, STATION NAME#130, CTRY#131, STATE#132, ICAO#133, LAT#134, LON#135, ELEV(M)#136, BEGIN#137, END#138]\n",
      "            +- *(3) Filter (isnotnull(USAF#128) AND isnotnull(WBAN#129))\n",
      "               +- FileScan csv [USAF#128,WBAN#129,STATION NAME#130,CTRY#131,STATE#132,ICAO#133,LAT#134,LON#135,ELEV(M)#136,BEGIN#137,END#138] Batched: false, DataFilters: [isnotnull(USAF#128), isnotnull(WBAN#129)], Format: CSV, Location: InMemoryFileIndex[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = weather_rep.join(stations, [\"usaf\",\"wban\"])\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Spark removed our explicit repartition, since it doesn't help and replaced it with the implicit repartition with 200 partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Pre-partition and Cache (second try)\n",
    "\n",
    "Now let us try if we can cache the shuffle (repartition) and sort operation. This is useful in cases, where you have to perform multiple joins on the same set of columns, for example with different DataFrames.\n",
    "\n",
    "So let's simply repartition the `weather` DataFrame on the two columns `usaf` and `wban`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, usaf: string, wban: string, date: string, time: string, report_type: string, wind_direction: string, wind_direction_qual: string, wind_observation: string, wind_speed: double, wind_speed_qual: string, air_temperature: double, air_temperature_qual: string]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_rep = weather.repartition(20, weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution Plan\n",
    "\n",
    "Let's analyze the resulting execution plan. Ideally all the preparation work before the `SortMergeJoin` happens before the `cache` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [usaf#87, wban#88], [usaf#128, wban#129], Inner\n",
      ":- *(2) Sort [usaf#87 ASC NULLS FIRST, wban#88 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(usaf#87, wban#88, 200), true, [id=#550]\n",
      ":     +- *(1) Filter (isnotnull(wban#88) AND isnotnull(usaf#87))\n",
      ":        +- InMemoryTableScan [year#84, usaf#87, wban#88, date#89, time#90, report_type#91, wind_direction#92, wind_direction_qual#93, wind_observation#94, wind_speed#95, wind_speed_qual#96, air_temperature#97, air_temperature_qual#98], [isnotnull(wban#88), isnotnull(usaf#87)]\n",
      ":              +- InMemoryRelation [year#84, usaf#87, wban#88, date#89, time#90, report_type#91, wind_direction#92, wind_direction_qual#93, wind_observation#94, wind_speed#95, wind_speed_qual#96, air_temperature#97, air_temperature_qual#98], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      ":                    +- Exchange hashpartitioning(usaf#87, wban#88, 20), false, [id=#402]\n",
      ":                       +- *(1) Project [2003 AS year#84, substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, substring(value#82, 16, 8) AS date#89, substring(value#82, 24, 4) AS time#90, substring(value#82, 42, 5) AS report_type#91, substring(value#82, 61, 3) AS wind_direction#92, substring(value#82, 64, 1) AS wind_direction_qual#93, substring(value#82, 65, 1) AS wind_observation#94, (cast(cast(substring(value#82, 66, 4) as float) as double) / 10.0) AS wind_speed#95, substring(value#82, 70, 1) AS wind_speed_qual#96, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      ":                          +- FileScan text [value#82] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "+- *(4) Sort [usaf#128 ASC NULLS FIRST, wban#129 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(usaf#128, wban#129, 200), true, [id=#559]\n",
      "      +- *(3) Project [USAF#128, WBAN#129, STATION NAME#130, CTRY#131, STATE#132, ICAO#133, LAT#134, LON#135, ELEV(M)#136, BEGIN#137, END#138]\n",
      "         +- *(3) Filter (isnotnull(usaf#128) AND isnotnull(wban#129))\n",
      "            +- FileScan csv [USAF#128,WBAN#129,STATION NAME#130,CTRY#131,STATE#132,ICAO#133,LAT#134,LON#135,ELEV(M)#136,BEGIN#137,END#138] Batched: false, DataFilters: [isnotnull(USAF#128), isnotnull(WBAN#129)], Format: CSV, Location: InMemoryFileIndex[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = weather_rep.join(stations, [\"usaf\",\"wban\"])\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "Ouch, now we have *two* shuffle operations. The reason is that Spark will use the default number of partitions for the JOIN operation, but we cached a differently partitioned DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Pre-partition and Cache (third try)\n",
    "\n",
    "Now let us try if we can cache the shuffle (repartition) and sort operation. This is useful in cases, where you have to perform multiple joins on the same set of columns, for example with different DataFrames.\n",
    "\n",
    "So let's simply repartition the `weather` DataFrame on the two columns `usaf` and `wban`. We also have to use 200 partitions, because this is what Spark will use for `JOIN` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, usaf: string, wban: string, date: string, time: string, report_type: string, wind_direction: string, wind_direction_qual: string, wind_observation: string, wind_speed: double, wind_speed_qual: string, air_temperature: double, air_temperature_qual: string]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_rep = weather.repartition(200, weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution Plan\n",
    "\n",
    "Let's analyze the resulting execution plan. Ideally all the preparation work before the `SortMergeJoin` happens before the `cache` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) Project [usaf#87, wban#88, year#84, date#89, time#90, report_type#91, wind_direction#92, wind_direction_qual#93, wind_observation#94, wind_speed#95, wind_speed_qual#96, air_temperature#97, air_temperature_qual#98, STATION NAME#130, CTRY#131, STATE#132, ICAO#133, LAT#134, LON#135, ELEV(M)#136, BEGIN#137, END#138]\n",
      "+- *(4) SortMergeJoin [usaf#87, wban#88], [USAF#128, WBAN#129], Inner\n",
      "   :- *(1) Sort [usaf#87 ASC NULLS FIRST, wban#88 ASC NULLS FIRST], false, 0\n",
      "   :  +- *(1) Filter (isnotnull(wban#88) AND isnotnull(usaf#87))\n",
      "   :     +- InMemoryTableScan [year#84, usaf#87, wban#88, date#89, time#90, report_type#91, wind_direction#92, wind_direction_qual#93, wind_observation#94, wind_speed#95, wind_speed_qual#96, air_temperature#97, air_temperature_qual#98], [isnotnull(wban#88), isnotnull(usaf#87)]\n",
      "   :           +- InMemoryRelation [year#84, usaf#87, wban#88, date#89, time#90, report_type#91, wind_direction#92, wind_direction_qual#93, wind_observation#94, wind_speed#95, wind_speed_qual#96, air_temperature#97, air_temperature_qual#98], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :                 +- Exchange hashpartitioning(usaf#87, wban#88, 200), false, [id=#992]\n",
      "   :                    +- *(1) Project [2003 AS year#84, substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, substring(value#82, 16, 8) AS date#89, substring(value#82, 24, 4) AS time#90, substring(value#82, 42, 5) AS report_type#91, substring(value#82, 61, 3) AS wind_direction#92, substring(value#82, 64, 1) AS wind_direction_qual#93, substring(value#82, 65, 1) AS wind_observation#94, (cast(cast(substring(value#82, 66, 4) as float) as double) / 10.0) AS wind_speed#95, substring(value#82, 70, 1) AS wind_speed_qual#96, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      "   :                       +- FileScan text [value#82] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "   +- *(3) Sort [USAF#128 ASC NULLS FIRST, WBAN#129 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(USAF#128, WBAN#129, 200), true, [id=#1029]\n",
      "         +- *(2) Project [USAF#128, WBAN#129, STATION NAME#130, CTRY#131, STATE#132, ICAO#133, LAT#134, LON#135, ELEV(M)#136, BEGIN#137, END#138]\n",
      "            +- *(2) Filter (isnotnull(USAF#128) AND isnotnull(WBAN#129))\n",
      "               +- FileScan csv [USAF#128,WBAN#129,STATION NAME#130,CTRY#131,STATE#132,ICAO#133,LAT#134,LON#135,ELEV(M)#136,BEGIN#137,END#138] Batched: false, DataFilters: [isnotnull(USAF#128), isnotnull(WBAN#129)], Format: CSV, Location: InMemoryFileIndex[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = weather_rep.join(stations, [\"usaf\",\"wban\"])\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "We did not reach completely what we wanted. The `sort` and `filter` operation still occur after the cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Pre-partition and Cache (fourth try)\n",
    "\n",
    "We already partially achieved our goal of caching all preparational work of the `SortMergeJoin`, but the sorting was still preformed after the caching. So let's try to insert an appropriate sort operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, usaf: string, wban: string, date: string, time: string, report_type: string, wind_direction: string, wind_direction_qual: string, wind_observation: string, wind_speed: double, wind_speed_qual: string, air_temperature: double, air_temperature_qual: string]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Release cache to simplify execution plan\n",
    "weather_rep.unpersist()\n",
    "\n",
    "weather_rep = weather.repartition(200, weather[\"usaf\"], weather[\"wban\"]) \\\n",
    "    .orderBy(weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [usaf#87, wban#88], [usaf#128, wban#129], Inner\n",
      ":- *(2) Sort [usaf#87 ASC NULLS FIRST, wban#88 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(usaf#87, wban#88, 200), true, [id=#662]\n",
      ":     +- *(1) Filter (isnotnull(wban#88) AND isnotnull(usaf#87))\n",
      ":        +- InMemoryTableScan [year#84, usaf#87, wban#88, date#89, time#90, report_type#91, wind_direction#92, wind_direction_qual#93, wind_observation#94, wind_speed#95, wind_speed_qual#96, air_temperature#97, air_temperature_qual#98], [isnotnull(wban#88), isnotnull(usaf#87)]\n",
      ":              +- InMemoryRelation [year#84, usaf#87, wban#88, date#89, time#90, report_type#91, wind_direction#92, wind_direction_qual#93, wind_observation#94, wind_speed#95, wind_speed_qual#96, air_temperature#97, air_temperature_qual#98], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      ":                    +- *(2) Sort [usaf#87 ASC NULLS FIRST, wban#88 ASC NULLS FIRST], true, 0\n",
      ":                       +- Exchange rangepartitioning(usaf#87 ASC NULLS FIRST, wban#88 ASC NULLS FIRST, 200), true, [id=#623]\n",
      ":                          +- Exchange hashpartitioning(usaf#87, wban#88, 200), false, [id=#622]\n",
      ":                             +- *(1) Project [2003 AS year#84, substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, substring(value#82, 16, 8) AS date#89, substring(value#82, 24, 4) AS time#90, substring(value#82, 42, 5) AS report_type#91, substring(value#82, 61, 3) AS wind_direction#92, substring(value#82, 64, 1) AS wind_direction_qual#93, substring(value#82, 65, 1) AS wind_observation#94, (cast(cast(substring(value#82, 66, 4) as float) as double) / 10.0) AS wind_speed#95, substring(value#82, 70, 1) AS wind_speed_qual#96, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      ":                                +- FileScan text [value#82] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "+- *(4) Sort [usaf#128 ASC NULLS FIRST, wban#129 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(usaf#128, wban#129, 200), true, [id=#671]\n",
      "      +- *(3) Project [USAF#128, WBAN#129, STATION NAME#130, CTRY#131, STATE#132, ICAO#133, LAT#134, LON#135, ELEV(M)#136, BEGIN#137, END#138]\n",
      "         +- *(3) Filter (isnotnull(usaf#128) AND isnotnull(wban#129))\n",
      "            +- FileScan csv [USAF#128,WBAN#129,STATION NAME#130,CTRY#131,STATE#132,ICAO#133,LAT#134,LON#135,ELEV(M)#136,BEGIN#137,END#138] Batched: false, DataFilters: [isnotnull(USAF#128), isnotnull(WBAN#129)], Format: CSV, Location: InMemoryFileIndex[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = weather_rep.join(stations, [\"usaf\",\"wban\"])\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarks\n",
    "\n",
    "We actually created a worse situation: Now we have two sort operations! Definately not what we wanted to have.\n",
    "\n",
    "So let's think for a moment: The `SortMergeJoin` requires that each partition is sorted, but after the repartioning occured. The `orderBy` operation we used above will create a global order over all partitions (and thereby destroy all the repartition work immediately). So we need something else, which still keeps the current partitions but only sort in each partition independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Pre-partition and Cache (final try)\n",
    "\n",
    "Fortunately Spark provides a `sortWithinPartitions` method, which does exactly what it sounds like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, usaf: string, wban: string, date: string, time: string, report_type: string, wind_direction: string, wind_direction_qual: string, wind_observation: string, wind_speed: double, wind_speed_qual: string, air_temperature: double, air_temperature_qual: string]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Release cache to simplify execution plan\n",
    "weather_rep.unpersist()\n",
    "\n",
    "weather_rep = weather.repartition(200, weather[\"usaf\"], weather[\"wban\"]) \\\n",
    "    .sortWithinPartitions(weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) SortMergeJoin [usaf#87, wban#88], [usaf#128, wban#129], Inner\n",
      ":- *(1) Filter (isnotnull(wban#88) AND isnotnull(usaf#87))\n",
      ":  +- InMemoryTableScan [year#84, usaf#87, wban#88, date#89, time#90, report_type#91, wind_direction#92, wind_direction_qual#93, wind_observation#94, wind_speed#95, wind_speed_qual#96, air_temperature#97, air_temperature_qual#98], [isnotnull(wban#88), isnotnull(usaf#87)]\n",
      ":        +- InMemoryRelation [year#84, usaf#87, wban#88, date#89, time#90, report_type#91, wind_direction#92, wind_direction_qual#93, wind_observation#94, wind_speed#95, wind_speed_qual#96, air_temperature#97, air_temperature_qual#98], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      ":              +- *(2) Sort [usaf#87 ASC NULLS FIRST, wban#88 ASC NULLS FIRST], false, 0\n",
      ":                 +- Exchange hashpartitioning(usaf#87, wban#88, 200), false, [id=#694]\n",
      ":                    +- *(1) Project [2003 AS year#84, substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, substring(value#82, 16, 8) AS date#89, substring(value#82, 24, 4) AS time#90, substring(value#82, 42, 5) AS report_type#91, substring(value#82, 61, 3) AS wind_direction#92, substring(value#82, 64, 1) AS wind_direction_qual#93, substring(value#82, 65, 1) AS wind_observation#94, (cast(cast(substring(value#82, 66, 4) as float) as double) / 10.0) AS wind_speed#95, substring(value#82, 70, 1) AS wind_speed_qual#96, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      ":                       +- FileScan text [value#82] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "+- *(3) Sort [usaf#128 ASC NULLS FIRST, wban#129 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(usaf#128, wban#129, 200), true, [id=#727]\n",
      "      +- *(2) Project [USAF#128, WBAN#129, STATION NAME#130, CTRY#131, STATE#132, ICAO#133, LAT#134, LON#135, ELEV(M)#136, BEGIN#137, END#138]\n",
      "         +- *(2) Filter (isnotnull(usaf#128) AND isnotnull(wban#129))\n",
      "            +- FileScan csv [USAF#128,WBAN#129,STATION NAME#130,CTRY#131,STATE#132,ICAO#133,LAT#134,LON#135,ELEV(M)#136,BEGIN#137,END#138] Batched: false, DataFilters: [isnotnull(USAF#128), isnotnull(WBAN#129)], Format: CSV, Location: InMemoryFileIndex[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = weather_rep.join(stations, (weather[\"usaf\"] == stations[\"usaf\"]) & (weather[\"wban\"] == stations[\"wban\"]))\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "That looks really good. The filter operation is still executed after the cache, but that cannot be cached such that Spark uses this information.\n",
    "\n",
    "So whenever you want to prepartition data, you need to execute the following steps:\n",
    "* repartition with the join columns and default number of partitions\n",
    "* sortWithinPartitions with the join columns\n",
    "* probably cache (otherwise there is no benefit at all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect WebUI\n",
    "\n",
    "We can also inspect the WebUI and see how everything is executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 1: Build cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1798753"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 2: Use cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1798753"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Repartition & Aggregations\n",
    "\n",
    "Similar to `JOIN` operations, Spark also requires an appropriate partitioning in grouped aggregations. Again, we can use the same strategy and appropriateky prepartition data in cases where multiple joins and aggregations are performed using the same columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Simple Aggregation\n",
    "\n",
    "So let's perform the usual aggregation (but this time without a previous `JOIN`) with groups defined by the station id (`usaf` and `wban`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[usaf#87, wban#88], functions=[min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "+- Exchange hashpartitioning(usaf#87, wban#88, 200), true, [id=#779]\n",
      "   +- *(1) HashAggregate(keys=[usaf#87, wban#88], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), partial_max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "      +- *(1) Project [substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      "         +- FileScan text [value#82] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = weather.groupBy(weather[\"usaf\"], weather[\"wban\"]).agg(\n",
    "        f.min(f.when(weather.air_temperature_qual == f.lit(1), weather.air_temperature)).alias('min_temp'),\n",
    "        f.max(f.when(weather.air_temperature_qual == f.lit(1), weather.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "Each grouped aggregation is executed with the following steps:\n",
    "1. Perform partial aggregation (`HashAggregate`)\n",
    "2. Shuffle intermediate result (`Exchange hashpartitioning`)\n",
    "3. Perform final aggregation (`HashAggregate`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Aggregation after repartition\n",
    "\n",
    "Now let us perform the same aggregation, but this time let's use the preaggregated weather data set `weather_rep` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, usaf: string, wban: string, date: string, time: string, report_type: string, wind_direction: string, wind_direction_qual: string, wind_observation: string, wind_speed: double, wind_speed_qual: string, air_temperature: double, air_temperature_qual: string]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_rep = weather.repartition(87, weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[usaf#87, wban#88], functions=[min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "+- *(2) HashAggregate(keys=[usaf#87, wban#88], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), partial_max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "   +- Exchange hashpartitioning(usaf#87, wban#88, 87), false, [id=#391]\n",
      "      +- *(1) Project [substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      "         +- FileScan text [value#82] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = weather_rep.groupBy(weather[\"usaf\"], weather[\"wban\"]).agg(\n",
    "        f.min(f.when(weather_rep.air_temperature_qual == f.lit(1), weather_rep.air_temperature)).alias('min_temp'),\n",
    "        f.max(f.when(weather_rep.air_temperature_qual == f.lit(1), weather_rep.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "Spark obviously detects the correct partitioning of the `weather_rep` DataFrame. The sorting actually is not required, but does not hurt either (except performance...). Therefore only two steps are executed after the cache operation:\n",
    "1. Partial aggregation (`HashAggregate`)\n",
    "2. Final aggregation (`HashAggregate`)\n",
    "\n",
    "But note that although you saved a shuffle operation of partial aggregates, in most cases it is not adviseable to prepartition data only for aggregations for the following reasons:\n",
    "* You could perform all aggregations in a single `groupBy` and `agg` chain\n",
    "* In most cases the preaggregated data is significantly smaller than the original data, therefore the shuffle doesn't hurt that much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Interaction between Join, Aggregate & Repartition\n",
    "\n",
    "Now we have seen two operations which require a shuffle of the data. Of course Spark is clever enough to avoid an additional shuffle operation in chains of `JOIN` and grouped aggregations, which use the same aggregation columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Aggregation after Join on same key\n",
    "\n",
    "So let's see what happens with a grouped aggregation after a join operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) HashAggregate(keys=[usaf#87, wban#88], functions=[min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "+- *(5) HashAggregate(keys=[usaf#87, wban#88], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), partial_max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "   +- *(5) Project [usaf#87, wban#88, air_temperature#97, air_temperature_qual#98]\n",
      "      +- *(5) SortMergeJoin [usaf#87, wban#88], [usaf#128, wban#129], Inner\n",
      "         :- *(2) Sort [usaf#87 ASC NULLS FIRST, wban#88 ASC NULLS FIRST], false, 0\n",
      "         :  +- Exchange hashpartitioning(usaf#87, wban#88, 200), true, [id=#840]\n",
      "         :     +- *(1) Project [substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      "         :        +- *(1) Filter (isnotnull(substring(value#82, 5, 6)) AND isnotnull(substring(value#82, 11, 5)))\n",
      "         :           +- FileScan text [value#82] Batched: false, DataFilters: [isnotnull(substring(value#82, 5, 6)), isnotnull(substring(value#82, 11, 5))], Format: Text, Location: InMemoryFileIndex[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "         +- *(4) Sort [usaf#128 ASC NULLS FIRST, wban#129 ASC NULLS FIRST], false, 0\n",
      "            +- Exchange hashpartitioning(usaf#128, wban#129, 200), true, [id=#849]\n",
      "               +- *(3) Project [USAF#128, WBAN#129]\n",
      "                  +- *(3) Filter (isnotnull(usaf#128) AND isnotnull(wban#129))\n",
      "                     +- FileScan csv [USAF#128,WBAN#129] Batched: false, DataFilters: [isnotnull(USAF#128), isnotnull(WBAN#129)], Format: CSV, Location: InMemoryFileIndex[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined = weather.join(stations, (weather[\"usaf\"] == stations[\"usaf\"]) & (weather[\"wban\"] == stations[\"wban\"]))\n",
    "result = joined.groupBy(weather[\"usaf\"], weather[\"wban\"]).agg(\n",
    "        f.min(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('min_temp'),\n",
    "        f.max(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "As you can see, Spark performs a single shuffle operation. The order of operation is as follows:\n",
    "1. Filter `NULL` values (it's an inner join)\n",
    "2. Shuffle data on `usaf` and `wban`\n",
    "3. Sort partitions by `usaf` and `wban`\n",
    "4. Perform `SortMergeJoin`\n",
    "5. Perform partial aggregation `HashAggregate`\n",
    "6. Perform final aggregation `HashAggregate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Aggregation after Join using repartitioned data\n",
    "\n",
    "Of course we can also use the pre-repartitioned weather DataFrame. This will work as expected, Spark does not add any additional shuffle operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) HashAggregate(keys=[usaf#87, wban#88], functions=[min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "+- *(5) HashAggregate(keys=[usaf#87, wban#88], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), partial_max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "   +- *(5) Project [usaf#87, wban#88, air_temperature#97, air_temperature_qual#98]\n",
      "      +- *(5) SortMergeJoin [usaf#87, wban#88], [USAF#128, WBAN#129], Inner\n",
      "         :- *(2) Sort [usaf#87 ASC NULLS FIRST, wban#88 ASC NULLS FIRST], false, 0\n",
      "         :  +- Exchange hashpartitioning(usaf#87, wban#88, 200), true, [id=#893]\n",
      "         :     +- *(1) Project [substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      "         :        +- *(1) Filter (isnotnull(substring(value#82, 5, 6)) AND isnotnull(substring(value#82, 11, 5)))\n",
      "         :           +- FileScan text [value#82] Batched: false, DataFilters: [isnotnull(substring(value#82, 5, 6)), isnotnull(substring(value#82, 11, 5))], Format: Text, Location: InMemoryFileIndex[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "         +- *(4) Sort [USAF#128 ASC NULLS FIRST, WBAN#129 ASC NULLS FIRST], false, 0\n",
      "            +- Exchange hashpartitioning(USAF#128, WBAN#129, 200), true, [id=#902]\n",
      "               +- *(3) Project [USAF#128, WBAN#129]\n",
      "                  +- *(3) Filter (isnotnull(USAF#128) AND isnotnull(WBAN#129))\n",
      "                     +- FileScan csv [USAF#128,WBAN#129] Batched: false, DataFilters: [isnotnull(USAF#128), isnotnull(WBAN#129)], Format: CSV, Location: InMemoryFileIndex[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_rep = weather.repartition(84, weather[\"usaf\"], weather[\"wban\"])\n",
    "\n",
    "joined = weather_rep.join(stations, [\"usaf\",\"wban\"])\n",
    "result = joined.groupBy(weather[\"usaf\"], weather[\"wban\"]).agg(\n",
    "        f.min(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('min_temp'),\n",
    "        f.max(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the explicit repartition has been removed by Spark - therefore it doesn't make any sense to `repartition` before a join operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Aggregation after Join with different key\n",
    "\n",
    "So far we only looked at join and grouping operations using the same keys. If we use different keys (for example the country) in both operations, we expect Spark to add an additional shuffle operations. Let's see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(6) HashAggregate(keys=[ctry#131], functions=[min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "+- Exchange hashpartitioning(ctry#131, 200), true, [id=#645]\n",
      "   +- *(5) HashAggregate(keys=[ctry#131], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), partial_max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "      +- *(5) Project [air_temperature#97, air_temperature_qual#98, CTRY#131]\n",
      "         +- *(5) SortMergeJoin [usaf#87, wban#88], [USAF#128, WBAN#129], Inner\n",
      "            :- *(2) Sort [usaf#87 ASC NULLS FIRST, wban#88 ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(usaf#87, wban#88, 200), true, [id=#627]\n",
      "            :     +- *(1) Project [substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      "            :        +- *(1) Filter (isnotnull(substring(value#82, 5, 6)) AND isnotnull(substring(value#82, 11, 5)))\n",
      "            :           +- FileScan text [value#82] Batched: false, DataFilters: [isnotnull(substring(value#82, 5, 6)), isnotnull(substring(value#82, 11, 5))], Format: Text, Location: InMemoryFileIndex[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "            +- *(4) Sort [USAF#128 ASC NULLS FIRST, WBAN#129 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(USAF#128, WBAN#129, 200), true, [id=#636]\n",
      "                  +- *(3) Project [USAF#128, WBAN#129, CTRY#131]\n",
      "                     +- *(3) Filter (isnotnull(USAF#128) AND isnotnull(WBAN#129))\n",
      "                        +- FileScan csv [USAF#128,WBAN#129,CTRY#131] Batched: false, DataFilters: [isnotnull(USAF#128), isnotnull(WBAN#129)], Format: CSV, Location: InMemoryFileIndex[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,CTRY:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined = weather.join(stations, [\"usaf\",\"wban\"])\n",
    "result = joined.groupBy(stations[\"ctry\"]).agg(\n",
    "        f.min(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('min_temp'),\n",
    "        f.max(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Aggregation after Broadcast-Join \n",
    "\n",
    "If we use a broadcast join instead of a sort merge join, the we will have a shuffle operation for the aggregation again (since the broadcast join just avoids the shuffle). Let's verify that theory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) HashAggregate(keys=[usaf#87, wban#88], functions=[min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "+- Exchange hashpartitioning(usaf#87, wban#88, 200), true, [id=#578]\n",
      "   +- *(2) HashAggregate(keys=[usaf#87, wban#88], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), partial_max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "      +- *(2) Project [usaf#87, wban#88, air_temperature#97, air_temperature_qual#98]\n",
      "         +- *(2) BroadcastHashJoin [usaf#87, wban#88], [USAF#128, WBAN#129], Inner, BuildRight\n",
      "            :- *(2) Project [substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      "            :  +- *(2) Filter (isnotnull(substring(value#82, 5, 6)) AND isnotnull(substring(value#82, 11, 5)))\n",
      "            :     +- FileScan text [value#82] Batched: false, DataFilters: [isnotnull(substring(value#82, 5, 6)), isnotnull(substring(value#82, 11, 5))], Format: Text, Location: InMemoryFileIndex[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "            +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true], input[1, string, true])), [id=#572]\n",
      "               +- *(1) Project [USAF#128, WBAN#129]\n",
      "                  +- *(1) Filter (isnotnull(USAF#128) AND isnotnull(WBAN#129))\n",
      "                     +- FileScan csv [USAF#128,WBAN#129] Batched: false, DataFilters: [isnotnull(USAF#128), isnotnull(WBAN#129)], Format: CSV, Location: InMemoryFileIndex[file:/dimajix/data/weather-noaa-sample/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined = weather.join(f.broadcast(stations), [\"usaf\",\"wban\"])\n",
    "result = joined.groupBy(weather[\"usaf\"], weather[\"wban\"]).agg(\n",
    "        f.min(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('min_temp'),\n",
    "        f.max(f.when(joined.air_temperature_qual == f.lit(1), joined.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Coalesce\n",
    "\n",
    "There is another use case for changing the number of partitions: Writing results to HDFS/S3/whatever. Per design Spark writes each partition into a separate file, and there is no way around that. But when partitions do not contain many records, this may not only be ugly, but also unperformant and might cause additional trouble. Specifically currently HDFS is not designed to handle many small files, but prefers fewer large files instead.\n",
    "\n",
    "Therefore it is often desireable to reduce the number of partitions of a DataFrame just before writing the result to disk. You could perform this task by a `repartition` operation, but this is an expensive operation requiring an additional shuffle operation. Therefore Spark provides an additional method called `coalesce` which can be used to reduce the number of partitions without incurring an additional shuffle. Spark simply logically concatenates multiple partitions into new partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Number of Partitions\n",
    "\n",
    "For this example, we will use the `weather_rep` DataFrame, which contains exactly 200 partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_rep = weather.repartition(200, weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_rep.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Merge Partitions using coalesce\n",
    "\n",
    "In order to reduce the number of partitions, we simply use the `coalesce` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Coalesce 16\n",
      "+- InMemoryTableScan [year#84, usaf#87, wban#88, date#89, time#90, report_type#91, wind_direction#92, wind_direction_qual#93, wind_observation#94, wind_speed#95, wind_speed_qual#96, air_temperature#97, air_temperature_qual#98]\n",
      "      +- InMemoryRelation [year#84, usaf#87, wban#88, date#89, time#90, report_type#91, wind_direction#92, wind_direction_qual#93, wind_observation#94, wind_speed#95, wind_speed_qual#96, air_temperature#97, air_temperature_qual#98], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "            +- Exchange hashpartitioning(usaf#87, wban#88, 200), false, [id=#237]\n",
      "               +- *(1) Project [2003 AS year#84, substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, substring(value#82, 16, 8) AS date#89, substring(value#82, 24, 4) AS time#90, substring(value#82, 42, 5) AS report_type#91, substring(value#82, 61, 3) AS wind_direction#92, substring(value#82, 64, 1) AS wind_direction_qual#93, substring(value#82, 65, 1) AS wind_observation#94, (cast(cast(substring(value#82, 66, 4) as float) as double) / 10.0) AS wind_speed#95, substring(value#82, 70, 1) AS wind_speed_qual#96, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      "                  +- FileScan text [value#82] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex[file:/dimajix/data/weather-noaa-sample/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_small = weather_rep.coalesce(16)\n",
    "weather_small.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_small.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect WebUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1798753"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_rep.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Saving files\n",
    "\n",
    "We already discussed that Spark writes a separate file per partition. So let's see the result when we write the `weather_rep` DataFrame containing 200 partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write 200 Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_rep.write.mode(\"overwrite\").parquet(\"/tmp/weather_rep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the Result\n",
    "Using a simple HDFS CLI util, we can inspect the result on HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 91 items\n",
      "-rw-r--r--   1 hadoop hadoop          0 2018-10-07 07:17 /tmp/weather_rep/_SUCCESS\n",
      "-rw-r--r--   1 hadoop hadoop       1337 2018-10-07 07:16 /tmp/weather_rep/part-00000-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      24241 2018-10-07 07:16 /tmp/weather_rep/part-00003-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      63340 2018-10-07 07:17 /tmp/weather_rep/part-00005-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      32695 2018-10-07 07:17 /tmp/weather_rep/part-00006-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     126661 2018-10-07 07:17 /tmp/weather_rep/part-00011-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      89610 2018-10-07 07:17 /tmp/weather_rep/part-00013-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      73063 2018-10-07 07:17 /tmp/weather_rep/part-00014-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      70655 2018-10-07 07:17 /tmp/weather_rep/part-00016-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      61512 2018-10-07 07:17 /tmp/weather_rep/part-00017-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     181909 2018-10-07 07:17 /tmp/weather_rep/part-00025-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      67545 2018-10-07 07:17 /tmp/weather_rep/part-00026-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      87515 2018-10-07 07:17 /tmp/weather_rep/part-00028-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      76725 2018-10-07 07:17 /tmp/weather_rep/part-00031-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      16246 2018-10-07 07:17 /tmp/weather_rep/part-00032-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      68058 2018-10-07 07:17 /tmp/weather_rep/part-00033-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      84538 2018-10-07 07:17 /tmp/weather_rep/part-00034-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      73316 2018-10-07 07:17 /tmp/weather_rep/part-00035-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     123655 2018-10-07 07:17 /tmp/weather_rep/part-00036-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      37920 2018-10-07 07:17 /tmp/weather_rep/part-00038-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      57775 2018-10-07 07:17 /tmp/weather_rep/part-00039-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      67351 2018-10-07 07:17 /tmp/weather_rep/part-00040-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      55996 2018-10-07 07:17 /tmp/weather_rep/part-00041-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      59784 2018-10-07 07:17 /tmp/weather_rep/part-00043-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      80773 2018-10-07 07:17 /tmp/weather_rep/part-00046-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      84986 2018-10-07 07:17 /tmp/weather_rep/part-00048-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     133418 2018-10-07 07:17 /tmp/weather_rep/part-00049-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      75265 2018-10-07 07:17 /tmp/weather_rep/part-00050-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      60268 2018-10-07 07:17 /tmp/weather_rep/part-00053-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      76993 2018-10-07 07:17 /tmp/weather_rep/part-00058-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     199806 2018-10-07 07:17 /tmp/weather_rep/part-00059-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      40241 2018-10-07 07:17 /tmp/weather_rep/part-00066-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      97540 2018-10-07 07:17 /tmp/weather_rep/part-00068-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      29008 2018-10-07 07:17 /tmp/weather_rep/part-00071-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      73180 2018-10-07 07:17 /tmp/weather_rep/part-00078-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop       3393 2018-10-07 07:17 /tmp/weather_rep/part-00081-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      62817 2018-10-07 07:17 /tmp/weather_rep/part-00084-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop       3359 2018-10-07 07:17 /tmp/weather_rep/part-00088-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      34895 2018-10-07 07:17 /tmp/weather_rep/part-00092-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      21333 2018-10-07 07:17 /tmp/weather_rep/part-00096-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      76141 2018-10-07 07:17 /tmp/weather_rep/part-00098-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      48870 2018-10-07 07:17 /tmp/weather_rep/part-00099-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      31191 2018-10-07 07:17 /tmp/weather_rep/part-00100-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      61306 2018-10-07 07:17 /tmp/weather_rep/part-00102-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     145618 2018-10-07 07:17 /tmp/weather_rep/part-00104-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      60617 2018-10-07 07:17 /tmp/weather_rep/part-00108-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      78265 2018-10-07 07:17 /tmp/weather_rep/part-00111-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      31085 2018-10-07 07:17 /tmp/weather_rep/part-00112-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      90587 2018-10-07 07:17 /tmp/weather_rep/part-00113-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      59706 2018-10-07 07:17 /tmp/weather_rep/part-00114-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      22701 2018-10-07 07:17 /tmp/weather_rep/part-00118-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      66911 2018-10-07 07:17 /tmp/weather_rep/part-00119-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     161560 2018-10-07 07:17 /tmp/weather_rep/part-00122-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      79337 2018-10-07 07:17 /tmp/weather_rep/part-00124-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      73118 2018-10-07 07:17 /tmp/weather_rep/part-00127-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     123673 2018-10-07 07:17 /tmp/weather_rep/part-00129-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      75963 2018-10-07 07:17 /tmp/weather_rep/part-00130-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      86810 2018-10-07 07:17 /tmp/weather_rep/part-00132-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      57741 2018-10-07 07:17 /tmp/weather_rep/part-00133-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop       3160 2018-10-07 07:17 /tmp/weather_rep/part-00134-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     124276 2018-10-07 07:17 /tmp/weather_rep/part-00137-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      68907 2018-10-07 07:17 /tmp/weather_rep/part-00141-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      37198 2018-10-07 07:17 /tmp/weather_rep/part-00143-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      80649 2018-10-07 07:17 /tmp/weather_rep/part-00145-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      12477 2018-10-07 07:17 /tmp/weather_rep/part-00150-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      52018 2018-10-07 07:17 /tmp/weather_rep/part-00151-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      79631 2018-10-07 07:17 /tmp/weather_rep/part-00152-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      90223 2018-10-07 07:17 /tmp/weather_rep/part-00154-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     135687 2018-10-07 07:17 /tmp/weather_rep/part-00156-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     142939 2018-10-07 07:17 /tmp/weather_rep/part-00157-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      63448 2018-10-07 07:17 /tmp/weather_rep/part-00158-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     144695 2018-10-07 07:17 /tmp/weather_rep/part-00163-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      56188 2018-10-07 07:17 /tmp/weather_rep/part-00164-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     163375 2018-10-07 07:17 /tmp/weather_rep/part-00165-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      61759 2018-10-07 07:17 /tmp/weather_rep/part-00166-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      18942 2018-10-07 07:17 /tmp/weather_rep/part-00171-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop       8239 2018-10-07 07:17 /tmp/weather_rep/part-00172-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      78075 2018-10-07 07:17 /tmp/weather_rep/part-00173-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      69343 2018-10-07 07:17 /tmp/weather_rep/part-00174-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      86969 2018-10-07 07:17 /tmp/weather_rep/part-00178-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      30513 2018-10-07 07:17 /tmp/weather_rep/part-00179-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      78521 2018-10-07 07:17 /tmp/weather_rep/part-00181-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      69376 2018-10-07 07:17 /tmp/weather_rep/part-00182-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      15683 2018-10-07 07:17 /tmp/weather_rep/part-00186-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      70658 2018-10-07 07:17 /tmp/weather_rep/part-00187-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      33030 2018-10-07 07:17 /tmp/weather_rep/part-00189-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      56766 2018-10-07 07:17 /tmp/weather_rep/part-00191-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      78657 2018-10-07 07:17 /tmp/weather_rep/part-00192-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      50076 2018-10-07 07:17 /tmp/weather_rep/part-00195-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      78921 2018-10-07 07:17 /tmp/weather_rep/part-00198-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      60186 2018-10-07 07:17 /tmp/weather_rep/part-00199-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /tmp/weather_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write 16 Partitions\n",
    "\n",
    "Now let's write the `coalesce`d DataFrame and inspect the result on HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_small.write.mode(\"overwrite\").parquet(\"/tmp/weather_small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 items\n",
      "-rw-r--r--   1 hadoop hadoop          0 2018-10-07 07:17 /tmp/weather_small/_SUCCESS\n",
      "-rw-r--r--   1 hadoop hadoop     290888 2018-10-07 07:17 /tmp/weather_small/part-00000-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     539188 2018-10-07 07:17 /tmp/weather_small/part-00001-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     490533 2018-10-07 07:17 /tmp/weather_small/part-00002-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     338415 2018-10-07 07:17 /tmp/weather_small/part-00003-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     460959 2018-10-07 07:17 /tmp/weather_small/part-00004-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     358779 2018-10-07 07:17 /tmp/weather_small/part-00005-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     394439 2018-10-07 07:17 /tmp/weather_small/part-00006-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     295745 2018-10-07 07:17 /tmp/weather_small/part-00007-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     274293 2018-10-07 07:17 /tmp/weather_small/part-00008-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     352943 2018-10-07 07:17 /tmp/weather_small/part-00009-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     405437 2018-10-07 07:17 /tmp/weather_small/part-00010-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     337051 2018-10-07 07:17 /tmp/weather_small/part-00011-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     521293 2018-10-07 07:17 /tmp/weather_small/part-00012-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     330085 2018-10-07 07:17 /tmp/weather_small/part-00013-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     365699 2018-10-07 07:17 /tmp/weather_small/part-00014-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     398450 2018-10-07 07:17 /tmp/weather_small/part-00015-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /tmp/weather_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
