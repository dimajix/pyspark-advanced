{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"4G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watson Sales Product Sample Data\n",
    "\n",
    "In this example, we want to have a look at the pivoting capabilities of Spark. Since pivoting is commonly used with sales data containing information for different product categories or countries, we will use a data set called \"Watson Sales Product Sample Data\" which was downloaded from https://www.ibm.com/communities/analytics/watson-analytics-blog/sales-products-sample-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load and inspect data\n",
    "\n",
    "First we load the data, which is provided as a single CSV file, which again is well supported by Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = \"s3://dimajix-training/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read\\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(basedir + \"/watson-sales-products/WA_Sales_Products_2012-14.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect schema\n",
    "\n",
    "Since we used the existing header information and also let Spark infer appropriate data types, let us inspect the schema now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect pivoting candidates\n",
    "\n",
    "Now let us find some good candidates for a pivoting column. A pivoting column shouldn't have too many distinct entries, otherwise the result probably doesn't make too much sense and doesn't help the business expert in interpretation.\n",
    "\n",
    "We can either use\n",
    "```\n",
    "data.select(\"Retailer type\").distinct().count()\n",
    "```\n",
    "which will give us the number of distinct values for a single column, or we can use the Spark aggregate function `countDistinct` which allows us to retrieve information for multiple columns within a single `select`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data.select(\n",
    "    sf.countDistinct(\"Retailer country\"),\n",
    "    sf.countDistinct(\"Retailer type\"),\n",
    "    sf.countDistinct(\"Product line\"),\n",
    "    sf.countDistinct(\"Product type\"),\n",
    "    sf.countDistinct(\"Quarter\")\n",
    ")\n",
    "\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Pivoting by Product Line\n",
    "\n",
    "The first example pivots by the product line, since there are only five different distinct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_per_product_line = # YOUR CODE HERE\n",
    "revenue_per_product_line.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Exercise\n",
    "\n",
    "Craete an aggragated table with\n",
    "* Country and Product Line in Rows\n",
    "* The quantity for each quarter in different columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Unpivoting again\n",
    "\n",
    "Sometimes you just need the opposite operation: You have a data set in pivoted format and want to unpivot it. There is no simple built in function provided by Spark, but you can construct the unpivoted table as follows\n",
    "* For every pivoted column:\n",
    "  * Project data frame onto non-pivot columns\n",
    "  * Add a new column with an appropriate name containing the name of the pivot column as its value\n",
    "  * Add a new column with an appropriate name containing the values of the pivot column\n",
    "* Union together all these data frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Specific Example\n",
    "\n",
    "Now let us perform these steps for the pivoted table above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_camping = revenue_per_product_line.select(\n",
    "    # YOUR CODE HERE\n",
    ")\n",
    "\n",
    "revenue_golf = revenue_per_product_line.select(\n",
    "    sf.col(\"Quarter\"),\n",
    "    sf.col(\"Retailer Country\"),\n",
    "    sf.lit(\"Golf Equipment\").alias(\"Product line\"),\n",
    "    sf.col(\"Golf Equipment\").alias(\"Revenue\")\n",
    ")\n",
    "\n",
    "revenue_mountaineering = revenue_per_product_line.select(\n",
    "    sf.col(\"Quarter\"),\n",
    "    sf.col(\"Retailer Country\"),\n",
    "    sf.lit(\"Mountaineering Equipment\").alias(\"Product line\"),\n",
    "    sf.col(\"Mountaineering Equipment\").alias(\"Revenue\")\n",
    ")\n",
    "\n",
    "revenue_outdoor = revenue_per_product_line.select(\n",
    "    sf.col(\"Quarter\"),\n",
    "    sf.col(\"Retailer Country\"),\n",
    "    sf.lit(\"Outdoor Protection\").alias(\"Product line\"),\n",
    "    sf.col(\"Outdoor Protection\").alias(\"Revenue\")\n",
    ")\n",
    "\n",
    "revenue_personal = revenue_per_product_line.select(\n",
    "    sf.col(\"Quarter\"),\n",
    "    sf.col(\"Retailer Country\"),\n",
    "    sf.lit(\"Personal Accessories\").alias(\"Product line\"),\n",
    "    sf.col(\"Personal Accessories\").alias(\"Revenue\")\n",
    ")\n",
    "\n",
    "result = # YOUR CODE HERE\n",
    "\n",
    "result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Generic Approach\n",
    "\n",
    "Of course manually unpivoting is somewhat tedious, but we already see a pattern:\n",
    "* Select all non-pivot columns\n",
    "* Create a new column containing the pivot column name\n",
    "* Create a new column containing the pivot column values\n",
    "* Union together everything\n",
    "\n",
    "This can be done by writing some small Python functions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "# Unpivot a single column, thereby creating one data frame\n",
    "def unpivot_column(df, other, pivot_column, pivot_value, result_column):\n",
    "    columns = [df[c] for c in other] + \\\n",
    "        [sf.lit(pivot_value).alias(pivot_column)] + \\\n",
    "        [df[pivot_value].alias(result_column)]\n",
    "    return df.select(*columns)\n",
    "\n",
    "# Unpivot multiple columns by using the above method\n",
    "def unpivot(df, pivot_column, pivot_values, result_column):\n",
    "    \"\"\"\n",
    "    df - input data frame\n",
    "    pivot_column - the name of the new column containg each pivot column name\n",
    "    pivot_values - the list of pivoted column names\n",
    "    result_column - the name of the column containing the values of the pivot columns\n",
    "    \"\"\"\n",
    "    common_columns = [f.name for f in df.schema.fields if not f.name in pivot_values]\n",
    "    unpivot_dfs = [unpivot_column(df, common_columns, pivot_column, v, result_column) for v in pivot_values]\n",
    "    return functools.reduce(lambda x,y: x.union(y), unpivot_dfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_lines = # YOUR CODE HERE\n",
    "result_per_product_line = # YOUR CODE HERE\n",
    "\n",
    "result_per_product_line.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Exercise\n",
    "\n",
    "Now unpivot the result of exercise 2.1. You can do that either manually or try using the generic function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
