{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivoting\n",
    "\n",
    "Pivoting is a special operation, which adds new columns containing aggregated information from previously separate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"4G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and inspect data\n",
    "\n",
    "## Watson Sales Product Sample Data\n",
    "\n",
    "In this example, we want to have a look at the pivoting capabilities of Spark. Since pivoting is commonly used with sales data containing information for different product categories or countries, we will use a data set called \"Watson Sales Product Sample Data\" which was downloaded from https://www.ibm.com/communities/analytics/watson-analytics-blog/sales-products-sample-data/\n",
    "\n",
    "First we load the data, which is provided as a single CSV file, which again is well supported by Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = \"s3://dimajix-training/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read\\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(basedir + \"/watson-sales-products/WA_Sales_Products_2012-14.csv\")\n",
    "\n",
    "data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect schema\n",
    "\n",
    "Since we used the existing header information and also let Spark infer appropriate data types, let us inspect the schema now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect pivoting candidates\n",
    "\n",
    "Now let us find some good candidates for a pivoting column. A pivoting column shouldn't have too many distinct entries, otherwise the result probably doesn't make too much sense and doesn't help the business expert in interpretation.\n",
    "\n",
    "We can either use\n",
    "```\n",
    "data.select(\"Retailer type\").distinct().count()\n",
    "```\n",
    "which will give us the number of distinct values for a single column, or we can use the Spark aggregate function `countDistinct` which allows us to retrieve information for multiple columns within a single `select`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data.select(\n",
    "    sf.countDistinct(\"Retailer country\"),\n",
    "    sf.countDistinct(\"Retailer type\"),\n",
    "    sf.countDistinct(\"Product line\"),\n",
    "    sf.countDistinct(\"Product type\"),\n",
    "    sf.countDistinct(\"Quarter\")\n",
    ")\n",
    "\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pivoting\n",
    "\n",
    "The first example pivots by the product line, since there are only five different distinct values. The operation will create new columns for every value in the column `Product Line`. All rows within each grouping will be aggregated according to the expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_per_product_line = # YOUR CODE HERE\n",
    "revenue_per_product_line.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Exercise\n",
    "\n",
    "Create an aggragated table with\n",
    "* Country and Product Line in Rows\n",
    "* The quantity for each quarter in different columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Unpivoting (Spark 3.4+)\n",
    "\n",
    "The inverse operation of pivoring is either called *unpivoting* (suprise) or *melt*. This operation is immediately available in Spark since version 3.4. Since there are still some people forced to use older versions of Spark, we will discuss a manual workaround in section four of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create well defined pivoted DataFrame again\n",
    "pivoted = data.groupBy(\"Quarter\", \"Retailer Country\").pivot(\"Product line\").agg(sf.sum(\"Revenue\"))\n",
    "pivoted.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame now has the method `unpivot` which has the following options\n",
    "* `ids` - list of columns which should serve as IDs or which should otherwise be preserved\n",
    "* `values` - list of values which should be unpivoted. Can be `None`\n",
    "* `variableColumnName` - The name of the new column which should contain the names of the `values` columns\n",
    "* `valueColumnName` - The name of the new column which should contain the value of the `values` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = # YOUR CODE HERE\n",
    "result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Omitting Values\n",
    "\n",
    "The `values` parameter can be omitted from the `unpivot` method. In this case, Spark will pick all columns which are not part of the `ids` column list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = # YOUR CODE HERE\n",
    "result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Exercise\n",
    "\n",
    "Perform a `unpivot` operation for the DataFrame created in exercise 2.1. The following cell will contain an appropriate definition of the DataFrame, so you don't need to look it up above :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create well defined pivoted DataFrame again\n",
    "pivoted = data.groupBy(\"Retailer Country\", \"Product\").pivot(\"Quarter\").agg(sf.sum(\"Quantity\"))\n",
    "pivoted.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now unpivot the DataFrame `pivoted`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Execution Plan\n",
    "\n",
    "Out of curiosity, let's inspect the execution plan. Note that unpivoting will be done using a special `Expan` step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Manual Unpivoting (for Spark < 3.4)\n",
    "\n",
    "Sometimes you just need the opposite operation: You have a data set in pivoted format and want to unpivot it. As stated above, older Version of Spark offer no simple built in function. But you can construct the unpivoted table as follows\n",
    "* For every pivoted column:\n",
    "  * Project data frame onto non-pivot columns\n",
    "  * Add a new column with an appropriate name containing the name of the pivot column as its value\n",
    "  * Add a new column with an appropriate name containing the values of the pivot column\n",
    "* Union together all these data frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Specific Example\n",
    "\n",
    "Now let us perform these steps for the pivoted table above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_camping = revenue_per_product_line.select(\n",
    "    # YOUR CODE HERE\n",
    ")\n",
    "\n",
    "revenue_golf = revenue_per_product_line.select(\n",
    "    sf.col(\"Quarter\"),\n",
    "    sf.col(\"Retailer Country\"),\n",
    "    sf.lit(\"Golf Equipment\").alias(\"Product line\"),\n",
    "    sf.col(\"Golf Equipment\").alias(\"Revenue\")\n",
    ")\n",
    "\n",
    "revenue_mountaineering = revenue_per_product_line.select(\n",
    "    sf.col(\"Quarter\"),\n",
    "    sf.col(\"Retailer Country\"),\n",
    "    sf.lit(\"Mountaineering Equipment\").alias(\"Product line\"),\n",
    "    sf.col(\"Mountaineering Equipment\").alias(\"Revenue\")\n",
    ")\n",
    "\n",
    "revenue_outdoor = revenue_per_product_line.select(\n",
    "    sf.col(\"Quarter\"),\n",
    "    sf.col(\"Retailer Country\"),\n",
    "    sf.lit(\"Outdoor Protection\").alias(\"Product line\"),\n",
    "    sf.col(\"Outdoor Protection\").alias(\"Revenue\")\n",
    ")\n",
    "\n",
    "revenue_personal = revenue_per_product_line.select(\n",
    "    sf.col(\"Quarter\"),\n",
    "    sf.col(\"Retailer Country\"),\n",
    "    sf.lit(\"Personal Accessories\").alias(\"Product line\"),\n",
    "    sf.col(\"Personal Accessories\").alias(\"Revenue\")\n",
    ")\n",
    "\n",
    "result = # YOUR CODE HERE\n",
    "\n",
    "result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Execution Plan\n",
    "\n",
    "Again let's inspect the execution plan of the manual `unpivot` operation. Note that it is much more expensive, since the `Union` operator will reprocess the same data over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Generic Approach\n",
    "\n",
    "Of course manually unpivoting is somewhat tedious, but we already see a pattern:\n",
    "* Select all non-pivot columns\n",
    "* Create a new column containing the pivot column name\n",
    "* Create a new column containing the pivot column values\n",
    "* Union together everything\n",
    "\n",
    "This can be done by writing some small Python functions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "# Unpivot a single column, thereby creating one data frame\n",
    "def unpivot_column(df, ids, value, variableColumnName, valueColumnName):\n",
    "    columns = [df[c] for c in ids] + \\\n",
    "        [sf.lit(value).alias(variableColumnName)] + \\\n",
    "        [df[value].alias(valueColumnName)]\n",
    "    return df.select(*columns)\n",
    "\n",
    "# Unpivot multiple columns by using the above method\n",
    "def unpivot(df, values, variableColumnName, valueColumnName):\n",
    "    \"\"\"\n",
    "    df - input data frame\n",
    "    pivot_column - the name of the new column containg each pivot column name\n",
    "    values - the list of pivoted column names\n",
    "    valueColumnName - the name of the column containing the values of the pivot columns\n",
    "    \"\"\"\n",
    "    ids = [f for f in df.columns if not f in values]\n",
    "    unpivot_dfs = [unpivot_column(df, ids, v, variableColumnName, valueColumnName) for v in values]\n",
    "    return functools.reduce(lambda x,y: x.union(y), unpivot_dfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_lines = # YOUR CODE HERE\n",
    "result_per_product_line = # YOUR CODE HERE\n",
    "\n",
    "result_per_product_line.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Exercise\n",
    "\n",
    "Now unpivot the result of exercise 2.1. You can do that either manually or try using the generic function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create well defined pivoted DataFrame again\n",
    "pivoted = data.groupBy(\"Retailer Country\", \"Product\").pivot(\"Quarter\").agg(sf.sum(\"Quantity\"))\n",
    "pivoted.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
