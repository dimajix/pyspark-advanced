{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching Data\n",
    "\n",
    "Spark offers the possibility to cache data, which means that it tries to keep (intermediate) results either in memory or on disk. This can be very helpful in iterative algorithms or interactive analysis, where you want to prevent that the same processing steps are performed over and over again.\n",
    "\n",
    "### Approach to Caching\n",
    "Instead of performing timings of individual executions, we use the `explain()` method again to see how output changes with cached intermediate results.\n",
    "\n",
    "### Weather Example\n",
    "We will again use the weather example to understand how caching works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Reuse Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"24G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data\n",
    "\n",
    "First we load the weather data, which consists of the measurement data and some station metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageLocation = \"s3://dimajix-training/data/weather\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Measurements\n",
    "\n",
    "Measurements are stored in multiple directories (one per year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_weather = spark.read.text(storageLocation + \"/2003\").withColumn(\"year\", f.lit(2003))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Measurements\n",
    "\n",
    "Measurements were stored in a proprietary text based format, with some values at fixed positions. We need to extract these values with a simple `SELECT` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = raw_weather.select(\n",
    "    f.col(\"year\"),\n",
    "    f.substring(f.col(\"value\"),5,6).alias(\"usaf\"),\n",
    "    f.substring(f.col(\"value\"),11,5).alias(\"wban\"),\n",
    "    f.substring(f.col(\"value\"),16,8).alias(\"date\"),\n",
    "    f.substring(f.col(\"value\"),24,4).alias(\"time\"),\n",
    "    f.substring(f.col(\"value\"),42,5).alias(\"report_type\"),\n",
    "    f.substring(f.col(\"value\"),61,3).alias(\"wind_direction\"),\n",
    "    f.substring(f.col(\"value\"),64,1).alias(\"wind_direction_qual\"),\n",
    "    f.substring(f.col(\"value\"),65,1).alias(\"wind_observation\"),\n",
    "    (f.substring(f.col(\"value\"),66,4).cast(\"float\") / f.lit(10.0)).alias(\"wind_speed\"),\n",
    "    f.substring(f.col(\"value\"),70,1).alias(\"wind_speed_qual\"),\n",
    "    (f.substring(f.col(\"value\"),88,5).cast(\"float\") / f.lit(10.0)).alias(\"air_temperature\"),\n",
    "    f.substring(f.col(\"value\"),93,1).alias(\"air_temperature_qual\")\n",
    ")\n",
    "    \n",
    "weather.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load Station Metadata\n",
    "\n",
    "We also need to load the weather station meta data containing information about the geo location, country etc of individual weather stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(storageLocation + \"/isd-history\")\n",
    "\n",
    "# Display first 10 records    \n",
    "stations.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Caching Data\n",
    "\n",
    "For analysing the impact of cachign data, we will use a slightly simplified variant of the weather analysis (only temperature will be aggregated). We will change the execution by caching intermediate results and watch how execution plans change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Original Execution Plan\n",
    "\n",
    "First let's have the execution plans of the original query as our reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_weather = weather.join(stations, [\"usaf\", \"wban\"])\n",
    "aggregates = joined_weather.groupBy(joined_weather.CTRY, joined_weather.year).agg(\n",
    "        f.min(f.when(joined_weather.air_temperature_qual == f.lit(1), joined_weather.air_temperature)).alias('min_temp'),\n",
    "        f.max(f.when(joined_weather.air_temperature_qual == f.lit(1), joined_weather.air_temperature)).alias('max_temp')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = joined_weather.join(f.broadcast(aggregates), [\"ctry\", \"year\"])\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Caching Weather\n",
    "\n",
    "First let us simply cache the joined input DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forcing physical caching\n",
    "\n",
    "The `cache()` method again works lazily and only marks the DataFrame to be cached. The physical cache itself will only take place once the elements are evaluated. A common and easy way to enforce this is to call a `count()` on the to-be cached DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you now perform `count` a second time, it should be much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Plan with Cache\n",
    "\n",
    "Now let us have a look at the execution plan with the cache for the `weather` DataFrame enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregates = joined_weather.groupBy(joined_weather.CTRY, joined_weather.year).agg(\n",
    "        f.min(f.when(joined_weather.air_temperature_qual == f.lit(1), joined_weather.air_temperature)).alias('min_temp'),\n",
    "        f.max(f.when(joined_weather.air_temperature_qual == f.lit(1), joined_weather.air_temperature)).alias('max_temp')\n",
    "    )\n",
    "\n",
    "result = joined_weather.join(f.broadcast(aggregates), [\"ctry\", \"year\"])\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "Although the data is already cached, the execution plan still contains all steps. But the caching step won't be executed any more (since data is already cached), it is only mentioned here for completenss of the plan. We will see in the web interface.\n",
    "\n",
    "The cache itself is presented as two steps in the execution plan:\n",
    "* Creating the cache (InMemoryRelation)\n",
    "* Using the cache (InMemoryTableScan)\n",
    "\n",
    "If you look closely at the execution plans and compare these to the original uncached plan, you will notice that certain optimizations are not performed any more:\n",
    "* Cache contains ALL columns of the weather DataFrame, although only a subset is required.\n",
    "* Filter operation of JOIN is performed part of caching.\n",
    "\n",
    "Caching is an optimization barrier. This means that Spark can only optimize plans before building the cache and plans after using the cache. No optimization is possible that spans building and using the cache. The idea simply is that the DataFrame should be cached exactly how it was specified without any column truncating or record filtering in place which appears after the cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Uncaching Data\n",
    "\n",
    "Caches occupy resources (memory and/or disk). Once you do not need the cache any more, you'd probably like to free up the resources again. This is easily possible with the `unpersist()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exeuction plan after unpersist\n",
    "\n",
    "Now we'd expect to have the original execution plan again. But for some reason (bug?) we don't get that any more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = joined_weather.groupBy(joined_weather.CTRY, joined_weather.year).agg(\n",
    "        f.min(f.when(joined_weather.air_temperature_qual == f.lit(1), joined_weather.air_temperature)).alias('min_temp'),\n",
    "        f.max(f.when(joined_weather.air_temperature_qual == f.lit(1), joined_weather.air_temperature)).alias('max_temp')\n",
    "    )\n",
    "\n",
    "result.explain(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "As you see in the execution plan, the cache has been removed now and the plan equals to the original one before we started caching data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Cache Levels\n",
    "\n",
    "Spark supports different levels of cache (memory, disk and a combination). These can be specified explicitly if you use `persist()` instead of `cache()`. Cache actually is a shortcut for `persist(MEMORY_AND_DISK)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "joined_weather.persist(StorageLevel.MEMORY_ONLY)\n",
    "joined_weather.persist(StorageLevel.DISK_ONLY)\n",
    "joined_weather.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "joined_weather.persist(StorageLevel.MEMORY_ONLY_2)\n",
    "joined_weather.persist(StorageLevel.DISK_ONLY_2)\n",
    "joined_weather.persist(StorageLevel.MEMORY_AND_DISK_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache level explanation\n",
    "\n",
    "* `MEMORY_ONLY` - stores all records directly in memory\n",
    "* `DISK_ONLY` - stores all records serialized on disk\n",
    "* `MEMORY_AND_DISK` - stores all records first in memory and spills onto disk when no space is left in memory\n",
    "* `..._2` - stores caches on two nodes instead of one for additional redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Caching within a Single Query\n",
    "\n",
    "Caching only helps in very rare cases within a single query, one case being if a DataFrame is used multiple times (for example in a `UNION` operation). But even then, things don't always work out nicely. Let's start with a small example, where the `weather` DataFrame is used twice with a simple modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any caches\n",
    "weather.unpersist()\n",
    "\n",
    "result = # YOUR CODE HERE\n",
    "\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Cache\n",
    "Now let's use some caching to prevent Spark from reading the input twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.cache()\n",
    "\n",
    "result = weather.union(\n",
    "        weather.withColumn(\"air_temperature\", 2*weather[\"air_temperature\"])\n",
    "    )\n",
    "\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Shuffle Reuse\n",
    "\n",
    "In some constellations Spark automatically detects that it can reuse the output of a shuffle operation. In this case, caching won't help much and even risks hurting the performance. But this Spark logic does not catch many cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any caches\n",
    "weather.unpersist()\n",
    "joined_weather.unpersist()\n",
    "\n",
    "result = # YOUR CODE HERE\n",
    "\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Don'ts\n",
    "\n",
    "Although reading from a cache can be faster than reprocessing data from scratch, especially if that involves reading original data from slow IO devices (S3) or complex operations (joins), some caution should be taken. Caching is not free, not only is it a optimization barrier, it also occupies resources (memory and disk) and definately slows down the first query that has to build the cache.\n",
    "\n",
    "In order to limit the physical resources (RAM and disk), you should reduce the amount to cache to the bare minimum and even exclude simple calculations from the cache. For example if we included conversions to mph and °F in our weather data as precalculated measurements, it would be a wise idea to exclude these simple calculations from the cache, since they would only blow up the overall volume while these conversions are simple and cheap to calculate even after reading from the cache (plus they can be removed by the optimizer when they are not needed in a specific query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any previous caches\n",
    "weather.unpersist()\n",
    "\n",
    "weather_intl = # YOUR CODE HERE\n",
    "\n",
    "# DON'T !\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any previous caches\n",
    "weather_intl.unpersist()\n",
    "\n",
    "# Prefer caching the smaller input data set and perform trivial calculations after caching\n",
    "# YOUR CODE HERE\n",
    "weather_intl = weather.withColumn(\"air_temperature_fahrenheit\", weather[\"air_temperature\"]*9.0/5.0+32) \\\n",
    "        .withColumn(\"wind_speed_mph\", weather[\"wind_speed\"]*2.236936)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
