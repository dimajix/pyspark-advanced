{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of grouped regressions\n",
    "\n",
    "In this section, we want to demanstrate a slightly advanced example for using Pandas grouped transformation for performing many ordinary least square model fits in parallel. We reuse the weather data and try to predict the temperature of all stations with a very simple model per station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pyspark.sql\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"24G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load Data\n",
    "First we load data of a single year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageLocation = \"s3://dimajix-training/data/weather\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "rawWeatherData = spark.read.text(storageLocation + \"/2003\")\n",
    "weather_all = rawWeatherData.select(\n",
    "    substring(col(\"value\"),5,6).alias(\"usaf\"),\n",
    "    substring(col(\"value\"),11,5).alias(\"wban\"),\n",
    "    to_timestamp(substring(col(\"value\"),16,12),\"yyyyMMddHHmm\").alias(\"timestamp\"),\n",
    "    to_timestamp(substring(col(\"value\"),16,12),\"yyyyMMddHHmm\").cast(\"long\").alias(\"ts\"),\n",
    "    substring(col(\"value\"),42,5).alias(\"report_type\"),\n",
    "    substring(col(\"value\"),61,3).alias(\"wind_direction\"),\n",
    "    substring(col(\"value\"),64,1).alias(\"wind_direction_qual\"),\n",
    "    substring(col(\"value\"),65,1).alias(\"wind_observation\"),\n",
    "    (substring(col(\"value\"),66,4).cast(\"float\") / lit(10.0)).alias(\"wind_speed\"),\n",
    "    substring(col(\"value\"),70,1).alias(\"wind_speed_qual\"),\n",
    "    (substring(col(\"value\"),88,5).cast(\"float\") / lit(10.0)).alias(\"air_temperature\"),\n",
    "    substring(col(\"value\"),93,1).alias(\"air_temperature_qual\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Analysis of one station\n",
    "\n",
    "First we only analyse a single station, just to check our approach and the expressiveness of our model. It won't be a very good fit, but it will be good enough for our needs to demonstrate the concept.\n",
    "\n",
    "So first we pick a single station, and we also only keep those records with a valid temeprature measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_single = weather_all.where(\"usaf='954920' and wban='99999'\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = # YOUR CODE HERE\n",
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Create Feature Space\n",
    "\n",
    "Our model will simply predict the temperature depending on the time and day of year. We use sin and cos of with a day-wide period and a year-wide period as features for fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "seconds_per_day = 24*60*60\n",
    "seconds_per_year = 365*seconds_per_day\n",
    "\n",
    "# Add sin and cos as features for fitting\n",
    "pdf['daily_sin'] = np.sin(pdf['ts']/seconds_per_day*2.0*math.pi)\n",
    "pdf['daily_cos'] = np.cos(pdf['ts']/seconds_per_day*2.0*math.pi)\n",
    "pdf['yearly_sin'] = np.sin(pdf['ts']/seconds_per_year*2.0*math.pi)\n",
    "pdf['yearly_cos'] = np.cos(pdf['ts']/seconds_per_year*2.0*math.pi)\n",
    "\n",
    "# Make a plot, just to check how it looks like\n",
    "pdf[0:200].plot(x='timestamp', y=['daily_sin','daily_cos','air_temperature'], figsize=[16,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Fit model\n",
    "\n",
    "Now that we have the temperature and some features, we fit a simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# define target variable y\n",
    "y = pdf['air_temperature']\n",
    "# define feature variables X\n",
    "X = pdf[['ts', 'daily_sin', 'daily_cos', 'yearly_sin', 'yearly_cos']]\n",
    "X = sm.add_constant(X)\n",
    "# fit model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# perform prediction\n",
    "pdf['pred'] = model.predict(X)\n",
    "\n",
    "# Make a plot of real temperature vs predicted temperature\n",
    "pdf[0:200].plot(x='timestamp', y=['pred','air_temperature'], figsize=[16,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Inspect Model\n",
    "\n",
    "Now let us inspect the model, in order to find a way to store it in a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let us create a Pandas DataFrame from the model parameters. This code snippet will be needed later when we want to parallelize the fitting for different weather stations using Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_columns = X.columns\n",
    "pd.DataFrame([[model.params[i] for i in  x_columns]], columns=x_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Perform OLS for all stations\n",
    "\n",
    "Now we want to create a model for all stations. First we filter the data again, such that we only have valid temperature measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_weather = weather_all.filter(weather_all.air_temperature_qual == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Feature extraction\n",
    "\n",
    "Now we generate the same features, but this time we use Spark instead of Pandas operations. This simplifies later model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "seconds_per_day = 24*60*60\n",
    "seconds_per_year = 365*seconds_per_day\n",
    "\n",
    "features = valid_weather.select(\n",
    "    valid_weather.usaf,\n",
    "    valid_weather.wban,\n",
    "    valid_weather.air_temperature,\n",
    "    valid_weather.ts,\n",
    "    lit(1.0).alias('const'),\n",
    "    sin(valid_weather.ts * 2.0 * math.pi / seconds_per_day).alias('daily_sin'),\n",
    "    cos(valid_weather.ts * 2.0 * math.pi / seconds_per_day).alias('daily_cos'),\n",
    "    sin(valid_weather.ts * 2.0 * math.pi / seconds_per_year).alias('yearly_sin'),\n",
    "    cos(valid_weather.ts * 2.0 * math.pi / seconds_per_year).alias('yearly_cos')\n",
    ")\n",
    "\n",
    "features.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Fit Models\n",
    "\n",
    "Now we use a Spark Pandas grouped UDF in order to fit models for all weather stations in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_columns = ['usaf', 'wban']\n",
    "y_column = 'air_temperature'\n",
    "x_columns = ['ts', 'const', 'daily_sin', 'daily_cos', 'yearly_sin', 'yearly_cos']\n",
    "schema = features.select(*group_columns, *x_columns).schema\n",
    "\n",
    "\n",
    "def ols(pdf):\n",
    "    # Extract grouping information from appropriate columns\n",
    "    group = # YOUR CODE HERE\n",
    "    \n",
    "    # Extract target variable\n",
    "    y = # YOUR CODE HERE\n",
    "    \n",
    "    # Extract predictor variables\n",
    "    X = # YOUR CODE HERE\n",
    "    \n",
    "    # Create model using Python statsmodel package to fit y to input variables x\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    # Create a Pandas data frame with one row containing the grouping columns and all model parameters\n",
    "    return pd.DataFrame([group + [model.params[i] for i in x_columns]], columns=group_columns + x_columns)\n",
    "\n",
    "# Now fit model for all weather stations in parallel using Spark\n",
    "models = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Inspect and compare results\n",
    "\n",
    "Now let's pick the same station again, and compare the model to the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.where(\"usaf='954920' and wban='99999'\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
