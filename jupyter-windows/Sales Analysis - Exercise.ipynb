{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales Data Example\n",
    "\n",
    "Window functions are commonly used together with sales data. In this notebook we will be using a data set called \"Watson Sales Product Sample Data\" which was downloaded from https://www.ibm.com/communities/analytics/watson-analytics-blog/sales-products-sample-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"4G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Watson Sales Product Sample Data\n",
    "\n",
    "First we load the data, which is provided as a single CSV file, which again is well supported by Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = \"s3://dimajix-training/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(basedir + \"/watson-sales-products/WA_Sales_Products_2012-14.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect schema\n",
    "\n",
    "Since we used the existing header information and also let Spark infer appropriate data types, let us inspect the schema now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preaggregate data\n",
    "\n",
    "Since we are not interested in all details, we preaggregate the data into the following columns:\n",
    "* Retailer country\n",
    "* Retailer type\n",
    "* Product line\n",
    "* Quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data = data.groupBy(\n",
    "    \"Retailer country\",\n",
    "    \"Retailer type\",\n",
    "    \"Product line\",\n",
    "    \"Quarter\"\n",
    ").agg(\n",
    "    f.sum(\"Revenue\").alias(\"Revenue\"),\n",
    "    f.sum(\"Quantity\").alias(\"Quantity\")\n",
    ")\n",
    "\n",
    "# Inspect the schema (you could also peek at the first 10 records instead)\n",
    "aggregated_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Find Difference to Average\n",
    "\n",
    "In the first example, we try to find the difference of the revenue of each quarter to the average revenue for each retailer country and retailer type over all quarters. This can be done either using a grouped aggregated followed by a join or by using window functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Self Join\n",
    "\n",
    "Just for the sake of completeness, let us start with the aggragetion and join approach. It will turn out later that this is much more complicated than using a window function, but nevertheless we implement this approach such that we can compare both approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Extarct year and quarter\n",
    "\n",
    "Technically the first step is not required, but in order to provide some meaningful sorting, we extract the quarter (Q1, Q2, Q3 and Q4) and the year from the incoming column `Quarter`. Otherwise sorting wouldn't work, since that column is formatted as `'Q'q YYYY` which doesn't provide a chronologically ordering if sorted alphabetically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_data = aggregated_data.select(\n",
    "    f.col(\"*\"),\n",
    "    f.substring(aggregated_data[\"Quarter\"],1,2).alias(\"q\"),\n",
    "    f.substring(aggregated_data[\"Quarter\"],3,8).alias(\"y\")\n",
    ")\n",
    "\n",
    "# Inspect the schema (you could also peek at the first 10 records instead)\n",
    "extended_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Calculate average revenue\n",
    "\n",
    "Now we calculate the average revenue per `Retailer Country`, `Retailer type` and `Product line`. Do NOT use a window function, perform a simple grouped aggregation instead (`groupBy(...).agg(..)`) is your friend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_data = # YOUR CODE HERE\n",
    "\n",
    "avg_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Join and calculate\n",
    "\n",
    "Now we join the average revenue with the original data set, such that we can calculate the difference of the revenue and the average revenue. To do so, we need to join the two DataFrames `extended_data` and `avg_data` on the relevant columns `Retailer country`, `Retailer type` and `Product line`. This then allows us to compare the revenue within each quarter with the average revenue (both sides partitioned by these three dimensions used for joining)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join extended_data with avg_data on the columns \"Retailer country\", \"Retailer type\", \"Product line\"\n",
    "joined_data = # YOUR CODE HERE\n",
    "\n",
    "# Select all columns from \"extended_data\" and add a new column to \"joined_data\", which contains the difference \n",
    "# beteween the current revenue from \"extended_data\" and the average revenue from \"avg_data\"\n",
    "result = # YOUR CODE HERE\n",
    "\n",
    "# Finally sort the result by \"Retailer Country\", \"Retailer Type\", \"Product line\", \"y\", \"q\", then drop the two helper columns \"q\" and \"y\"\n",
    "sorted_result = result \\\n",
    "    .orderBy(\"Retailer Country\", \"Retailer Type\", \"Product line\", \"y\", \"q\") \\\n",
    "    .drop(\"q\", \"y\")\n",
    "\n",
    "# Inspect the schema (you could also peek at the first 10 records instead)\n",
    "sorted_result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Better use Windowing\n",
    "\n",
    "Now let us perform the very same analysis, but using windowed aggregation instead of aggregation and joining. A *window* aggregates groups of records, but this grouping and aggregation will be performed (conceptionally) individually for every input record and the result will be attached to each input record. Therefore a windowed aggregation works like a normal aggregation followed by a join.\n",
    "\n",
    "In Spark we always need to specify how this aggregation window is to be constructed. It always has up to three components:\n",
    "* Partitioning - controls which records will be considered for each window\n",
    "* Sorting - sorts all records in a window\n",
    "* Range - controls how many records in the sorted list should be aggregated\n",
    "\n",
    "### Aggregation functions\n",
    "After the window has been created, you can use any conventional aggregation function like `sum`, `avg` etc. In addition Spark also provides some special window functions which make use of the ordering (which is not available in normal aggregations). The most important window aggregation functions are:\n",
    "* `rank()`\n",
    "* `dense_rank()`\n",
    "* `row_number()`\n",
    "* `lag(column, n)` and `lead(column, n)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Extarct year and quarter\n",
    "\n",
    "Technically the first step is not required, but in order to provide some meaningful sorting, we extract the quarter (Q1, Q2, Q3 and Q4) and the year from the incoming column Quarter. Otherwise sorting wouldn't work, since that column is formatted as 'Q'q YYYY which doesn't provide a chronologically ordering if sorted alphabetically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_data = aggregated_data.select(\n",
    "    f.col(\"*\"),\n",
    "    f.substring(aggregated_data[\"Quarter\"],1,2).alias(\"q\"),\n",
    "    f.substring(aggregated_data[\"Quarter\"],3,8).alias(\"y\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define window\n",
    "\n",
    "This time we use a windowed aggregation to calculate the average price. As the first step we need to construct a *window*. In this case it contains the following ingredients:\n",
    "* A definition of partitions (i.e. which rows should be averages together)\n",
    "* A definition of the window size in rows (i.e. which rows within each partition should take part for each average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a window with\n",
    "#   * partitioned by \"Retailer country\", \"Retailer type\", \"Product line\"\n",
    "#   * unbounded preceeding and unbounded following (either do not specify a range, or use Window.unboundedPreceding and Window.unboundedFollowing)\n",
    "avg_window = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Perform analysis\n",
    "\n",
    "Now we want to conduct the simple analysis as follows: We calculate the average revenue per \"Retailer country\", \"Retailer type\" and \"Product line\". This effectively removes the \"Quarter\" from the list of dimensions. The average should be provided as a new column `avg_revenue`. Then we simply subtract this average revenue from each Revenue and store the result again in a new column `revenue_diff`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the following two columns to the DataFrame \"extended_data\":\n",
    "#  * avg_revenue - this should contain the average revenue within each window defined above\n",
    "#  * revenue_diff - this should contain the difference between the original Revenue and the new \"avg_revenue\"\n",
    "result = # YOUR CODE HERE\n",
    "\n",
    "# Sort result for nicer output\n",
    "sorted_result = result \\\n",
    "    .orderBy(\"Retailer Country\", \"Retailer Type\", \"Product line\", \"y\", \"q\") \\\n",
    "    .drop(\"q\", \"y\")\n",
    "\n",
    "sorted_result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Best Quarter\n",
    "\n",
    "Another interesting question would be, which quarter was the best one in each country for each retailer type and product line. This would be already much harder to do with a join, since the join key would probably need to contain the maximum revenue, which is a double (never join on floating point values, it might not work)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Using windowing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Perform analysis\n",
    "\n",
    "Again we need to define a window, and within each window partition we want to sort the rows by the `Revenue` column and add the sorted position as a new column. This then allows us to trivially simply select the top most row in each window, which contains the best revenue. \n",
    "\n",
    "This time the window again needs to be partitioned by the dimensions `Retailer country`, `Retailer type` and `Product line`. In order to identified the best quarter, we also need to sort all entries within each window by `Revenue`, such that we can easily pick the top most revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a ranking window. This is defined by\n",
    "#   * partition by \"Retailer country\", \"Retailer type\" and \"Product line\"\n",
    "#   * sorted by \"Revenue\"\n",
    "rank_window = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Perform analysis\n",
    "\n",
    "By using this window, we can easily perform the analysis be calculating the position of each record within its window by using the `row_number` function and then select the top most record by filtering the row number to be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column \"rank\" by using the \"row_number\" window function together with the window defined above. \n",
    "ranked_data = # YOUR CODE HERE\n",
    "\n",
    "# Pick the top entry of every window by filtering on the row number. Only keep records with rank = 1\n",
    "result = # YOUR CODE HERE\n",
    "\n",
    "# Sort result, just to improve output\n",
    "sorted_result = result \\\n",
    "    .orderBy(\"Retailer Country\", \"Retailer Type\", \"Product line\", \"y\", \"q\") \\\n",
    "    .drop(\"q\", \"y\", \"rank\")\n",
    "\n",
    "sorted_result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Difference between Quarters\n",
    "\n",
    "Another common example where windowing will greatly simplify processing is accessing different rows in a single query. This cannot be done in Spark without using some trick, since Spark normally processes all rows independently. In a simple `select` you can access any number of columns, but you only have access to a single row.\n",
    "\n",
    "As an example, we'd like to calculate the difference in revenue of two consecutive quarters. Obviously we need to access the revenue of two quarters to calulcate the difference. Again we use two different approaches, the first using a `join` operation and the second using a windowed aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Self Join\n",
    "\n",
    "The first approach will join the data set to itself, such that two different quarters of the same retailer country, retailer type and product type are put together into a single row. Then a simple subtraction will provide the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Calculate previous quarter\n",
    "\n",
    "As a first step, we need to create a small helper function for calculating the previous quarter from a given quarter using the provided format `Qq YYYY`. With this function we can generate the join key required for joining the same dataset on the previous quarter.\n",
    "\n",
    "We will write a small Python UDF to perform the desired operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prev_quarter(quarter):\n",
    "    q = int(quarter[1:2])\n",
    "    y = int(quarter[3:8])\n",
    "    \n",
    "    prev_q = q - 1\n",
    "    if (prev_q <= 0):\n",
    "        prev_y = y - 1\n",
    "        prev_q = 4\n",
    "    else:\n",
    "        prev_y = y\n",
    "    \n",
    "    return \"Q\" + str(prev_q) + \" \" + str(prev_y)\n",
    "    \n",
    "print(prev_quarter(\"Q1 2012\"))\n",
    "print(prev_quarter(\"Q4 2012\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_quarter_udf = f.udf(prev_quarter, 'string')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the `prev_quarter` UDF to the data set to create a new column containing the previous quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_data = aggregated_data.withColumn(\"prev_quarter\", prev_quarter_udf(aggregated_data[\"Quarter\"]))\n",
    "\n",
    "extended_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Join current and previous Quarter\n",
    "\n",
    "Now we need to join the current quarter with the last quarter using the newly created column `prev_quarter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data = extended_data.alias(\"current\").join(\n",
    "        extended_data.alias(\"prev\"),\n",
    "        (f.col(\"current.Quarter\") == f.col(\"prev.prev_quarter\")) &\n",
    "        (f.col(\"current.Retailer country\") == f.col(\"prev.Retailer country\")) &\n",
    "        (f.col(\"current.Retailer type\") == f.col(\"prev.Retailer type\")) &\n",
    "        (f.col(\"current.Product Line\") == f.col(\"prev.Product Line\")),\n",
    "        \"left\"\n",
    "    )\n",
    "\n",
    "joined_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that most columns are present twice now, but by using the data frame aliases `current` and `prev` we still can distinguish between the two original sources. We need that capability in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculate difference\n",
    "\n",
    "Now that we have the current revenue and the previous revenue joined together in a single data frame, we finally can now calculate the difference and keep only the columns from the `current` data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data.select(\n",
    "        f.col(\"current.*\"),\n",
    "        (f.col(\"current.Revenue\") - f.col(\"prev.Revenue\")).alias(\"revenue_delta\")\n",
    "    )\n",
    "\n",
    "result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Use Windows\n",
    "\n",
    "Now that we saw how to solve the problem with a join (and a UDF for calculating the previous quarter), let us get to a different approach using a windowed aggregation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_data = aggregated_data.select(\n",
    "    f.col(\"*\"),\n",
    "    f.substring(aggregated_data[\"Quarter\"],1,2).alias(\"q\"),\n",
    "    f.substring(aggregated_data[\"Quarter\"],3,8).alias(\"y\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define Window\n",
    "\n",
    "What we essentially want to do is to access values from *different rows* for calculating the difference between quarters. So what we need is something like follows:\n",
    "* Create window per retailer country, retailer type and product line\n",
    "* Sort by quarter\n",
    "* Pick previous row\n",
    "\n",
    "The last step is the interesting one. This is done by using the `lag` window aggregation function which allows you to access some preceeding record within the window. Note that the window actually has to contain exactly one record, otherwise you'll get an error by Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a window with the following properties:\n",
    "#   * partitioned by \"Retailer country\", \"Retailer type\" and \"Product line\"\n",
    "#   * orderedBy \"y\" and \"q\"\n",
    "#   * rows between -1 and -1 (just pick the previous row in the window)\n",
    "prev_window = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Perform analysis\n",
    "\n",
    "Now we can use the window in the following simple select statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column \"revenue_delta\" to the DataFrame extended_data, which contains the difference between the current revenue and the revenue from the previous quarter.\n",
    "# Use the \"lag\" function with the window defined above to access the previous revenue within the DataFrame\n",
    "result = # YOUR CODE HERE\n",
    "\n",
    "# Sort and tidy up the DataFrame\n",
    "sorted_result = result \\\n",
    "    .orderBy(\"Retailer Country\", \"Retailer Type\", \"Product line\", \"y\", \"q\") \\\n",
    "    .drop(\"q\", \"y\")\n",
    "\n",
    "sorted_result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Putting it all together\n",
    "\n",
    "Of course you can also use different window aggregations with different windows in a single query as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_window = Window\\\n",
    "    .orderBy(extended_data[\"Revenue\"].desc())\\\n",
    "    .partitionBy(\n",
    "        \"Retailer country\",\n",
    "        \"Retailer type\",\n",
    "        \"Product line\"\n",
    "    )\n",
    "avg_window = Window\\\n",
    "    .orderBy(extended_data[\"Revenue\"].desc())\\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing) \\\n",
    "    .partitionBy(\n",
    "        \"Retailer country\",\n",
    "        \"Retailer type\",\n",
    "        \"Product line\"\n",
    "    )\n",
    "\n",
    "prev_window = Window \\\n",
    "    .orderBy(extended_data[\"y\"].asc(),extended_data[\"q\"].asc())\\\n",
    "    .rowsBetween(-1, -1) \\\n",
    "    .partitionBy(\n",
    "        \"Retailer country\",\n",
    "        \"Retailer type\",\n",
    "        \"Product line\"\n",
    "    )\n",
    "\n",
    "result = extended_data.select(\n",
    "        f.col(\"*\"),\n",
    "        f.row_number().over(rank_window).alias(\"rank\"),\n",
    "        f.avg(extended_data[\"Revenue\"]).over(avg_window).alias(\"avg_revenue\"),\n",
    "        (extended_data[\"Revenue\"] - f.lag(extended_data[\"Revenue\"], 1).over(prev_window)).alias(\"revenue_delta\")\n",
    "    )\n",
    "\n",
    "sorted_result = result\\\n",
    "    .orderBy(\"Retailer Country\", \"Retailer Type\", \"Product line\", \"y\", \"q\") \\\n",
    "    .drop(\"q\", \"y\")\n",
    "\n",
    "result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
