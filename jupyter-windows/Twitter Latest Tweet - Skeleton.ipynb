{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"4G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Example\n",
    "\n",
    "In this notebook, we will work with some Twitter data. It was downloaded from *The Interet Archive* at https://archive.org/details/twitterstream. To demonstrate some use case for Spark window functions, we want to find the latest tweet for each hashtag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load Twitter Data\n",
    "\n",
    "In a first step, we load the Twitter data. It is stored as JSONs, which are well supported by Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = \"s3://dimajix-training/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read\\\n",
    "    .json(basedir + \"/twitter-sample/00.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Schema\n",
    "\n",
    "Now let us inspect the schema. As we will see, the meta data for each tweet is really massive and the complete data model quite complex. Fortunately we are only interested in the tweet itself and the list of hashtags. Note that the hashtags are already extracted for us, so there is no need to use some custom extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Reduce Schema\n",
    "\n",
    "Since we don't want to work with the whole schema, let us select only the relevant columns. Note that this is only a simplification for us human beings. Spark itself would also only extract the required columns anyway, so there is no performance improvement here (which is a good thing, since Spark automatically optimizes performance).\n",
    "\n",
    "Specifically we are interested in the following columns:\n",
    "* `created_at` contains the date and time when the tweet was originally created\n",
    "* `text` contains the full text of the tweet\n",
    "* `entities.hashtags.text` contains an array of all hash tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = data.select(\n",
    "    data[\"created_at\"],\n",
    "    data[\"text\"],\n",
    "    data[\"entities.hashtags.text\"].alias(\"hashtags_array\")\n",
    ")\n",
    "hashtags.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Unpack Hashtags\n",
    "\n",
    "Now the schema contains an array element with a list of all hashtags. But what we want and need is one record per hashtag with all other attributes copied into the generated records. This can be done with the Spark function `explode`. So we try again, but this time we generate a new record for every entry in the hashtag array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = data.select(\n",
    "    data[\"created_at\"],\n",
    "    data[\"text\"],\n",
    "    # YOUR CODE HERE\n",
    ")\n",
    "hashtags.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Count Hashtag Frequency\n",
    "\n",
    "Our primary goal is to find the latest tweet for every hashtag. But this only makes sense, if individual hashtags are present more than only once in our data set. So as a pre-analysis step, let us count the frequency of all hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = # YOUR CODE HERE\n",
    "\n",
    "result.orderBy(result[\"count\"].desc()).limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Find Latest Tweet per Hashtag\n",
    "\n",
    "Now we want to find the newest/latest tweet for every hashtag. This could be done using a self join, but using windows is much simpler and more natural. In addition to the latest tweet, we also want to have the count of every hashtag. We already did that before, but if we want to combine both data sets, this would require a join. Instead we also count using a window function.\n",
    "\n",
    "In the first step, we simply perform the window aggregation and inspect the intermediate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First window for finding the newest hash tag. We will use the row number within the window to select the newest hash tag\n",
    "rank_window = # YOUR CODE HERE\n",
    "\n",
    "# Second window for counting the total frequency of every hash tag\n",
    "count_window = # YOUR CODE HERE\n",
    "\n",
    "ranked_hashtags = hashtags.select(\n",
    "    # YOUR CODE HERE\n",
    ")\n",
    "ranked_hashtags.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect result\n",
    "\n",
    "Now let us inspect the intermediate result. We do not want to view all records, but we want to restrict ourselves to the non-trivial cases where there are multiple tweets for a given hashtag (i.e. `count > 1`). \n",
    "\n",
    "Moreover we also want to sort the result\n",
    "* First sort by count, descending. This ensures that the most commonly used hashtag comes first\n",
    "* Then sort by hashtag in case that there are two hashtags with the same count\n",
    "* Finally sort by rank\n",
    "This sorting more or less gives us the windows concatenated into a new data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ranked_hashtags.filter(ranked_hashtags[\"count\"] > 1) \\\n",
    "    .orderBy(ranked_hashtags[\"count\"].desc(), ranked_hashtags[\"hashtag\"], ranked_hashtags[\"rank\"].asc())\n",
    "\n",
    "result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find latest Tweet\n",
    "\n",
    "Now we only need to filter the result and select the tweets with `rank == 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = # YOUR CODE HERE\n",
    "\n",
    "result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
